{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owcrW1IV0Eox"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notes:\n",
        "\n",
        "1. considering how much upsampling is going in in each channels for accounting for the perception of each frequency band... I could get away with downsampling the whole thing in the frequency direction by a factor of 4, 8 or even 16, reducing the sample rate post upsample.... this could worth for both the masking model and the diffusion model easily, reducing the image size from (1024, 175) to (128, 175)\n",
        "\n",
        "2. I need to edit the levels of the training data so that anything where the signal is greater than 1 (due to high noise), should be normalised to 1\n",
        "\n",
        "3. I may need to connect the features as images next to eachother, instead of channels in an input as the channels are cross effecting eachother in a negative way (where audio shoule be let through in one channel but bloced in one it is getting confused or assuming a relationship between channels when there is none)\n",
        "\n",
        "Next Steps:\n",
        "\n",
        "2. Compare custom res-net model with attention as masking with deeplabv3_resnet50 altered. To complete the project, compare:\n",
        "3. a) Custom simple, b) custom complex, c) deeplabv3_resnet50 d) lraspp_mobilenet_v3_large e) segformer\n",
        "4. maybe change how different samplerate images are concatenated\n",
        "5. stop using cosine anealing as a straight learning rate is better"
      ],
      "metadata": {
        "id": "x_OzgxdWa-aY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2uW1R-NqaTB",
        "outputId": "9d5b6de5-617d-4f07-f189-f81c9f28b81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9ohND2-qaTC",
        "outputId": "7d7c2d12-188d-4140-b277-dabf8798f116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab...\n"
          ]
        }
      ],
      "source": [
        "# Detect Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    print(\"Running in Google Colab...\")\n",
        "    os.system(\"git clone https://github.com/CiaranMaloy/audioautoencoder\")\n",
        "    os.chdir(\"/content/audioautoencoder/\")\n",
        "    os.system(\"git pull\")\n",
        "    os.system(\"git checkout test-with-features\")\n",
        "    os.system(\"git pull origin test-with-features\")\n",
        "    #os.system(\"pip install --upgrade torchmetrics\")\n",
        "else:\n",
        "    print(\"Running locally...\")\n",
        "    os.system(\"git pull origin main\")\n",
        "    #os.system(\"pip install --upgrade torchmetrics\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7TDd5eSmqaTC"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/audioautoencoder')\n",
        "sys.path.append('/content/audioautoencoder/audioautoencoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8zyus8N0Eoy"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from audioautoencoder.models.UNetRes12 import *\n",
        "from audioautoencoder.models.UNetConv4 import *"
      ],
      "metadata": {
        "id": "UAF0PUg2-C10"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwkxT6OP0Eoz"
      },
      "source": [
        "## Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReWh6Ybl0Eoz",
        "outputId": "c98ea144-a345-4919-8dd3-fadbb5f19583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output....\n",
            "torch.Size([1, 4, 256, 175])\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 1\n",
        "TEST_MODEL = True\n",
        "\n",
        "if TEST_MODEL:\n",
        "  if __name__ == \"__main__\":\n",
        "      x = torch.randn((BATCH_SIZE, 8, 1025 // 4, 175))\n",
        "      #model = UNetRes12(in_channels=4, out_channels=4)\n",
        "      model = UNetConv4(in_channels=8, out_channels=4)\n",
        "      model.eval()\n",
        "      output = model(x)\n",
        "\n",
        "      print('output....')\n",
        "      print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "_r46u9cXkIgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# For example, if the input size is (batch_size=2, channels=9, height=256, width=256):\n",
        "summary(model, input_size=(2, 8, 1025 // 4, 175))\n"
      ],
      "metadata": {
        "id": "1c_nBwCfNs4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smzGryj-0Eoz"
      },
      "source": [
        "## Define Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_wGpwKW0Eo0"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.plotting import *\n",
        "from audioautoencoder.datasets.utils import *\n",
        "#from audioautoencoder.models.UNetConv10mask import *\n",
        "# Instantiate the model, define loss function and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.serialization.add_safe_globals([UNetConv4])\n",
        "model = UNetConv4(in_channels=8, out_channels=4).to(device)\n",
        "\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps1ZBM_H0Eo0"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "train = True\n",
        "LOAD_DATA = True\n",
        "load_model = False\n",
        "\n",
        "# --------------- Main Execution parameters ---------------\n",
        "model_name = 'UNetConv4-features'\n",
        "train_diffusion = False\n",
        "SNRdB_load = [-10, 10]\n",
        "SNRdBs = [[-10, 10]] # SNR random range\n",
        "load_trigger = [load_model]\n",
        "#load_file = 'Autoencodermodel_earlystopping.pth'\n",
        "load_file = 'Autoencodermodel_checkpoint.pth'\n",
        "\n",
        "folder = 'remapped-signal-level' # sep\n",
        "\n",
        "# parameters\n",
        "learning_rates = [1e-3] # 1e-4 for re0training?, 1e-3 for training? # lowered learning rate for\n",
        "                        # when the loss explodes after the learning rate increases, it means dont increase the learning rate too much\n",
        "base_lr=1e-5\n",
        "max_lr=learning_rates[i]\n",
        "gamma=0.8\n",
        "\n",
        "# data params\n",
        "max_file_size_gb = 100\n",
        "IMPORT_TRAIN_NOISY = train\n",
        "batch_size = 32\n",
        "num_workers = 4\n",
        "\n",
        "# training params\n",
        "load = load_trigger[i]\n",
        "warm_start = True\n",
        "epochs = 100\n",
        "accumulation_steps = int((512*2)/batch_size)\n",
        "\n",
        "SNRdB = SNRdBs[i]\n",
        "learning_rate = learning_rates[i]\n",
        "eta_min = 1e-5\n",
        "\n",
        "print('lr:', learning_rate)\n",
        "print('SNRdB:', SNRdB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuZDrHtO0Eo0"
      },
      "outputs": [],
      "source": [
        "# --------------- In Loop Parameters --------------\n",
        "output_path = f'/content/drive/MyDrive/Projects/ML_Projects/De-noising-autoencoder/Models_Comparison/Checkpoints_{model_name}_{SNRdB[0]}-{SNRdB[1]}/'\n",
        "load_path = f'/content/drive/MyDrive/Projects/ML_Projects/De-noising-autoencoder/Models_Comparison/Checkpoints_{model_name}_{SNRdB_load[0]}-{SNRdB_load[1]}/{load_file}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8lgyHuN0Eo0"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zhoo24P0Eo0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import joblib  # or use pickle if you prefer\n",
        "\n",
        "def save_scalers(scalers, save_path):\n",
        "    \"\"\"Save scalers to a file.\"\"\"\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    joblib.dump(scalers, save_path)\n",
        "\n",
        "def load_scalers(save_path):\n",
        "    \"\"\"Load scalers from a file.\"\"\"\n",
        "    return joblib.load(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from audioautoencoder.datasets.utils import *\n",
        "from audioautoencoder.data import *\n",
        "from audioautoencoder.data_management import *\n",
        "from audioautoencoder.generate_dataset import *"
      ],
      "metadata": {
        "id": "-YEtuzpV3j2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efdO-tp40Eo0"
      },
      "outputs": [],
      "source": [
        "# Define the source and destination file paths\n",
        "if LOAD_DATA:\n",
        "  scaler_file = output_path + \"scalers.pkl\"  # Static filename since it's unique per run\n",
        "  os.makedirs(os.path.dirname(scaler_file), exist_ok=True)\n",
        "  source_folder = f\"/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_{folder}/SNRdB_{SNRdB[0]}-{SNRdB[1]}/\"\n",
        "  source_path = source_folder + \"train/\"\n",
        "  destination_path = f\"/content/SNRdB_{SNRdB[0]}-{SNRdB[1]}/train/\"\n",
        "  save_path = source_folder + \"combined_000.h5\"\n",
        "  subset = False\n",
        "\n",
        "  if IMPORT_TRAIN_NOISY:\n",
        "    dataset_path = f\"/content/SNRdB_{SNRdB[0]}-{SNRdB[1]}/train/combined_000.h5\"\n",
        "    if not os.path.exists(destination_path):\n",
        "      combine_h5_files_features(source_path, destination_path, max_file_size_gb=max_file_size_gb)\n",
        "      #combine_h5_files_spectrograms(source_path, destination_path, max_file_size_gb=max_file_size_gb, timeout=40)\n",
        "\n",
        "    if os.path.exists(scaler_file):\n",
        "        print(\"Loading existing scalers...\")\n",
        "        scalers = load_scalers(scaler_file)\n",
        "    else:\n",
        "        print(\"Training new scalers...\")\n",
        "        #scalers = train_scalers_no_features(dataset_path, sample_size=8000)\n",
        "        scalers = train_scalers(dataset_path, sample_size=8000)\n",
        "        save_scalers(scalers, scaler_file)\n",
        "\n",
        "    print(scalers)\n",
        "\n",
        "    train_loader = ChannelDatasetLoader(\n",
        "          dataset_path=dataset_path,\n",
        "          scalers=scalers,\n",
        "          output_time_length=175,\n",
        "          channels=1,\n",
        "          snr_db=SNRdB,\n",
        "          subset=subset,\n",
        "          batch_size=batch_size,\n",
        "          num_workers=num_workers\n",
        "      )\n",
        "\n",
        "    print(f\"Training set size: {len(train_loader.train_dataset)}\")\n",
        "    print(f\"Validation set size: {len(train_loader.val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect Data"
      ],
      "metadata": {
        "id": "GJ74jmahB2ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_DATA:\n",
        "  _, output, medatata = train_loader.train_dataset[200]\n",
        "  #input, output = train_dataset[0]\n",
        "  #print(metadata['snr_db'])\n",
        "  #print(input.shape)\n",
        "  print(output.shape)"
      ],
      "metadata": {
        "id": "l6-sIxJ-qEDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_DATA:\n",
        "  import matplotlib.pyplot as plt\n",
        "  import torch\n",
        "  import numpy as np\n",
        "  import torch\n",
        "\n",
        "  # Fetch a sample with SNR > 8\n",
        "  for i in range(100):\n",
        "      input_tensor, output_tensor, metadata = train_loader.train_dataset[i]\n",
        "      if metadata['snr_db'] > 8:\n",
        "          print(\"Found sample with SNR > 8:\")\n",
        "          print(\"SNR:\", metadata['snr_db'])\n",
        "          print(\"Index:\", i)\n",
        "          break  # exit loop after finding the first match\n",
        "\n",
        "\n",
        "\n",
        "  print(metadata['snr_db'])\n",
        "\n",
        "  # Convert to NumPy for plotting\n",
        "  #input_array = np.clip(input_tensor.numpy(), -1, a_max=None)\n",
        "  output_array = np.clip(output_tensor.numpy(), -1, a_max=None)\n",
        "  input_array = np.clip(input_tensor.numpy(), -1, a_max=None)\n",
        "\n",
        "  # remove negatve values\n",
        "  #input_array = torch.clamp(input_array, min=0)  # Sets all negative values to 0\n",
        "  print(np.shape(output_array))\n",
        "\n",
        "  num_channels_out = output_array.shape[0]\n",
        "  num_channels_in = input_array.shape[0]\n",
        "\n",
        "\n",
        "  # Create subplots\n",
        "  fig, axes = plt.subplots(num_channels_in + num_channels_out, 1, figsize=(15, 15))\n",
        "\n",
        "  # Plot each input channel\n",
        "  for i in range(num_channels_in):\n",
        "      input = input_array[i]\n",
        "      print(np.shape(input))\n",
        "      print('Min, Max: ', np.min(input), np.max(input))\n",
        "      im = axes[i].imshow(input, aspect='auto', cmap='magma')\n",
        "      axes[i].invert_yaxis()\n",
        "\n",
        "      axes[i].set_title(f\"Input Channel {i+1}\")\n",
        "      axes[i].axis(\"off\")\n",
        "\n",
        "      # Add colorbar\n",
        "      cbar = fig.colorbar(im, ax=axes[i], orientation=\"vertical\")\n",
        "      cbar.set_label(\"Amplitude\")\n",
        "\n",
        "    # Plot each input channel\n",
        "  for i in range(num_channels_out):\n",
        "      output = output_array[i]\n",
        "      print('Min, Max: ', np.min(output), np.max(output))\n",
        "      im = axes[num_channels_in + i].imshow(output, aspect='auto', cmap='magma')\n",
        "      axes[num_channels_in + i].invert_yaxis()\n",
        "\n",
        "      axes[num_channels_in + i].set_title(f\"Output Channel {i+1}\")\n",
        "      axes[num_channels_in + i].axis(\"off\")\n",
        "\n",
        "      # Add colorbar\n",
        "      cbar = fig.colorbar(im, ax=axes[num_channels_in + i], orientation=\"vertical\")\n",
        "      cbar.set_label(\"Amplitude\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "YO8_4lXBtXim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reverse the octave band encoding"
      ],
      "metadata": {
        "id": "5rceGlGZ1NNL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtS-e5ED0Eo0"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK_yD-3v0Eo0"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.loss import *\n",
        "from audioautoencoder.utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKcwRjh60Eo0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJX1Rbee0Eo0"
      },
      "outputs": [],
      "source": [
        "if load:\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=eta_min)\n",
        "  scheduler_loss = False\n",
        "else:\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=eta_min)\n",
        "  scheduler_loss = False\n",
        "\n",
        "  #optimizer = None #torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "  #scheduler = None #torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
        "  #scheduler_loss = False #True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from audioautoencoder.plotting import *\n",
        "from audioautoencoder.training import *"
      ],
      "metadata": {
        "id": "QlZqoKmIjeqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nJj4nyA0Eo0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clears all allocated GPU memory in PyTorch.\"\"\"\n",
        "    torch.cuda.empty_cache()  # Clears cache\n",
        "    gc.collect()  # Runs Python garbage collector\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        torch.cuda.reset_peak_memory_stats(i)  # Resets peak memory tracking\n",
        "\n",
        "clear_gpu_memory()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from audioautoencoder.training import *"
      ],
      "metadata": {
        "id": "YwpI8xSyWJ22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# plot example inputs\n",
        "\n"
      ],
      "metadata": {
        "id": "ZdUysIK4fAlW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbTxS-gi0Eo0"
      },
      "outputs": [],
      "source": [
        "if train:\n",
        "  trainer = DenoisingTrainer(\n",
        "      model=model, noisy_train_loader=train_loader.train_loader, noisy_val_loader=train_loader.val_loader,\n",
        "      SNRdB=SNRdB, output_path=output_path, epochs=epochs, learning_rate=learning_rate,\n",
        "      load=load, warm_start=warm_start, train=train, verbose=False, accumulation_steps=accumulation_steps, load_path=load_path,\n",
        "      base_lr=base_lr, max_lr=max_lr, gamma=gamma, optimizer=optimizer, scheduler=scheduler, scheduler_loss=scheduler_loss,\n",
        "      max_noise=0.1, noise_epochs=5, train_diffusion=train_diffusion\n",
        "  )\n",
        "  trainer.train_or_evaluate()\n",
        "  model = trainer.get_model()\n",
        "\n",
        "  # I need a flat load model function somewhere, as now I need to define a train loader before I can load a model\n",
        "  csv_file_path = output_path + \"training_log.csv\"\n",
        "  plot_training_log(csv_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdJxQP5_0Eo0"
      },
      "source": [
        "___"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}