{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange \n",
    "\n",
    "## Masking Unet\n",
    "# enhanced attention on skip connections\n",
    "# deep 3 headded self attention in bottleneck\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels:int, num_groups:int, dropout_prob:float, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.gnorm1 = nn.GroupNorm(num_groups=num_groups, num_channels=channels)\n",
    "        self.gnorm2 = nn.GroupNorm(num_groups=num_groups, num_channels=channels)\n",
    "        \n",
    "        # Adding proper padding calculation to maintain spatial dimensions\n",
    "        padding = kernel_size // 2 if isinstance(kernel_size, int) else (kernel_size[0] // 2, kernel_size[1] // 2)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=kernel_size, padding=padding)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = self.conv1(self.relu(self.gnorm1(x)))\n",
    "        r = self.dropout(r)\n",
    "        r = self.conv2(self.relu(self.gnorm2(r)))  # Fixed: use r instead of x for the second conv\n",
    "        return r + x\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, channels: int, num_heads:int , dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.proj1 = nn.Linear(channels, channels*3)\n",
    "        self.proj2 = nn.Linear(channels, channels)\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[2:]\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.proj1(x)\n",
    "        x = rearrange(x, 'b L (C H K) -> K b H L C', K=3, H=self.num_heads)\n",
    "        q,k,v = x[0], x[1], x[2]\n",
    "        x = F.scaled_dot_product_attention(q,k,v, is_causal=False, dropout_p=self.dropout_prob)\n",
    "        x = rearrange(x, 'b H (h w) C -> b h w (C H)', h=h, w=w)\n",
    "        x = self.proj2(x)\n",
    "        return rearrange(x, 'b h w C -> b C h w')\n",
    "    \n",
    "\n",
    "class ResLayer(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3, attention=False, num_groups=16, dropout_prob=0.1, num_heads=8, upscale=False, downscale=False):\n",
    "        super().__init__()\n",
    "        self.upscale = upscale\n",
    "        self.downscale = downscale\n",
    "\n",
    "        # Calculate proper padding\n",
    "        padding = kernel_size // 2 if isinstance(kernel_size, int) else (kernel_size[0] // 2, kernel_size[1] // 2)\n",
    "        \n",
    "        self.ResBlock1 = ResBlock(channels=channels, kernel_size=kernel_size, num_groups=num_groups, dropout_prob=dropout_prob)\n",
    "        self.ResBlock2 = ResBlock(channels=channels, kernel_size=kernel_size, num_groups=num_groups, dropout_prob=dropout_prob)\n",
    "\n",
    "        if upscale:\n",
    "            self.conv = nn.ConvTranspose2d(channels, channels // 2, kernel_size=2, stride=2, padding=0)\n",
    "        elif downscale:\n",
    "            self.conv = nn.Conv2d(channels, channels * 2, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        if attention:\n",
    "            self.attention_layer = Attention(channels, num_heads=num_heads, dropout_prob=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ResBlock1(x)\n",
    "        if hasattr(self, 'attention_layer'):\n",
    "            x = self.attention_layer(x)\n",
    "        x = self.ResBlock2(x)\n",
    "\n",
    "        if self.upscale:\n",
    "            x = self.conv(x)\n",
    "        elif self.downscale:\n",
    "            x = self.conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EnhancedSkipAttention(nn.Module):\n",
    "    def __init__(self, encoder_channels, decoder_channels, reduction_ratio=4):\n",
    "        super().__init__()\n",
    "        self.channels = encoder_channels\n",
    "        \n",
    "        # Channel attention for encoder features\n",
    "        self.encoder_channel_attn = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(encoder_channels, encoder_channels // reduction_ratio, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(encoder_channels // reduction_ratio, encoder_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Projection for decoder features to match encoder dimensions if needed\n",
    "        self.decoder_proj = None\n",
    "        if encoder_channels != decoder_channels:\n",
    "            self.decoder_proj = nn.Conv2d(decoder_channels, encoder_channels, kernel_size=1)\n",
    "        \n",
    "        # Cross-attention between encoder and decoder features\n",
    "        self.cross_attn = nn.Sequential(\n",
    "            nn.Conv2d(encoder_channels*2, 2, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, encoder_features, decoder_features):\n",
    "        # Process decoder features if dimensions don't match\n",
    "        if self.decoder_proj is not None:\n",
    "            decoder_features = self.decoder_proj(decoder_features)\n",
    "\n",
    "        # Apply channel attention to encoder features\n",
    "        channel_attn = self.encoder_channel_attn(encoder_features)\n",
    "        encoder_features = encoder_features * channel_attn\n",
    "        \n",
    "        # Concatenate encoder and decoder features\n",
    "        combined = torch.cat([encoder_features, decoder_features], dim=1)\n",
    "        \n",
    "        # Generate attention weights for each feature set\n",
    "        attn_weights = self.cross_attn(combined)\n",
    "        encoder_weight, decoder_weight = torch.split(attn_weights, 1, dim=1)\n",
    "        \n",
    "        # Apply weights and combine features\n",
    "        result = encoder_features * encoder_weight + decoder_features * decoder_weight\n",
    "        \n",
    "        return result\n",
    "\n",
    "class UNetConv11(nn.Module):\n",
    "    # Update from UnetConv6, moving to a masking model, which hopefully works better\n",
    "    def __init__(self, in_channels=9, out_channels=4):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = 64\n",
    "\n",
    "        # Fixed input layer with proper padding calculation for given kernel size\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, channels, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        # Encoder (Downsampling) - using standard kernel sizes with proper padding\n",
    "        self.enc1 = ResLayer(channels, kernel_size=3, downscale=True)\n",
    "        self.enc2 = ResLayer(channels * 2, kernel_size=3, downscale=True)\n",
    "        self.enc3 = ResLayer(channels * 4, kernel_size=3, downscale=True)\n",
    "        self.enc4 = ResLayer(channels * 8, kernel_size=3, downscale=True)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck_in = ResLayer(channels * 16, kernel_size=3)\n",
    "        self.resattention = ResLayer(channels * 16, kernel_size=3, attention=True)\n",
    "        self.bottleneck_out = ResLayer(channels * 16, kernel_size=3)\n",
    "\n",
    "        # Decoder (Upsampling) - using standard kernel sizes\n",
    "        self.dec4 = ResLayer(channels * 16, kernel_size=3, upscale=True)\n",
    "        self.dec3 = ResLayer(channels * 8, kernel_size=3, upscale=True)\n",
    "        self.dec2 = ResLayer(channels * 4, kernel_size=3, upscale=True)\n",
    "        self.dec1 = ResLayer(channels * 2, kernel_size=3, upscale=True)\n",
    "\n",
    "        # Final output layer\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Conv2d(channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Initialize Spatial Attention Modules\n",
    "        self.attn4 = EnhancedSkipAttention(channels * 16, channels * 16)\n",
    "        self.attn3 = EnhancedSkipAttention(channels * 8, channels * 8)\n",
    "        self.attn2 = EnhancedSkipAttention(channels * 4, channels * 4)\n",
    "        self.attn1 = EnhancedSkipAttention(channels * 2, channels * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with skip connections\"\"\"\n",
    "        input_shape = x.shape[2:]  # Remember original input spatial dimensions\n",
    "        \n",
    "        # Encoding\n",
    "        input_features = self.input_layer(x)\n",
    "        e1 = self.enc1(input_features)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck_in(e4)\n",
    "        b = self.resattention(b)\n",
    "        b = self.bottleneck_out(b)\n",
    "\n",
    "        # Decoding with proper feature alignment\n",
    "        # For decoder stage 4\n",
    "        b = F.interpolate(b, size=e4.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        d4 = self.attn4(e4, b)\n",
    "        d4 = self.dec4(d4)\n",
    "\n",
    "        # For decoder stage 3\n",
    "        d4 = F.interpolate(d4, size=e3.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        d3 = self.attn3(e3, d4)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        # For decoder stage 2\n",
    "        d3 = F.interpolate(d3, size=e2.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        d2 = self.attn2(e2, d3)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        # For decoder stage 1\n",
    "        d2 = F.interpolate(d2, size=e1.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        d1 = self.attn1(e1, d2)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        # Final output with bilinear interpolation to match input size\n",
    "        mask = self.output_layer(d1)\n",
    "        mask = F.interpolate(mask, size=input_shape, mode=\"bilinear\", align_corners=False)\n",
    "        \n",
    "        # Apply mask to the first 4 channels of input\n",
    "        return x[:, :4] * mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "TEST_MODEL = True\n",
    "\n",
    "if TEST_MODEL:\n",
    "  if __name__ == \"__main__\":\n",
    "      x = torch.randn((BATCH_SIZE, 9, 1025, 175))\n",
    "      model = UNetConv11()\n",
    "      output = model(x)\n",
    "\n",
    "      print('output....')\n",
    "      print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "class TestAutoencoder(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.model = UNetConv11()\n",
    "        self.input_channels = 9\n",
    "        self.output_channels = 4\n",
    "        self.input_height = 1025\n",
    "        self.input_width = 175\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def test_model_initialization(self):\n",
    "        self.assertIsInstance(self.model, UNetConv11, \"Model initialization failed\")\n",
    "\n",
    "    def test_forward_pass(self):\n",
    "        x = torch.randn(self.batch_size, self.input_channels, self.input_height, self.input_width, device=self.device)\n",
    "        output = self.model(x)\n",
    "        self.assertEqual(\n",
    "            output.shape,\n",
    "            (self.batch_size, self.output_channels, self.input_height, self.input_width),\n",
    "            f\"Expected output shape {(self.batch_size, self.output_channels, self.input_height, self.input_width)}, but got {output.shape}\"\n",
    "        )\n",
    "\n",
    "    def test_model_summary(self):\n",
    "        try:\n",
    "            summary(self.model, input_size=(self.input_channels, self.input_height, self.input_width))\n",
    "        except Exception as e:\n",
    "            self.fail(f\"Model summary failed: {str(e)}\")\n",
    "\n",
    "# This allows running tests externally\n",
    "def suite():\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestAutoencoder)\n",
    "    return test_suite\n",
    "\n",
    "# runner\n",
    "class TestRunner:\n",
    "    def __init__(self):\n",
    "        self.runner = unittest.TextTestRunner()\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Running autoencoder tests...\")\n",
    "        self.runner.run(suite())\n",
    "\n",
    "if TEST_MODEL:\n",
    "  if __name__ == \"__main__\":\n",
    "      runner = TestRunner()\n",
    "      runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audioautoencoder.plotting import *\n",
    "from audioautoencoder.datasets.utils import *\n",
    "from audioautoencoder.models.UNetConv10mask import *\n",
    "# Instantiate the model, define loss function and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetConv10().to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "train = False\n",
    "LOAD_DATA = False\n",
    "load_model = True\n",
    "\n",
    "# --------------- Main Execution parameters ---------------\n",
    "model_name = 'UNetConv10_mask'\n",
    "SNRdB_load = [-10, 10]\n",
    "SNRdBs = [[-10, 10]] # SNR random range\n",
    "load_trigger = [load_model]\n",
    "load_file = 'Autoencodermodel_earlystopping.pth'\n",
    "#load_file = 'Autoencodermodel_checkpoint.pth'\n",
    "\n",
    "folder = ['sep_features'][i] # sep\n",
    "\n",
    "# parameters\n",
    "learning_rates = [1e-3] # 1e-4 for re0training?, 1e-3 for training?\n",
    "\n",
    "base_lr=1e-5\n",
    "max_lr=learning_rates[i]\n",
    "gamma=0.8\n",
    "\n",
    "# data params\n",
    "max_file_size_gb = 100\n",
    "IMPORT_TRAIN_NOISY = train\n",
    "batch_size = 64\n",
    "\n",
    "# training params\n",
    "load = load_trigger[i]\n",
    "warm_start = False\n",
    "epochs = 30\n",
    "accumulation_steps = int(512/batch_size)\n",
    "\n",
    "SNRdB = SNRdBs[i]\n",
    "learning_rate = learning_rates[i]\n",
    "eta_min = 1e-5\n",
    "\n",
    "print('lr:', learning_rate)\n",
    "print('SNRdB:', SNRdB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- In Loop Parameters --------------\n",
    "output_path = f'/content/drive/MyDrive/Projects/ML_Projects/De-noising-autoencoder/Models_Denoising/Checkpoints_{model_name}_{SNRdB[0]}-{SNRdB[1]}/'\n",
    "load_path = f'/content/drive/MyDrive/Projects/ML_Projects/De-noising-autoencoder/Models_Denoising/Checkpoints_{model_name}_{SNRdB_load[0]}-{SNRdB_load[1]}/{load_file}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib  # or use pickle if you prefer\n",
    "\n",
    "def save_scalers(scalers, save_path):\n",
    "    \"\"\"Save scalers to a file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    joblib.dump(scalers, save_path)\n",
    "\n",
    "def load_scalers(save_path):\n",
    "    \"\"\"Load scalers from a file.\"\"\"\n",
    "    return joblib.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the source and destination file paths\n",
    "if LOAD_DATA:\n",
    "  scaler_file = output_path + \"scalers.pkl\"  # Static filename since it's unique per run\n",
    "  source_folder = f\"/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_{folder}/SNRdB_{SNRdB[0]}-{SNRdB[1]}/\"\n",
    "  source_path = source_folder + \"train/\"\n",
    "  destination_path = f\"/content/SNRdB_{SNRdB[0]}-{SNRdB[1]}/train/\"\n",
    "  save_path = source_folder + \"combined_000.h5\"\n",
    "  subset = False\n",
    "\n",
    "  if IMPORT_TRAIN_NOISY:\n",
    "    dataset_path = f\"/content/SNRdB_{SNRdB[0]}-{SNRdB[1]}/train/combined_000.h5\"\n",
    "    if not os.path.exists(destination_path):\n",
    "      combine_h5_files_features(source_path, destination_path, max_file_size_gb=max_file_size_gb)\n",
    "\n",
    "    if os.path.exists(scaler_file):\n",
    "        print(\"Loading existing scalers...\")\n",
    "        scalers = load_scalers(scaler_file)\n",
    "    else:\n",
    "        print(\"Training new scalers...\")\n",
    "        scalers = train_scalers_separation(dataset_path, sample_size=8000)\n",
    "        save_scalers(scalers, scaler_file)\n",
    "\n",
    "    print(scalers)\n",
    "\n",
    "    train_loader = ChannelDatasetLoader(\n",
    "          dataset_path=dataset_path,\n",
    "          scalers=scalers,\n",
    "          output_time_length=175,\n",
    "          channels=1,\n",
    "          snr_db=SNRdB,\n",
    "          subset=subset,\n",
    "          batch_size=batch_size\n",
    "      )\n",
    "\n",
    "    print(f\"Training set size: {len(train_loader.train_dataset)}\")\n",
    "    print(f\"Validation set size: {len(train_loader.val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audioautoencoder.loss import *\n",
    "from audioautoencoder.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load:\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "  #scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "  scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=eta_min)\n",
    "  scheduler_loss = False\n",
    "else:\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "  scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=eta_min)\n",
    "  scheduler_loss = False\n",
    "\n",
    "  #optimizer = None #torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "  #scheduler = None #torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "  #scheduler_loss = False #True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clears all allocated GPU memory in PyTorch.\"\"\"\n",
    "    torch.cuda.empty_cache()  # Clears cache\n",
    "    gc.collect()  # Runs Python garbage collector\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.reset_peak_memory_stats(i)  # Resets peak memory tracking\n",
    "\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "  trainer = DenoisingTrainer(\n",
    "      model=model, noisy_train_loader=train_loader.train_loader, noisy_val_loader=train_loader.val_loader,\n",
    "      SNRdB=SNRdB, output_path=output_path, epochs=epochs, learning_rate=learning_rate,\n",
    "      load=load, warm_start=warm_start, train=train, verbose=False, accumulation_steps=accumulation_steps, load_path=load_path,\n",
    "      base_lr=base_lr, max_lr=max_lr, gamma=gamma, optimizer=optimizer, scheduler=scheduler, scheduler_loss=scheduler_loss,\n",
    "      max_noise=0.01, noise_epochs=5\n",
    "  )\n",
    "  trainer.train_or_evaluate()\n",
    "  model = trainer.get_model()\n",
    "\n",
    "  # I need a flat load model function somewhere, as now I need to define a train loader before I can load a model\n",
    "  csv_file_path = output_path + \"training_log.csv\"\n",
    "  plot_training_log(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
