{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoise Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audioautoencoder.denoising import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- In Loop Parameters --------------\n",
    "model_name = 'UNetConv10_mask'\n",
    "SNRdB_load = [-10, 10]\n",
    "load_file = 'Autoencodermodel_earlystopping.pth'\n",
    "load_path = f'/content/drive/MyDrive/Projects/ML_Projects/De-noising-autoencoder/Models_Denoising/Checkpoints_{model_name}_{SNRdB_load[0]}-{SNRdB_load[1]}/{load_file}'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_one = UNetConv10().to(device)\n",
    "denoiser = DenoisingLoader(model_one, load_path)\n",
    "model_one = denoiser.model\n",
    "print('Loaded Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "whole_files = '/content/drive/MyDrive/Datasets/Music/MUSDB18/test/'\n",
    "song_files = []\n",
    "\n",
    "# Walk through the directory tree\n",
    "for root, dirs, files in os.walk(whole_files):\n",
    "    # Filter files with '.wav' extension and 'mixture' in their name\n",
    "    for f in files:\n",
    "        if f.endswith('.wav') and 'mixture' in f:\n",
    "            full_path = os.path.join(root, f)\n",
    "            song_files.append(full_path)\n",
    "\n",
    "print(f\"\\nTotal matching files: {len(song_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/val/crowd noise (4)_xDoJJ9.wav'\n",
    "#noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/val/plane-noise-passengers-sound_s8OrJQ.mp3'\n",
    "#noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/val/Crowd - Mall ambience_UdLE4r.wav'\n",
    "#noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test/Crowd - Cheering - Strong cheering and soft rhythmic cheering_Pxj5eZ.wav'\n",
    "#noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test/Crowd - Street parade with music_FgF6cW.wav'\n",
    "noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test/Robocup 2019 4.1_vTYYoj.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio_with_noise(audio_file, noise_file, start_time=10, duration=10,\n",
    "                             signal_level=1, noise_level=0.1, sr=44100, plot=False):\n",
    "    \"\"\"\n",
    "    Loads an audio file and a noise file, trims them, normalizes, and adds Gaussian noise.\n",
    "\n",
    "    Parameters:\n",
    "        audio_file (str): Path to the main audio file.\n",
    "        noise_file (str): Path to the noise file.\n",
    "        start_time (int): Start time (in seconds) for trimming.\n",
    "        duration (int): Total duration (in seconds).\n",
    "        signal_level (float): Scaling factor for the audio signal.\n",
    "        noise_level (float): Scaling factor for the noise.\n",
    "        sr (int): Expected sample rate (default: 44100 Hz).\n",
    "\n",
    "    Returns:\n",
    "        noisy_audio (np.array): Processed noisy audio.\n",
    "        snr (float): Signal-to-noise ratio in dB.\n",
    "    \"\"\"\n",
    "    # Load audio and noise\n",
    "    audio, audio_sr = load_audio_file(audio_file)\n",
    "    noise_waveform, noise_sr = load_audio_file(noise_file)\n",
    "\n",
    "    if len(audio) == 2:\n",
    "      audio = audio[0]\n",
    "\n",
    "    if len(noise_waveform) == 2:\n",
    "      noise_waveform = noise_waveform[0]\n",
    "\n",
    "    # Trim audio and noise to the specified start time and duration\n",
    "\n",
    "    audio = audio.cpu().numpy() if isinstance(audio, torch.Tensor) else audio\n",
    "    noise_waveform = noise_waveform.cpu().numpy() if isinstance(noise_waveform, torch.Tensor) else noise_waveform\n",
    "\n",
    "    print('Noise Sample Rate:', noise_sr)\n",
    "\n",
    "    assert audio_sr == sr, f\"Expected sample rate {sr}, but got {audio_sr}\"\n",
    "\n",
    "    # Trim audio and noise to the specified start time and duration\n",
    "    audio = audio[start_time * sr : (start_time + duration) * sr]\n",
    "    noise_waveform = noise_waveform[start_time * noise_sr : (start_time + duration) * noise_sr]\n",
    "\n",
    "    # Normalize audio to [-1, 1]\n",
    "    audio = np.clip((audio / np.max(np.abs(audio))) * signal_level, -1, 1)\n",
    "    noise_waveform = np.clip((noise_waveform / np.max(np.abs(noise_waveform))) * noise_level, -1, 1)\n",
    "\n",
    "    # Add noise to the signal\n",
    "    noisy_audio = np.clip(audio + noise_waveform, -1, 1)\n",
    "\n",
    "    # Compute SNR\n",
    "    signal_power = np.mean(audio**2)\n",
    "    noise_power = np.mean(noise_waveform**2)\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "\n",
    "    print(f\"SNR: {snr:.2f} dB\")\n",
    "\n",
    "    # Plot results\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(noise_waveform, label=\"Noise\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(audio, label=\"Clean Audio\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(noisy_audio, label=\"Noisy Audio\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return noisy_audio, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "\n",
    "def denoise_audio_chunk(chunk, sr, model, scalers, chunk_samples=2*44100, device=None):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    features = extract_features(chunk, sr, audio_length=chunk_samples)\n",
    "    transformed_features, metadata = transform_features(features, scalers)\n",
    "    \n",
    "    # AI denoising\n",
    "    input_tensor = torch.tensor(np.array([transformed_features]), dtype=torch.float32).to(device)\n",
    "    denoised = model(input_tensor)\n",
    "    \n",
    "    #input_array = input_tensor.detach().cpu().numpy()[0]\n",
    "    denoised_array = denoised.detach().cpu().numpy()[0]\n",
    "    \n",
    "    denoised_spectrogram = reconstruct_spectrogram(denoised_array, metadata)\n",
    "    #input_spectrogram = reconstruct_spectrogram(input_array, metadata)\n",
    "    \n",
    "    # Remove lower-than-average values from the spectrogram\n",
    "    denoised_spectrogram = threshold_spectrogram(denoised_spectrogram, np.mean(denoised_spectrogram), percentage=0.5)\n",
    "    \n",
    "    # De-normalize\n",
    "    denoised_spectrogram = inverse_scale(denoised_spectrogram, scalers)\n",
    "    denoised_spectrogram = librosa.db_to_amplitude(denoised_spectrogram)\n",
    "    \n",
    "    output_chunk = magphase_to_waveform(denoised_spectrogram, features['phase'], chunk_samples)\n",
    "    \n",
    "    return output_chunk\n",
    "\n",
    "class AudioDenoiser:\n",
    "    def __init__(self, model_one, scalers, output_path, sample_rate=44100, chunk_duration=2, step_size=0.5, device=None):\n",
    "        \"\"\"\n",
    "        Audio Denoising Pipeline using AI model.\n",
    "\n",
    "        Parameters:\n",
    "            model (torch.nn.Module): AI model for denoising.\n",
    "            output_path (str): Directory to save output files.\n",
    "            sample_rate (int): Sample rate (default 44100 Hz).\n",
    "            chunk_duration (int): Duration of each chunk in seconds.\n",
    "            step_size (float): Step size for overlap-add in seconds.\n",
    "            device (str, optional): Device for PyTorch computation (\"cuda\" or \"cpu\").\n",
    "        \"\"\"\n",
    "        self.model_one = model_one\n",
    "        #self.model_two = model\n",
    "        self.output_path = output_path\n",
    "        self.sample_rate = sample_rate\n",
    "        self.chunk_samples = sample_rate * chunk_duration\n",
    "        self.scalers = scalers\n",
    "        self.step_samples = int(self.chunk_samples * step_size)\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_one.to(self.device)\n",
    "        self.model_one.eval()\n",
    "        #self.model_two.to(self.device)\n",
    "        #self.model_two.eval()\n",
    "\n",
    "    def process_audio(self, waveform, sr):\n",
    "        \"\"\"\n",
    "        Processes audio by adding noise, chunking, denoising, and reconstructing.\n",
    "\n",
    "        Parameters:\n",
    "            input_path (str): Path to the input song file.\n",
    "            noise_path (str): Path to the noise file.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (reconstructed_audio, reconstructed_audio_input).\n",
    "        \"\"\"\n",
    "        # Load audio\n",
    "        self.waveform = waveform\n",
    "        assert sr == self.sample_rate, f\"Sample rate mismatch: expected {self.sample_rate}, got {sr}\"\n",
    "\n",
    "        # Process in chunks\n",
    "        processed_audio, processed_input = [], []\n",
    "        for start in range(0, len(waveform) - self.chunk_samples + 1, self.step_samples):\n",
    "            input_chunk = waveform[start:start + self.chunk_samples]\n",
    "\n",
    "            # --- start denoising\n",
    "            output_chunk = denoise_audio_chunk(input_chunk, sr, self.model_one, self.scalers, self.chunk_samples, self.device)\n",
    "            #output_chunk = denoise_audio_chunk(output_chunk, sr, self.model_two, self.scalers, self.chunk_samples, self.device)\n",
    "\n",
    "            processed_input.append(input_chunk)\n",
    "            processed_audio.append(output_chunk)\n",
    "\n",
    "        # Reconstruct waveform with overlap-add\n",
    "        reconstructed_audio = self._overlap_add(processed_audio)\n",
    "        reconstructed_audio_input = waveform #self._overlap_add(processed_input)\n",
    "\n",
    "        # Save output\n",
    "        self._save_audio(reconstructed_audio, \"output_audio_song.wav\")\n",
    "        self._save_audio(reconstructed_audio_input, \"input_audio_song.wav\")\n",
    "\n",
    "        # Plot spectrograms\n",
    "        self._plot_spectrograms(reconstructed_audio, reconstructed_audio_input)\n",
    "\n",
    "        return reconstructed_audio, reconstructed_audio_input\n",
    "\n",
    "    def _overlap_add(self, chunks):\n",
    "        \"\"\"Reconstructs the waveform using overlap-add method.\"\"\"\n",
    "        reconstructed = np.zeros(len(self.waveform))\n",
    "        weight = np.zeros(len(self.waveform))\n",
    "\n",
    "        for i, start in enumerate(range(0, len(self.waveform) - self.chunk_samples + 1, self.step_samples)):\n",
    "            reconstructed[start:start + self.chunk_samples] += chunks[i]\n",
    "            weight[start:start + self.chunk_samples] += np.hanning(self.chunk_samples)\n",
    "\n",
    "        reconstructed /= np.maximum(weight, 1e-6)\n",
    "        reconstructed = np.clip(reconstructed, -1, 1)\n",
    "\n",
    "        fade_in = int(self.sample_rate / 2)\n",
    "        reconstructed[:fade_in] *= np.hanning(self.sample_rate)[:fade_in]\n",
    "        reconstructed[-fade_in:] *= np.hanning(self.sample_rate)[-fade_in:]\n",
    "\n",
    "        return reconstructed\n",
    "\n",
    "    def _save_audio(self, audio, filename):\n",
    "        \"\"\"Saves the audio file.\"\"\"\n",
    "        output_filename = os.path.join(self.output_path, add_datetime_to_filename(filename))\n",
    "        sf.write(output_filename, audio / np.max(audio), self.sample_rate)\n",
    "        print(f\"Saved: {output_filename}\")\n",
    "\n",
    "    def _plot_spectrograms(self, reconstructed_audio, reconstructed_audio_input):\n",
    "        \"\"\"Plots spectrograms of processed and input audio with consistent color scale.\"\"\"\n",
    "\n",
    "        import librosa\n",
    "        import librosa.display\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # Compute Mel spectrograms\n",
    "        Sxx1 = librosa.feature.melspectrogram(y=reconstructed_audio, sr=self.sample_rate, n_fft=2048, hop_length=1024)\n",
    "        Sxx2 = librosa.feature.melspectrogram(y=reconstructed_audio_input, sr=self.sample_rate, n_fft=2048, hop_length=1024)\n",
    "\n",
    "        # Convert to log scale (dB)\n",
    "        Sxx1_db = librosa.amplitude_to_db(Sxx1, ref=np.max)\n",
    "        Sxx2_db = librosa.amplitude_to_db(Sxx2, ref=np.max)\n",
    "\n",
    "        # Compute shared color limits\n",
    "        vmin, vmax = min(Sxx1_db.min(), Sxx2_db.min()), max(Sxx1_db.max(), Sxx2_db.max())\n",
    "\n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "\n",
    "        # Plot processed audio spectrogram\n",
    "        img1 = librosa.display.specshow(Sxx1_db, sr=self.sample_rate, hop_length=1024, cmap=\"viridis\", ax=axes[0], vmin=vmin, vmax=vmax)\n",
    "        axes[0].set_title(\"Spectrogram of Processed Audio\")\n",
    "        axes[0].set_xlabel(\"Time (s)\")\n",
    "        axes[0].set_ylabel(\"Frequency (Hz)\")\n",
    "\n",
    "        # Plot input spectrogram\n",
    "        img2 = librosa.display.specshow(Sxx2_db, sr=self.sample_rate, hop_length=1024, cmap=\"viridis\", ax=axes[1], vmin=vmin, vmax=vmax)\n",
    "        axes[1].set_title(\"Spectrogram of Input Audio\")\n",
    "        axes[1].set_xlabel(\"Time (s)\")\n",
    "\n",
    "        # Add shared colorbar\n",
    "        fig.colorbar(img1, ax=axes, orientation=\"vertical\", fraction=0.02, pad=0.02, label=\"Amplitude (dB)\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def resample_feature(feature, target_shape):\n",
    "    \"\"\"Resamples a 2D numpy feature array to match target shape using torch.nn.functional.interpolate.\"\"\"\n",
    "    feature_tensor = torch.tensor(feature, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, H, W)\n",
    "    target_size = (target_shape[0], target_shape[1])  # (new_H, new_W)\n",
    "\n",
    "    resized_feature = F.interpolate(feature_tensor, size=target_size, mode=\"bilinear\", align_corners=False)\n",
    "    return resized_feature.squeeze(0).squeeze(0).numpy()  # Remove batch/channel dim and return as numpy\n",
    "\n",
    "def transform_features(features, scalers):\n",
    "    input_spectrogram = features['spectrogram']\n",
    "    input_edges = features['edges']\n",
    "    input_cepstrum = features['cepstrum']\n",
    "\n",
    "    # function to transform the extracted features to an input to the\n",
    "    target_shape = input_spectrogram.shape\n",
    "    # Apply scalers\n",
    "        #input_phase = self.scalers[\"input_features_phase\"].transform(input_phase.reshape(1, -1)).reshape(input_phase.shape)\n",
    "    input_spectrogram = scalers[\"input_features_spectrogram\"].transform(input_spectrogram.reshape(1, -1)).reshape(input_spectrogram.shape)\n",
    "    input_edges = scalers[\"input_features_edges\"].transform(input_edges.reshape(1, -1)).reshape(input_edges.shape)\n",
    "    input_cepstrum = scalers[\"input_features_cepstrum\"].transform(input_cepstrum.reshape(1, -1)).reshape(input_cepstrum.shape)\n",
    "    #input_cepstrum_edges = self.scalers[\"input_features_cepstrum_edges\"].transform(input_cepstrum_edges.reshape(1, -1)).reshape(input_cepstrum_edges.shape)\n",
    "\n",
    "    # resample mfcc featues so theyre the same shape as the spectrogram and phase features\n",
    "    # Define frequency bins\n",
    "    sampling_rate = 44100  # 44.1 kHz audio\n",
    "    n_fft = 2048  # Adjust this for better resolution\n",
    "    freqs = np.linspace(0, sampling_rate / 2, n_fft // 2 + 1)  # STFT frequency bins\n",
    "\n",
    "    # Find indices corresponding to 0–4000 Hz\n",
    "    min_freq, hf, mf, lf = 0, 4000, 1000, 200\n",
    "    freq_indices_hf = np.where((freqs >= min_freq) & (freqs <= hf))[0]\n",
    "    freq_indices_mf = np.where((freqs >= min_freq) & (freqs <= mf))[0]\n",
    "    freq_indices_lf = np.where((freqs >= min_freq) & (freqs <= lf))[0]\n",
    "    # input spectrogram\n",
    "    input_spectrogram_hf = resample_feature(input_spectrogram[freq_indices_hf, :], target_shape)\n",
    "    input_spectrogram_mf = resample_feature(input_spectrogram[freq_indices_mf, :], target_shape)\n",
    "    input_spectrogram_lf = resample_feature(input_spectrogram[freq_indices_lf, :], target_shape)\n",
    "    # edges\n",
    "    input_edges_hf = resample_feature(input_edges[freq_indices_hf, :], target_shape)\n",
    "    input_edges_mf = resample_feature(input_edges[freq_indices_mf, :], target_shape)\n",
    "    input_edges_lf = resample_feature(input_edges[freq_indices_lf, :], target_shape)\n",
    "\n",
    "    # now input indices for 0-1000 and 0-200 to add as channels and as freq_indicies for reconstruction\n",
    "\n",
    "    # Resample MFCC features\n",
    "    input_cepstrum = resample_feature(input_cepstrum, target_shape)\n",
    "\n",
    "    # Convert to tensors - input_phase, is missing,..... it's too confusing\n",
    "    inputs = torch.tensor(np.stack([\n",
    "        input_spectrogram, input_spectrogram_hf, input_spectrogram_mf, input_spectrogram_lf,\n",
    "        input_edges, input_edges_hf, input_edges_mf, input_edges_lf,\n",
    "        input_cepstrum\n",
    "    ], axis=0), dtype=torch.float32)  # Shape: (6, H, W)\n",
    "\n",
    "    a = 3\n",
    "    inputs = (inputs/a) + 0.5\n",
    "\n",
    "    # metadata\n",
    "    # Extract metadata\n",
    "    metadata = {\n",
    "        \"hf_shape\": input_spectrogram[freq_indices_hf, :].shape,\n",
    "        \"mf_shape\": input_spectrogram[freq_indices_mf, :].shape,\n",
    "        \"lf_shape\": input_spectrogram[freq_indices_lf, :].shape,\n",
    "        \"freq_indices_hf\": freq_indices_hf,\n",
    "        \"freq_indices_mf\": freq_indices_mf,\n",
    "        \"freq_indices_lf\": freq_indices_lf\n",
    "    }\n",
    "\n",
    "    return inputs, metadata\n",
    "\n",
    "def reconstruct_spectrogram(outputs, metadata):\n",
    "    # lets evaluate this from a l1 loss perspective\n",
    "    # reconstruct spectrogram\n",
    "    out_spectrogram = np.array(outputs[0])\n",
    "    out_spectrogram[metadata[\"freq_indices_hf\"], :] = resample_feature(outputs[1], metadata[\"hf_shape\"])\n",
    "    out_spectrogram[metadata[\"freq_indices_mf\"], :] = resample_feature(outputs[2], metadata[\"mf_shape\"])\n",
    "    out_spectrogram[metadata[\"freq_indices_lf\"], :] = resample_feature(outputs[3], metadata[\"lf_shape\"])\n",
    "    return out_spectrogram\n",
    "\n",
    "def inverse_scale(out_spectrogram, scalers):\n",
    "    # inverse scale the\n",
    "    # transform back to 0 centred and\n",
    "    out_spectrogram = (out_spectrogram - 0.5) * 3\n",
    "    out_spec_shape = out_spectrogram.shape\n",
    "\n",
    "    # undo scaler\n",
    "    out_spectrogram = scalers[\"input_features_spectrogram\"].inverse_transform(np.array([out_spectrogram]).reshape(1, -1)).reshape(out_spec_shape)\n",
    "    return out_spectrogram\n",
    "\n",
    "def threshold_spectrogram(spectrogram, threshold, percentage=0.8):\n",
    "    \"\"\"\n",
    "    Zeroes out all values in the spectrogram that are below the given threshold.\n",
    "\n",
    "    Args:\n",
    "        spectrogram (np.ndarray): Input 2D array.\n",
    "        threshold (float): The threshold value.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The processed spectrogram with values below threshold set to zero.\n",
    "    \"\"\"\n",
    "    spectrogram = np.where(spectrogram >= threshold * percentage, spectrogram, 0)\n",
    "    return spectrogram\n",
    "\n",
    "def magphase_to_waveform(magnitude, phase, audio_length=44100):\n",
    "    \"\"\"\n",
    "    Converts a spectrogram image back into an audio waveform.\n",
    "\n",
    "    Parameters:\n",
    "        image (np.array): Spectrogram image (3 channels).\n",
    "        sr (int): Sampling rate.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Reconstructed audio waveform.\n",
    "    \"\"\"\n",
    "    stft = magnitude * np.exp(1j * phase)\n",
    "    return librosa.istft(stft, length=audio_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_number = 25\n",
    "noisy_audio, sr = generate_audio_with_noise(song_files[file_number], noise_file, start_time=20, duration=10, noise_level=0.7)\n",
    "denoiser = AudioDenoiser(model_one, scalers, output_path=output_path, chunk_duration=2, step_size=0.5)\n",
    "reconstructed_audio, reconstructed_input = denoiser.process_audio(noisy_audio, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from google.colab import files\n",
    "\n",
    "def save_wav_sf(file_path, audio_array, sample_rate):\n",
    "    \"\"\"Saves a NumPy array as a WAV file using soundfile.\"\"\"\n",
    "    sf.write(file_path, audio_array, sample_rate, subtype=\"PCM_16\")  # Can be PCM_24, PCM_32, FLOAT\n",
    "    files.download(file_path)  # Trigger download in Colab\n",
    "\n",
    "save_wav_sf(f\"output_{file_number}.wav\", reconstructed_audio, sr)\n",
    "save_wav_sf(f\"input_{file_number}.wav\", reconstructed_input, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
