{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8SELAW0r3K8"
      },
      "source": [
        "### Idea for denoising archetecture\n",
        "0. Option: Add noise during training\n",
        "1. Add attention on skip connections\n",
        "2. Larger models need more time to train\n",
        "3. Old model used average pooling (try max pooling)\n",
        "4. Replace phase feature with MFCC or another useful feature e.g level * frequency\n",
        "  a. edge detection\n",
        "\n",
        "\n",
        "## 1. Using a Laplacian Filter (Edge Detection)\n",
        "```python\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Apply the Laplacian filter\n",
        "filtered_spectrogram = cv2.Laplacian(spectrogram, cv2.CV_64F, ksize=3)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And either use MFCC, or Cepstrum"
      ],
      "metadata": {
        "id": "pk0VVeyChttz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1B6SS0YdX-xs",
        "outputId": "1cb0c9eb-ec81-4171-c8e8-970d03e559b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE8QDLOXuiE7"
      },
      "source": [
        "Environmental Sound Classification 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ2cUzDwQQmi",
        "outputId": "877fc7b3-05a3-4570-abf0-5171a597bd7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab...\n"
          ]
        }
      ],
      "source": [
        "# Detect Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    print(\"Running in Google Colab...\")\n",
        "    os.system(\"git clone https://github.com/CiaranMaloy/audioautoencoder\")\n",
        "    os.chdir(\"/content/audioautoencoder/\")\n",
        "    os.system(\"git pull\")\n",
        "    os.system(\"git checkout bandchannels\")\n",
        "    os.system(\"git pull origin bandchannels\")\n",
        "    #os.system(\"pip install --upgrade torchmetrics\")\n",
        "else:\n",
        "    print(\"Running locally...\")\n",
        "    os.system(\"git pull origin bandchannels\")\n",
        "    os.system(\"pip install --upgrade torchmetrics\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "av62pZLzSUjo"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/audioautoencoder')\n",
        "sys.path.append('/content/audioautoencoder/audioautoencoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJxpYCvxUBZZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "N23OLmvVQr6V"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.data import *\n",
        "from audioautoencoder.outputs import *\n",
        "from audioautoencoder.processing import *\n",
        "from audioautoencoder.training import *\n",
        "from audioautoencoder.datasets.loaders import *\n",
        "from audioautoencoder.generate_dataset import *\n",
        "from audioautoencoder.plotting import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBX7YjnSJpLC"
      },
      "source": [
        "test get item input and output to see if i can use the log magnitude and then reverse it on the output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Niz7eKR1xy18"
      },
      "source": [
        "## Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "4jLmXnwg8YL-"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.data_management import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "qIB1M7xDLIFR"
      },
      "outputs": [],
      "source": [
        "GENERATE=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "vHikpu86MO3s"
      },
      "outputs": [],
      "source": [
        "# Example Usage\n",
        "if GENERATE:\n",
        "  dataset_dirs = [\"/content/drive/MyDrive/Datasets/Noise/All_Noise\"]\n",
        "  output_dir = \"/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2\"\n",
        "  splits = create_datasets(dataset_dirs, output_dir)\n",
        "  print(\"Training Set:\", len(splits[\"train\"]))\n",
        "  print(\"Validation Set:\", len(splits[\"val\"]))\n",
        "  print(\"Testing Set:\", len(splits[\"test\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "_C4AGBiBPsQ8"
      },
      "outputs": [],
      "source": [
        "if GENERATE:\n",
        "  save_splits_to_directories(splits, output_dir, max_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "KT9U_mkQtxzx"
      },
      "outputs": [],
      "source": [
        "# generate audio files for noise and music (2s)\n",
        "if GENERATE:\n",
        "  noise_test = output_dir + \"/test\"\n",
        "  noise_train = output_dir + \"/train\"\n",
        "\n",
        "  noise_test_output = noise_test + \"-2s-44100\"\n",
        "  noise_train_output = noise_train + \"-2s-44100\"\n",
        "\n",
        "  for input_path, output_path in [(noise_test, noise_test_output), (noise_train, noise_train_output)]:\n",
        "    print(input_path, output_path)\n",
        "    generate_audio_files(input_path, output_path, t=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "wf3uGYyGwS0S"
      },
      "outputs": [],
      "source": [
        "# generate audio files for noise and music (2s)\n",
        "if False:\n",
        "  music_test = \"/content/drive/MyDrive/Datasets/Music/MUSDB18/test\"\n",
        "  music_train = \"/content/drive/MyDrive/Datasets/Music/MUSDB18/train\"\n",
        "\n",
        "  music_test_output = music_test + \"-2s-44100\"\n",
        "  music_train_output = music_train + \"-2s-44100\"\n",
        "\n",
        "  for input_path, output_path in [(music_test, music_test_output), (music_train, music_train_output)]:\n",
        "    print(input_path, output_path)\n",
        "    generate_audio_files(input_path, output_path, t=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxxKjl_JOAKE"
      },
      "source": [
        "## Process files to H5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "X82E9Uf6WdX_"
      },
      "outputs": [],
      "source": [
        "GENERATE_H5_FILES = False\n",
        "\n",
        "if GENERATE_H5_FILES:\n",
        "  checkpoint_file_size=50000\n",
        "  processor = DatasetProcessor(\n",
        "          train_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/train-2s-44100',\n",
        "          train_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/train-2s-44100',\n",
        "          test_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/test-2s-44100',\n",
        "          test_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test-2s-44100',\n",
        "          output_dir='/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_sep_features',\n",
        "          SNRdB=[-10, 10],\n",
        "          process_train=True,\n",
        "          process_test=True,\n",
        "          checkpoint_file_size=checkpoint_file_size\n",
        "      )\n",
        "  processor.process()\n",
        "\n",
        "  processor = DatasetProcessor(\n",
        "          train_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/train-2s-44100',\n",
        "          train_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/train-2s-44100',\n",
        "          test_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/test-2s-44100',\n",
        "          test_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test-2s-44100',\n",
        "          output_dir='/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_sep_features',\n",
        "          SNRdB=[0, 20],\n",
        "          process_train=True,\n",
        "          process_test=True,\n",
        "          checkpoint_file_size=checkpoint_file_size\n",
        "      )\n",
        "  processor.process()\n",
        "\n",
        "  processor = DatasetProcessor(\n",
        "          train_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/train-2s-44100',\n",
        "          train_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/train-2s-44100',\n",
        "          test_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/test-2s-44100',\n",
        "          test_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test-2s-44100',\n",
        "          output_dir='/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_sep_features',\n",
        "          SNRdB=[10, 30],\n",
        "          process_train=True,\n",
        "          process_test=True,\n",
        "          checkpoint_file_size=checkpoint_file_size\n",
        "      )\n",
        "  processor.process()\n",
        "\n",
        "  processor = DatasetProcessor(\n",
        "          train_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/train-2s-44100',\n",
        "          train_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/train-2s-44100',\n",
        "          test_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/test-2s-44100',\n",
        "          test_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test-2s-44100',\n",
        "          output_dir='/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_mix_features',\n",
        "          SNRdB=[-10, 30],\n",
        "          process_train=True,\n",
        "          process_test=True,\n",
        "          mix_only=True,\n",
        "          checkpoint_file_size=checkpoint_file_size\n",
        "      )\n",
        "  processor.process()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2m4ztn1qxC3"
      },
      "source": [
        "## Define Autoencoder structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep3iZRJC0tJi"
      },
      "source": [
        "## Improvement to UnetAutoencoder\n",
        "\n",
        "1. changed padding from explicit to on the output of the convolutional layer\n",
        "2. added batch normalisation between layers\n",
        "3. changed fro elu to leaky_relu for efficiency\n",
        "4. in the other one there is no pooling... and the skip connections are done by concatenation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOXAuPserTGv"
      },
      "source": [
        "#### For v3, remove pooling layers, and add attention onto skip connections\n",
        "\n",
        "In addition, try only music and only crowd as a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "zyXtMpe1PvID"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# too big without.... lets try, halfing the expansion\n",
        "# apternaitvely use max pooling instead of average pooling\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)  # Avg Pooling\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)  # Max Pooling\n",
        "        attn = self.conv(torch.cat([avg_out, max_out], dim=1))  # Convolution\n",
        "        return x * self.sigmoid(attn)  # Apply Attention Map\n",
        "\n",
        "class UNetConv6(nn.Module):\n",
        "    # Update from UnetConv3, stride dimentionality reduction + attention on skip connections\n",
        "    def __init__(self, in_channels=9, out_channels=4):\n",
        "        super(UNetConv6, self).__init__()\n",
        "\n",
        "        a = 2\n",
        "        A, B, C, D = 64, 128, 256, 512\n",
        "        bottleneck_channels = 1024\n",
        "\n",
        "        # Encoder (Downsampling)\n",
        "        enc_channels = [in_channels, A, B, C, D]\n",
        "        self.enc1 = self.conv_block(enc_channels[0], enc_channels[1], 7, 1)\n",
        "        self.enc2 = self.conv_block(enc_channels[1], enc_channels[2], 5, 2)\n",
        "        self.enc3 = self.conv_block(enc_channels[2], enc_channels[3], 3, 2)\n",
        "        self.enc4 = self.conv_block(enc_channels[3], enc_channels[4], 3, 2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(enc_channels[4], bottleneck_channels, 3, 2)\n",
        "\n",
        "        # Decoder (Upsampling)\n",
        "        dec_channels = [bottleneck_channels, D, C, B, A]\n",
        "        self.dec4 = self.upconv_block(dec_channels[0], dec_channels[1], 3, 2)\n",
        "        self.dec3 = self.upconv_block(dec_channels[1] + enc_channels[4], dec_channels[2], 3, 2)\n",
        "        self.dec2 = self.upconv_block(dec_channels[2] + enc_channels[3], dec_channels[3], 5, 2)\n",
        "        self.dec1 = self.upconv_block(dec_channels[3] + enc_channels[2], dec_channels[4], 7, 1)\n",
        "\n",
        "        # Final Output Layer\n",
        "        self.final = nn.Conv2d(dec_channels[4] + enc_channels[1], out_channels, kernel_size=3, padding=1, stride=1)\n",
        "\n",
        "        # Initialize Spatial Attention Modules\n",
        "        self.spatial_attn4 = SpatialAttention()\n",
        "        self.spatial_attn3 = SpatialAttention()\n",
        "        self.spatial_attn2 = SpatialAttention()\n",
        "        self.spatial_attn1 = SpatialAttention()\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels, kernel_size, stride, dropout=0.2):\n",
        "        \"\"\"Convolutional Block with Dropout in Deeper Layers Only\"\"\"\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1, stride=stride), # In the next iteration i should introduce a stride here\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        ]\n",
        "\n",
        "        # Dropout only for deeper encoder layers\n",
        "        if out_channels >= 256:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def upconv_block(self, in_channels, out_channels, kernel_size, stride, dropout=0.2):\n",
        "        \"\"\"Upsampling Block with Dropout in First Few Decoder Layers\"\"\"\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        ]\n",
        "\n",
        "        # Dropout only for first few decoder layers\n",
        "        if in_channels >= 256:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass with skip connections\"\"\"\n",
        "        # Encoding\n",
        "        e1 = self.enc1(x)  # (batch, 64, 1028, 175)\n",
        "        e2 = self.enc2(e1) # (batch, 128, 514, 87)\n",
        "        e3 = self.enc3(e2)  # (batch, 256, 257, 43)\n",
        "        e4 = self.enc4(e3) # (batch, 512, 128, 21)\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(e4)  # (batch, 1024, 64, 10)\n",
        "\n",
        "        # Decoding + Skip Connections with Spatial Attention\n",
        "        d4 = self.dec4(b)  # (batch, 512, ?, ?)\n",
        "        d4 = F.interpolate(d4, size=e4.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "        e4_attn = self.spatial_attn4(e4)  # Apply Spatial Attention\n",
        "        d4 = torch.cat([d4, e4_attn], dim=1)\n",
        "\n",
        "        d3 = self.dec3(d4)  # (batch, 256, ?, ?)\n",
        "        d3 = F.interpolate(d3, size=e3.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "        e3_attn = self.spatial_attn3(e3)  # Apply Spatial Attention\n",
        "        d3 = torch.cat([d3, e3_attn], dim=1)\n",
        "\n",
        "        d2 = self.dec2(d3)  # (batch, 128, ?, ?)\n",
        "        d2 = F.interpolate(d2, size=e2.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "        e2_attn = self.spatial_attn2(e2)  # Apply Spatial Attention\n",
        "        d2 = torch.cat([d2, e2_attn], dim=1)\n",
        "\n",
        "        d1 = self.dec1(d2)  # (batch, 64, ?, ?)\n",
        "        d1 = F.interpolate(d1, size=e1.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "        e1_attn = self.spatial_attn1(e1)  # Apply Spatial Attention\n",
        "        d1 = torch.cat([d1, e1_attn], dim=1)\n",
        "\n",
        "        # Final Convolution (output denoised spectrogram)\n",
        "        return F.interpolate(self.final(d1), size=(1025, 175), mode=\"bilinear\", align_corners=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4\n",
        "TEST_MODEL = False"
      ],
      "metadata": {
        "id": "dwpBexNA9X0Q"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "xgzwlHooHoiE"
      },
      "outputs": [],
      "source": [
        "if TEST_MODEL:\n",
        "  if __name__ == \"__main__\":\n",
        "      x = torch.randn((BATCH_SIZE, 5, 1025, 175))\n",
        "      model = UNetConv5()\n",
        "      output = model(x)\n",
        "\n",
        "      print('output....')\n",
        "      print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "zB2SGHy6BEby"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "\n",
        "class TestAutoencoder(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.model = UNetConv5()\n",
        "        self.input_channels = 5\n",
        "        self.output_channels = 2\n",
        "        self.input_height = 1025\n",
        "        self.input_width = 175\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def test_model_initialization(self):\n",
        "        self.assertIsInstance(self.model, UNetConv5, \"Model initialization failed\")\n",
        "\n",
        "    def test_forward_pass(self):\n",
        "        x = torch.randn(self.batch_size, self.input_channels, self.input_height, self.input_width, device=self.device)\n",
        "        output = self.model(x)\n",
        "        self.assertEqual(\n",
        "            output.shape,\n",
        "            (self.batch_size, self.output_channels, self.input_height, self.input_width),\n",
        "            f\"Expected output shape {(self.batch_size, self.output_channels, self.input_height, self.input_width)}, but got {output.shape}\"\n",
        "        )\n",
        "\n",
        "    def test_model_summary(self):\n",
        "        try:\n",
        "            summary(self.model, input_size=(self.input_channels, self.input_height, self.input_width))\n",
        "        except Exception as e:\n",
        "            self.fail(f\"Model summary failed: {str(e)}\")\n",
        "\n",
        "# This allows running tests externally\n",
        "def suite():\n",
        "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestAutoencoder)\n",
        "    return test_suite\n",
        "\n",
        "# runner\n",
        "class TestRunner:\n",
        "    def __init__(self):\n",
        "        self.runner = unittest.TextTestRunner()\n",
        "\n",
        "    def run(self):\n",
        "        print(\"Running autoencoder tests...\")\n",
        "        self.runner.run(suite())\n",
        "\n",
        "if TEST_MODEL:\n",
        "  if __name__ == \"__main__\":\n",
        "      runner = TestRunner()\n",
        "      runner.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihXSxknjq_Ks"
      },
      "source": [
        "## First train as an autoencoder for music"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q-mqM2myd0F"
      },
      "source": [
        "## Download file to local"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from audioautoencoder.models.UNetConv4 import UNetConv4"
      ],
      "metadata": {
        "id": "HW0x4Jh1sZsP"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "hNOpNkUFSL3C"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.plotting import *\n",
        "from audioautoencoder.datasets.utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzePP5Urw927",
        "outputId": "41f16432-1441-45a5-f811-c4d31fff96fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr: 0.001\n",
            "SNRdB: [-10, 10]\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "train = False\n",
        "LOAD_DATA = False\n",
        "\n",
        "# --------------- Main Execution parameters ---------------\n",
        "model_name = 'UNetConv6'\n",
        "SNRdB_load = [-10, 10]\n",
        "SNRdBs = [[-10, 10]] # SNR random range\n",
        "load_trigger = [True]\n",
        "load_file = 'Autoencodermodel_earlystopping.pth'\n",
        "#load_file = 'Autoencodermodel_checkpoint.pth'\n",
        "\n",
        "folder = ['sep_features'][i] # sep\n",
        "\n",
        "# parameters\n",
        "learning_rates = [1e-3] # 1e-4 for re0training?, 1e-3 for training?\n",
        "\n",
        "base_lr=1e-5\n",
        "max_lr=learning_rates[i]\n",
        "gamma=0.8\n",
        "\n",
        "# data params\n",
        "max_file_size_gb = 100\n",
        "IMPORT_TRAIN_NOISY = train\n",
        "batch_size = 32\n",
        "\n",
        "# training params\n",
        "load = load_trigger[i]\n",
        "warm_start = False\n",
        "epochs = 100\n",
        "accumulation_steps = int(512/batch_size)\n",
        "\n",
        "SNRdB = SNRdBs[i]\n",
        "learning_rate = learning_rates[i]\n",
        "eta_min = 1e-5\n",
        "\n",
        "print('lr:', learning_rate)\n",
        "print('SNRdB:', SNRdB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "7DKs1xyWxBdX"
      },
      "outputs": [],
      "source": [
        "# --------------- In Loop Parameters --------------\n",
        "output_path = f'/content/drive/MyDrive/Projects/ML_Projects/De-noising-autoencoder/Models_Denoising/Checkpoints_{model_name}_{SNRdB[0]}-{SNRdB[1]}/'\n",
        "load_path = f'/content/drive/MyDrive/Projects/ML_Projects/De-noising-autoencoder/Models_Denoising/Checkpoints_{model_name}_{SNRdB_load[0]}-{SNRdB_load[1]}/{load_file}'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import joblib  # or use pickle if you prefer\n",
        "\n",
        "def save_scalers(scalers, save_path):\n",
        "    \"\"\"Save scalers to a file.\"\"\"\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    joblib.dump(scalers, save_path)\n",
        "\n",
        "def load_scalers(save_path):\n",
        "    \"\"\"Load scalers from a file.\"\"\"\n",
        "    return joblib.load(save_path)"
      ],
      "metadata": {
        "id": "ISQ1XFZQuV2z"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "5MW2Gsna7ku_"
      },
      "outputs": [],
      "source": [
        "# Define the source and destination file paths\n",
        "if LOAD_DATA:\n",
        "  scaler_file = output_path + \"scalers.pkl\"  # Static filename since it's unique per run\n",
        "  source_folder = f\"/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_{folder}/SNRdB_{SNRdB[0]}-{SNRdB[1]}/\"\n",
        "  source_path = source_folder + \"train/\"\n",
        "  destination_path = f\"/content/SNRdB_{SNRdB[0]}-{SNRdB[1]}/train/\"\n",
        "  save_path = source_folder + \"combined_000.h5\"\n",
        "  subset = False\n",
        "\n",
        "  if IMPORT_TRAIN_NOISY:\n",
        "    dataset_path = f\"/content/SNRdB_{SNRdB[0]}-{SNRdB[1]}/train/combined_000.h5\"\n",
        "    if not os.path.exists(destination_path):\n",
        "      combine_h5_files_features(source_path, destination_path, max_file_size_gb=max_file_size_gb)\n",
        "\n",
        "    if os.path.exists(scaler_file):\n",
        "        print(\"Loading existing scalers...\")\n",
        "        scalers = load_scalers(scaler_file)\n",
        "    else:\n",
        "        print(\"Training new scalers...\")\n",
        "        scalers = train_scalers(dataset_path, sample_size=8000)\n",
        "        save_scalers(scalers, scaler_file)\n",
        "\n",
        "    print(scalers)\n",
        "\n",
        "    train_loader = ChannelDatasetLoader(\n",
        "          dataset_path=dataset_path,\n",
        "          scalers=scalers,\n",
        "          output_time_length=175,\n",
        "          channels=1,\n",
        "          snr_db=SNRdB,\n",
        "          subset=subset,\n",
        "          batch_size=batch_size\n",
        "      )\n",
        "\n",
        "    print(f\"Training set size: {len(train_loader.train_dataset)}\")\n",
        "    print(f\"Validation set size: {len(train_loader.val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_DATA:\n",
        "  input, output, medatata = train_loader.train_dataset[200]\n",
        "  print(input.shape)\n",
        "  print(output.shape)"
      ],
      "metadata": {
        "id": "l6-sIxJ-qEDC"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_DATA:\n",
        "  import matplotlib.pyplot as plt\n",
        "  import torch\n",
        "\n",
        "  # Fetch a sample\n",
        "  input_tensor, output_tensor, metadata = train_loader.train_dataset[50]\n",
        "\n",
        "  # Convert to NumPy for plotting\n",
        "  input_array = input_tensor.numpy()\n",
        "  output_array = output_tensor.numpy()\n",
        "\n",
        "  # Number of input channels\n",
        "  num_channels_in = input_array.shape[0]\n",
        "  num_channels_out = output_array.shape[0]\n",
        "\n",
        "  # Create subplots\n",
        "  fig, axes = plt.subplots(num_channels_in + num_channels_out, 1, figsize=(15, 30))\n",
        "\n",
        "  # Plot each input channel\n",
        "  for i in range(num_channels_in):\n",
        "      input = input_array[i]\n",
        "      print('Min, Max: ', np.min(input), np.max(input))\n",
        "      im = axes[i].imshow(input, aspect='auto', cmap='magma')\n",
        "      axes[i].invert_yaxis()\n",
        "\n",
        "      axes[i].set_title(f\"Input Channel {i+1}\")\n",
        "      axes[i].axis(\"off\")\n",
        "\n",
        "      # Add colorbar\n",
        "      cbar = fig.colorbar(im, ax=axes[i], orientation=\"vertical\")\n",
        "      cbar.set_label(\"Amplitude\")\n",
        "\n",
        "    # Plot each input channel\n",
        "  for i in range(num_channels_out):\n",
        "      output = output_array[i]\n",
        "      print('Min, Max: ', np.min(output), np.max(output))\n",
        "      im = axes[num_channels_in + i].imshow(output, aspect='auto', cmap='magma')\n",
        "      axes[num_channels_in + i].invert_yaxis()\n",
        "\n",
        "      axes[num_channels_in + i].set_title(f\"Output Channel {i+1}\")\n",
        "      axes[num_channels_in + i].axis(\"off\")\n",
        "\n",
        "      # Add colorbar\n",
        "      cbar = fig.colorbar(im, ax=axes[num_channels_in + i], orientation=\"vertical\")\n",
        "      cbar.set_label(\"Amplitude\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "YO8_4lXBtXim"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsVtpS6ksh_9"
      },
      "source": [
        "# Retrain Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "MNVGzk5jK856"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.loss import *\n",
        "from audioautoencoder.utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "v7M1VlCI5WHr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "R2BLTePNsRoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99be30e-9b06-405e-adc8-239f45a6731e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the model, define loss function and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNetConv6().to(device)\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "SGiVzAgVD0Ra"
      },
      "outputs": [],
      "source": [
        "if load:\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "  #scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=eta_min)\n",
        "  scheduler_loss = False\n",
        "else:\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=eta_min)\n",
        "  scheduler_loss = False\n",
        "\n",
        "  #optimizer = None #torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "  #scheduler = None #torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
        "  #scheduler_loss = False #True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clears all allocated GPU memory in PyTorch.\"\"\"\n",
        "    torch.cuda.empty_cache()  # Clears cache\n",
        "    gc.collect()  # Runs Python garbage collector\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        torch.cuda.reset_peak_memory_stats(i)  # Resets peak memory tracking\n",
        "\n",
        "clear_gpu_memory()\n"
      ],
      "metadata": {
        "id": "6pxw01Uptmvj"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "tv3swntPn-kj"
      },
      "outputs": [],
      "source": [
        "if train:\n",
        "  trainer = DenoisingTrainer(\n",
        "      model=model, noisy_train_loader=train_loader.train_loader, noisy_val_loader=train_loader.val_loader,\n",
        "      SNRdB=SNRdB, output_path=output_path, epochs=epochs, learning_rate=learning_rate,\n",
        "      load=load, warm_start=warm_start, train=train, verbose=False, accumulation_steps=accumulation_steps, load_path=load_path,\n",
        "      base_lr=base_lr, max_lr=max_lr, gamma=gamma, optimizer=optimizer, scheduler=scheduler, scheduler_loss=scheduler_loss,\n",
        "      max_noise=0.1, noise_epochs=5\n",
        "  )\n",
        "  trainer.train_or_evaluate()\n",
        "  model = trainer.get_model()\n",
        "\n",
        "  # I need a flat load model function somewhere, as now I need to define a train loader before I can load a model\n",
        "  csv_file_path = output_path + \"training_log.csv\"\n",
        "  plot_training_log(csv_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VFPK5D5Xab2"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNetConv6().to(device)\n",
        "denoiser = DenoisingLoader(model, load_path)\n",
        "model = denoiser.model\n",
        "print('Loaded Model')\n",
        "\n",
        "# Example input (batch_size=1, channels=2, height=1025, width=175)\n",
        "noisy_input = torch.randn(1, 9, 1025, 175)\n",
        "\n",
        "denoised_output = denoiser.denoise(noisy_input)\n",
        "print(denoised_output.shape)"
      ],
      "metadata": {
        "id": "U9A8qCuyqbam",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b13106f-3b45-4fff-a992-a35767fc1fb9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model from /content/drive/MyDrive/Projects/ML_Projects/De-noising-autoencoder/Models_Denoising/Checkpoints_UNetConv6_-10-10/Autoencodermodel_earlystopping.pth\n",
            "Loaded Model\n",
            "torch.Size([1, 4, 1025, 175])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r-L1IZ-l9O7"
      },
      "source": [
        "## Testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "GAcVayA_pesy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f4c333-706a-4d45-b45d-c3ca35f236a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-10, 10]\n"
          ]
        }
      ],
      "source": [
        "# Define the source and destination file paths\n",
        "SNRdB = SNRdB # SNR random range\n",
        "print(SNRdB)\n",
        "#filename = f\"train-SNRdB_{SNRdB}-1s-44-1khz-magnitude-freqweightmagnitude-phase.h5\"\n",
        "source_folder = f\"/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_{folder}/SNRdB_{SNRdB[0]}-{SNRdB[1]}/\"\n",
        "source_path = source_folder + \"test/\"\n",
        "destination_path = f\"/content/SNRdB_{SNRdB[0]}-{SNRdB[1]}/test/\"\n",
        "scaler_file = output_path + \"scalers.pkl\"  # Static filename since it's unique per run\n",
        "\n",
        "save_path = destination_path + \"combined_000.h5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "cQwjhSZJpesz"
      },
      "outputs": [],
      "source": [
        "IMPORT_TEST_NOISY = True\n",
        "load_dataframe = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "EEucu19Apesz"
      },
      "outputs": [],
      "source": [
        "max_file_size_gb = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "E_338U2bpesz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b04558-00b3-4f70-b2d5-641273540fb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created new file: /content/SNRdB_-10-10/test/combined_000.h5\n",
            "Progress: 1.08 GB - Processing /content/drive/MyDrive/Datasets/Music-Noise/SNRdB_sep_features/SNRdB_-10-10/test/test-SNRdB_-10-10_20250225_001556.h5\n",
            "Progress: 2.11 GB - Processing /content/drive/MyDrive/Datasets/Music-Noise/SNRdB_sep_features/SNRdB_-10-10/test/test-SNRdB_-10-10_20250225_001656.h5\n",
            "Progress: 3.13 GB - Processing /content/drive/MyDrive/Datasets/Music-Noise/SNRdB_sep_features/SNRdB_-10-10/test/test-SNRdB_-10-10_20250225_001755.h5\n",
            "Progress: 4.16 GB - Processing /content/drive/MyDrive/Datasets/Music-Noise/SNRdB_sep_features/SNRdB_-10-10/test/test-SNRdB_-10-10_20250225_001755.h5\n",
            "Finished combining files into 1 output files in /content/SNRdB_-10-10/test/\n"
          ]
        }
      ],
      "source": [
        "destination_path = f\"/content/SNRdB_{SNRdB[0]}-{SNRdB[1]}/test/\"\n",
        "\n",
        "if IMPORT_TEST_NOISY:\n",
        "  if not os.path.exists(destination_path):\n",
        "    combine_h5_files_features(source_path, destination_path, max_file_size_gb=max_file_size_gb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_YpNU8IjC8V"
      },
      "source": [
        "View Pretrained Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "jq8WWxdalRhK"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.datasets.utils import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(scaler_file):\n",
        "    print(\"Loading existing scalers...\")\n",
        "    scalers = load_scalers(scaler_file)\n",
        "else:\n",
        "    print(\"Training new scalers...\")\n",
        "    scalers = train_scalers(dataset_path, sample_size=8000)\n",
        "    save_scalers(scalers, scaler_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6qyNPFoPIu-",
        "outputId": "a3bc8dfa-5f74-4858-caa5-7cef6ff9852c"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing scalers...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "dbFb6LzXlRhL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ceb4fc0-2899-4765-8268-9f7fb5d0a184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing scalers...\n",
            "Dataset size: 1741\n",
            "Training set size: 487\n",
            "Validation set size: 122\n",
            "Training set size: 487\n",
            "Validation set size: 122\n"
          ]
        }
      ],
      "source": [
        "if IMPORT_TEST_NOISY:\n",
        "    print(\"Loading existing scalers...\")\n",
        "    scalers = load_scalers(scaler_file)\n",
        "    test_loader = ChannelDatasetLoader(\n",
        "          dataset_path=save_path,\n",
        "          scalers=scalers,\n",
        "          output_time_length=175,\n",
        "          channels=1,\n",
        "          snr_db=SNRdB,\n",
        "          subset=True,\n",
        "          batch_size=batch_size\n",
        "      )\n",
        "\n",
        "    print(f\"Training set size: {len(test_loader.train_dataset)}\")\n",
        "    print(f\"Validation set size: {len(test_loader.val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "YLtXGJboq-vp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb262bc2-d63c-4214-cff3-a207c481b5a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: You have divergent branches and need to specify how to reconcile them.\u001b[m\n",
            "\u001b[33mhint: You can do so by running one of the following commands sometime before\u001b[m\n",
            "\u001b[33mhint: your next pull:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint:   git config pull.rebase false  # merge (the default strategy)\u001b[m\n",
            "\u001b[33mhint:   git config pull.rebase true   # rebase\u001b[m\n",
            "\u001b[33mhint:   git config pull.ff only       # fast-forward only\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: You can replace \"git config\" with \"git config --global\" to set a default\u001b[m\n",
            "\u001b[33mhint: preference for all repositories. You can also pass --rebase, --no-rebase,\u001b[m\n",
            "\u001b[33mhint: or --ff-only on the command line to override the configured default per\u001b[m\n",
            "\u001b[33mhint: invocation.\u001b[m\n",
            "fatal: Need to specify how to reconcile divergent branches.\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m4TfuxEJ0ti"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.system(\"pip install --upgrade torchmetrics\")"
      ],
      "metadata": {
        "id": "mycn_B1xfe36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a61faf-6846-4dc3-a66b-01232d49b6b9"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "LyyEUEWoLXYs"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.testing import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "nTS6rjFFHy5T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from loss import *\n",
        "\n",
        "# Testing loop\n",
        "def test_model(model, test_loader, criterion, scalers):\n",
        "    evaluation = Evaluation(scalers)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0.0\n",
        "        progress_bar = tqdm(test_loader, desc=\"Testing\", unit=\"batch\")\n",
        "        for inputs, targets, metadata in progress_bar:\n",
        "\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "          test_loss += loss.item()\n",
        "\n",
        "          # evaluation\n",
        "          evaluation.evaluate(inputs, targets, outputs, metadata)\n",
        "\n",
        "        test_loss /= len(test_loader)\n",
        "\n",
        "    return test_loss, evaluation.process()\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torchmetrics.audio import SignalDistortionRatio\n",
        "\n",
        "class Evaluation:\n",
        "    def __init__(self, scalers, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "        \"\"\"Initialize storage for evaluation metrics.\"\"\"\n",
        "        self.results = []\n",
        "        self.device = torch.device(device)  # Store the device\n",
        "        self.sdr = SignalDistortionRatio().to(self.device)  # Move SDR metric to the device\n",
        "        self.scalers = scalers\n",
        "\n",
        "\n",
        "    def evaluate(self, inputs, targets, outputs, metadata):\n",
        "        \"\"\"\n",
        "        Compute SDR and L1 loss for input vs. target and input vs. output.\n",
        "        \"\"\"\n",
        "        batch_size = inputs.shape[0]\n",
        "        chunk_length = 44100 * 2\n",
        "\n",
        "        # I need to inverse the standardisation for this, as it is skewing the results\n",
        "\n",
        "        # Convert tensors to NumPy\n",
        "        inputs = inputs.detach().cpu().numpy()\n",
        "        targets = targets.detach().cpu().numpy()\n",
        "        outputs = outputs.detach().cpu().numpy()\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            filename = metadata[i][\"filename\"]\n",
        "            snr_db = metadata[i][\"snr_db\"]\n",
        "            phase = metadata[i][\"phase\"]\n",
        "            lf_shape = metadata[i][\"lf_shape\"]\n",
        "\n",
        "            input = inputs[i]\n",
        "            output = outputs[i]\n",
        "            target = targets[i]\n",
        "\n",
        "            in_track = input.copy()\n",
        "            out_track = output.copy()\n",
        "            tar_track = target.copy()\n",
        "\n",
        "            # Inverse standardisation\n",
        "            #input_temp = input[0]\n",
        "            #input[0] = self.scalers[\"input_features_spectrogram\"].transform(input_temp.reshape(1, -1)).reshape(input_temp.shape)\n",
        "            #output_temp = output[0]\n",
        "            #output[0] = self.scalers[\"target_features_spectrogram\"].transform(output_temp.reshape(1, -1)).reshape(output_temp.shape)\n",
        "            #target_temp = target[0]\n",
        "            #target[0] = self.scalers[\"target_features_spectrogram\"].transform(target_temp.reshape(1, -1)).reshape(target_temp.shape)\n",
        "\n",
        "            #input_chunk = magphase_to_waveform(input[0], input[1], chunk_length)\n",
        "            #output_chunk = magphase_to_waveform(output[0], input[1], chunk_length)\n",
        "            #target_chunk = magphase_to_waveform(target[0], input[1], chunk_length)\n",
        "\n",
        "            # Move tensors to the correct device\n",
        "            #input_chunk = torch.from_numpy(input_chunk).to(self.device).float()\n",
        "            #output_chunk = torch.from_numpy(output_chunk).to(self.device).float()\n",
        "            #target_chunk = torch.from_numpy(target_chunk).to(self.device).float()\n",
        "\n",
        "            input = torch.from_numpy(input).to(self.device).float()\n",
        "            output = torch.from_numpy(output).to(self.device).float()\n",
        "            target = torch.from_numpy(target).to(self.device).float()\n",
        "\n",
        "            # Compute SDR (using torchaudio)\n",
        "            #sdr_invstar = self.sdr(input_chunk, target_chunk).item()\n",
        "            #sdr_outvstar = self.sdr(output_chunk, target_chunk).item()\n",
        "\n",
        "            # Compute L1 loss\n",
        "            l1_invstar = F.l1_loss(input[0:4, :, :], target[0:4, :, :]).item()\n",
        "            l1_outvstar = F.l1_loss(output[0:4, :, :], target[0:4, :, :]).item()\n",
        "\n",
        "            l1_invstar_4k = F.l1_loss(input[1:2, :, :], target[1:2, :, :]).item()\n",
        "            l1_outvstar_4k = F.l1_loss(output[1:2, :, :], target[1:2, :, :]).item()\n",
        "\n",
        "            l1_invstar_full = F.l1_loss(input[0:1, :, :], target[0:1, :, :]).item()\n",
        "            l1_outvstar_full = F.l1_loss(output[0:1, :, :], target[0:1, :, :]).item()\n",
        "\n",
        "            # Store results\n",
        "            self.results.append({\n",
        "                \"instance\": len(self.results),\n",
        "                #\"sdr_invstar\": sdr_invstar,\n",
        "                #\"sdr_outvstar\": sdr_outvstar,\n",
        "                \"l1_invstar\": l1_invstar,\n",
        "                \"l1_outvstar\": l1_outvstar,\n",
        "                \"l1_invstar_4k\": l1_invstar_4k,\n",
        "                \"l1_outvstar_4k\": l1_outvstar_4k,\n",
        "                \"l1_invstar_full\": l1_invstar_full,\n",
        "                \"l1_outvstar_full\": l1_outvstar_full,\n",
        "                \"filename\": filename,\n",
        "                \"snr_db\": snr_db,\n",
        "                \"in_track\":in_track,\n",
        "                \"out_track\":out_track,\n",
        "                \"tar_track\":tar_track,\n",
        "                \"metadata\": metadata[i],\n",
        "            })\n",
        "\n",
        "    def process(self):\n",
        "        \"\"\"Return the stored evaluation results as a Pandas DataFrame.\"\"\"\n",
        "        return pd.DataFrame(self.results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if load_dataframe:\n",
        "  df_subset = pd.read_csv(output_path + f\"df_subset_SNRdB_{SNRdB[0]}-{SNRdB[1]}.csv\")"
      ],
      "metadata": {
        "id": "OthiH-tDgRC0"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ8jaUW4j3Pr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2816b02b-3a90-4572-bcb0-0edb16978fa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing:   6%|▋         | 1/16 [01:11<17:49, 71.33s/batch, loss=0.1932]"
          ]
        }
      ],
      "source": [
        "if not load_dataframe:\n",
        "  criterion = nn.L1Loss()\n",
        "  loss, df_eval = test_model(model, test_loader.train_loader, criterion, scalers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "IrOfxekHV0C3"
      },
      "outputs": [],
      "source": [
        "if not load_dataframe:\n",
        "  # Assuming `df` is your original dataframe\n",
        "  #df_eval[\"Improvement\"] = df_eval[\"l1_outvstar\"] df_eval[\"l1_invstar\"]  # Higher SDR is better\n",
        "  subset_columns = [\"instance\", \"l1_invstar\", \"l1_outvstar\", \"l1_invstar_4k\", \"l1_outvstar_4k\", \"l1_invstar_full\", \"l1_outvstar_full\",  \"filename\", \"snr_db\"] #\"Improvement\"]\n",
        "  df_subset = df_eval#[subset_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "istycCl4X5bQ"
      },
      "outputs": [],
      "source": [
        "if not load_dataframe:\n",
        "  # Create a function to map filename to a class\n",
        "  def get_class_from_filename(filename, classes):\n",
        "      for keyword in classes:\n",
        "          if keyword in filename:\n",
        "              return keyword\n",
        "      return 'Unknown'  # Default if no match found\n",
        "\n",
        "  df_subset[['filename_audio', 'filename_noise']] = pd.DataFrame(df_subset['filename'].tolist(), index=df_subset.index)\n",
        "  df_subset['filename_audio'] = df_subset['filename_audio'].apply(lambda x: x.decode('utf-8'))\n",
        "  df_subset['filename_noise'] = df_subset['filename_noise'].apply(lambda x: x.decode('utf-8'))\n",
        "\n",
        "  classes = ['mixture', 'vocals', 'drums', 'guitar', 'bass', 'piano', 'electric_guitar', 'acoustic_guitar', 'synthesizer', 'strings', 'brass']\n",
        "  df_subset['audio_class'] = df_subset['filename_audio'].apply(lambda x: get_class_from_filename(x, classes))\n",
        "\n",
        "  classes = ['0707', 'Rain', 'Crowd', 'Water', 'Ice']\n",
        "  df_subset['noise_class'] = df_subset['filename_noise'].apply(lambda x: get_class_from_filename(x, classes))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_subset[\"Improvement_L1\"] = df_subset[\"l1_invstar\"] - df_subset[\"l1_outvstar\"]  # Lower L1 loss is better\n",
        "df_subset[\"Improvement_L1_4k\"] = df_subset[\"l1_invstar_4k\"] - df_subset[\"l1_outvstar_4k\"]  # Lower L1 loss is better\n",
        "df_subset[\"Improvement_L1_full\"] = df_subset[\"l1_invstar_full\"] - df_subset[\"l1_outvstar_full\"]  # Lower L1 loss is better"
      ],
      "metadata": {
        "id": "XQ9SAAWnm1fO"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "1scTZvFaY4lQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "7fe336bf-9dac-4eb4-8d16-aba98f12d314"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   instance  l1_invstar  l1_outvstar  l1_invstar_4k  l1_outvstar_4k  \\\n",
              "0         0    0.287995     0.323869       0.328501        0.343199   \n",
              "1         1    0.275927     0.321507       0.335857        0.400347   \n",
              "2         2    0.208919     0.140836       0.192437        0.124402   \n",
              "3         3    0.235673     0.201469       0.290922        0.280663   \n",
              "4         4    0.211520     0.189634       0.236738        0.199493   \n",
              "\n",
              "   l1_invstar_full  l1_outvstar_full  \\\n",
              "0         0.247490          0.304539   \n",
              "1         0.215997          0.242668   \n",
              "2         0.225402          0.157269   \n",
              "3         0.180424          0.122275   \n",
              "4         0.186303          0.179775   \n",
              "\n",
              "                                            filename    snr_db  \\\n",
              "0  [b'/content/drive/MyDrive/Datasets/Music/MUSDB... -8.189236   \n",
              "1  [b\"/content/drive/MyDrive/Datasets/Music/MUSDB... -8.327471   \n",
              "2  [b'/content/drive/MyDrive/Datasets/Music/MUSDB...  0.291863   \n",
              "3  [b'/content/drive/MyDrive/Datasets/Music/MUSDB...  2.241535   \n",
              "4  [b'/content/drive/MyDrive/Datasets/Music/MUSDB... -1.356703   \n",
              "\n",
              "                                            in_track  ...  \\\n",
              "0  [[[0.3978465, 0.29657388, 0.538415, 0.5428498,...  ...   \n",
              "1  [[[0.726088, 0.3395326, 0.669181, 0.7176643, 0...  ...   \n",
              "2  [[[0.66869396, 0.54443616, 0.44985816, 0.63280...  ...   \n",
              "3  [[[0.5883056, 0.65517056, 0.213943, 0.5526634,...  ...   \n",
              "4  [[[0.40303016, 0.41310564, 0.26492864, 0.38816...  ...   \n",
              "\n",
              "                                           tar_track  \\\n",
              "0  [[[0.5590081, 0.5262259, 0.5411487, 0.10410178...   \n",
              "1  [[[0.83663344, 0.81121397, 1.0769688, 1.007881...   \n",
              "2  [[[0.92772007, 0.17736584, 0.6346608, 0.605742...   \n",
              "3  [[[0.62849355, 0.36866778, 0.28301823, 0.43781...   \n",
              "4  [[[0.35766077, 0.3244235, -0.313973, -0.000427...   \n",
              "\n",
              "                                               phase   lf_shape  \\\n",
              "0  [[3.1415927, 3.1415927, 3.1415927, 3.1415927, ...  (10, 175)   \n",
              "1  [[0.0, 0.0, 3.1415927, 0.0, 0.0, 3.1415927, 3....  (10, 175)   \n",
              "2  [[0.0, 3.1415927, 3.1415927, 0.0, 3.1415927, 3...  (10, 175)   \n",
              "3  [[3.1415927, 3.1415927, 3.1415927, 0.0, 3.1415...  (10, 175)   \n",
              "4  [[3.1415927, 3.1415927, 3.1415927, 0.0, 0.0, 0...  (10, 175)   \n",
              "\n",
              "                                      filename_audio  \\\n",
              "0  /content/drive/MyDrive/Datasets/Music/MUSDB18/...   \n",
              "1  /content/drive/MyDrive/Datasets/Music/MUSDB18/...   \n",
              "2  /content/drive/MyDrive/Datasets/Music/MUSDB18/...   \n",
              "3  /content/drive/MyDrive/Datasets/Music/MUSDB18/...   \n",
              "4  /content/drive/MyDrive/Datasets/Music/MUSDB18/...   \n",
              "\n",
              "                                      filename_noise audio_class noise_class  \\\n",
              "0  /content/drive/MyDrive/Datasets/Noise/All_Nois...        bass       Crowd   \n",
              "1  /content/drive/MyDrive/Datasets/Noise/All_Nois...       drums     Unknown   \n",
              "2  /content/drive/MyDrive/Datasets/Noise/All_Nois...     mixture     Unknown   \n",
              "3  /content/drive/MyDrive/Datasets/Noise/All_Nois...        bass       Crowd   \n",
              "4  /content/drive/MyDrive/Datasets/Noise/All_Nois...     Unknown       Crowd   \n",
              "\n",
              "  Improvement_L1  Improvement_L1_4k  Improvement_L1_full  \n",
              "0      -0.035874          -0.014698            -0.057049  \n",
              "1      -0.045580          -0.064490            -0.026671  \n",
              "2       0.068084           0.068035             0.068133  \n",
              "3       0.034204           0.010260             0.058149  \n",
              "4       0.021886           0.037245             0.006528  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d1ba3381-cdd4-470f-80e5-449b0cf6ed51\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instance</th>\n",
              "      <th>l1_invstar</th>\n",
              "      <th>l1_outvstar</th>\n",
              "      <th>l1_invstar_4k</th>\n",
              "      <th>l1_outvstar_4k</th>\n",
              "      <th>l1_invstar_full</th>\n",
              "      <th>l1_outvstar_full</th>\n",
              "      <th>filename</th>\n",
              "      <th>snr_db</th>\n",
              "      <th>in_track</th>\n",
              "      <th>...</th>\n",
              "      <th>tar_track</th>\n",
              "      <th>phase</th>\n",
              "      <th>lf_shape</th>\n",
              "      <th>filename_audio</th>\n",
              "      <th>filename_noise</th>\n",
              "      <th>audio_class</th>\n",
              "      <th>noise_class</th>\n",
              "      <th>Improvement_L1</th>\n",
              "      <th>Improvement_L1_4k</th>\n",
              "      <th>Improvement_L1_full</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.287995</td>\n",
              "      <td>0.323869</td>\n",
              "      <td>0.328501</td>\n",
              "      <td>0.343199</td>\n",
              "      <td>0.247490</td>\n",
              "      <td>0.304539</td>\n",
              "      <td>[b'/content/drive/MyDrive/Datasets/Music/MUSDB...</td>\n",
              "      <td>-8.189236</td>\n",
              "      <td>[[[0.3978465, 0.29657388, 0.538415, 0.5428498,...</td>\n",
              "      <td>...</td>\n",
              "      <td>[[[0.5590081, 0.5262259, 0.5411487, 0.10410178...</td>\n",
              "      <td>[[3.1415927, 3.1415927, 3.1415927, 3.1415927, ...</td>\n",
              "      <td>(10, 175)</td>\n",
              "      <td>/content/drive/MyDrive/Datasets/Music/MUSDB18/...</td>\n",
              "      <td>/content/drive/MyDrive/Datasets/Noise/All_Nois...</td>\n",
              "      <td>bass</td>\n",
              "      <td>Crowd</td>\n",
              "      <td>-0.035874</td>\n",
              "      <td>-0.014698</td>\n",
              "      <td>-0.057049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.275927</td>\n",
              "      <td>0.321507</td>\n",
              "      <td>0.335857</td>\n",
              "      <td>0.400347</td>\n",
              "      <td>0.215997</td>\n",
              "      <td>0.242668</td>\n",
              "      <td>[b\"/content/drive/MyDrive/Datasets/Music/MUSDB...</td>\n",
              "      <td>-8.327471</td>\n",
              "      <td>[[[0.726088, 0.3395326, 0.669181, 0.7176643, 0...</td>\n",
              "      <td>...</td>\n",
              "      <td>[[[0.83663344, 0.81121397, 1.0769688, 1.007881...</td>\n",
              "      <td>[[0.0, 0.0, 3.1415927, 0.0, 0.0, 3.1415927, 3....</td>\n",
              "      <td>(10, 175)</td>\n",
              "      <td>/content/drive/MyDrive/Datasets/Music/MUSDB18/...</td>\n",
              "      <td>/content/drive/MyDrive/Datasets/Noise/All_Nois...</td>\n",
              "      <td>drums</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>-0.045580</td>\n",
              "      <td>-0.064490</td>\n",
              "      <td>-0.026671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.208919</td>\n",
              "      <td>0.140836</td>\n",
              "      <td>0.192437</td>\n",
              "      <td>0.124402</td>\n",
              "      <td>0.225402</td>\n",
              "      <td>0.157269</td>\n",
              "      <td>[b'/content/drive/MyDrive/Datasets/Music/MUSDB...</td>\n",
              "      <td>0.291863</td>\n",
              "      <td>[[[0.66869396, 0.54443616, 0.44985816, 0.63280...</td>\n",
              "      <td>...</td>\n",
              "      <td>[[[0.92772007, 0.17736584, 0.6346608, 0.605742...</td>\n",
              "      <td>[[0.0, 3.1415927, 3.1415927, 0.0, 3.1415927, 3...</td>\n",
              "      <td>(10, 175)</td>\n",
              "      <td>/content/drive/MyDrive/Datasets/Music/MUSDB18/...</td>\n",
              "      <td>/content/drive/MyDrive/Datasets/Noise/All_Nois...</td>\n",
              "      <td>mixture</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>0.068084</td>\n",
              "      <td>0.068035</td>\n",
              "      <td>0.068133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.235673</td>\n",
              "      <td>0.201469</td>\n",
              "      <td>0.290922</td>\n",
              "      <td>0.280663</td>\n",
              "      <td>0.180424</td>\n",
              "      <td>0.122275</td>\n",
              "      <td>[b'/content/drive/MyDrive/Datasets/Music/MUSDB...</td>\n",
              "      <td>2.241535</td>\n",
              "      <td>[[[0.5883056, 0.65517056, 0.213943, 0.5526634,...</td>\n",
              "      <td>...</td>\n",
              "      <td>[[[0.62849355, 0.36866778, 0.28301823, 0.43781...</td>\n",
              "      <td>[[3.1415927, 3.1415927, 3.1415927, 0.0, 3.1415...</td>\n",
              "      <td>(10, 175)</td>\n",
              "      <td>/content/drive/MyDrive/Datasets/Music/MUSDB18/...</td>\n",
              "      <td>/content/drive/MyDrive/Datasets/Noise/All_Nois...</td>\n",
              "      <td>bass</td>\n",
              "      <td>Crowd</td>\n",
              "      <td>0.034204</td>\n",
              "      <td>0.010260</td>\n",
              "      <td>0.058149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.211520</td>\n",
              "      <td>0.189634</td>\n",
              "      <td>0.236738</td>\n",
              "      <td>0.199493</td>\n",
              "      <td>0.186303</td>\n",
              "      <td>0.179775</td>\n",
              "      <td>[b'/content/drive/MyDrive/Datasets/Music/MUSDB...</td>\n",
              "      <td>-1.356703</td>\n",
              "      <td>[[[0.40303016, 0.41310564, 0.26492864, 0.38816...</td>\n",
              "      <td>...</td>\n",
              "      <td>[[[0.35766077, 0.3244235, -0.313973, -0.000427...</td>\n",
              "      <td>[[3.1415927, 3.1415927, 3.1415927, 0.0, 0.0, 0...</td>\n",
              "      <td>(10, 175)</td>\n",
              "      <td>/content/drive/MyDrive/Datasets/Music/MUSDB18/...</td>\n",
              "      <td>/content/drive/MyDrive/Datasets/Noise/All_Nois...</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Crowd</td>\n",
              "      <td>0.021886</td>\n",
              "      <td>0.037245</td>\n",
              "      <td>0.006528</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1ba3381-cdd4-470f-80e5-449b0cf6ed51')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d1ba3381-cdd4-470f-80e5-449b0cf6ed51 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d1ba3381-cdd4-470f-80e5-449b0cf6ed51');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2829b07c-1c5d-4845-9107-e43c6d66081a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2829b07c-1c5d-4845-9107-e43c6d66081a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2829b07c-1c5d-4845-9107-e43c6d66081a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_subset"
            }
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "df_subset.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_rate = 44100  # 44.1 kHz audio\n",
        "n_fft = 2048  # Adjust this for better resolution\n",
        "freqs = np.linspace(0, sampling_rate / 2, n_fft // 2 + 1)  # STFT frequency bins\n",
        "\n",
        "# Find indices corresponding to 0–4000 Hz\n",
        "min_freq, max_freq = 0, 4000\n",
        "freq_indices = np.where((freqs >= min_freq) & (freqs <= max_freq))[0]\n",
        "\n",
        "# in spectrogram\n",
        "index = 40\n",
        "\n",
        "snr_db = np.array(df_subset.loc[index, \"snr_db\"])\n",
        "print(snr_db)\n",
        "\n",
        "# lets evaluate this from a l1 loss perspective\n",
        "# reconstruct spectrogram\n",
        "out_spectrogram = np.array(df_subset.loc[index, \"out_track\"][0])\n",
        "out_spectrogram[df_subset.loc[index, \"metadata\"][\"freq_indices_hf\"], :] = resample_feature(np.array(df_subset.loc[index, \"out_track\"][1]), df_subset.loc[index, \"metadata\"][\"hf_shape\"])\n",
        "out_spectrogram[df_subset.loc[index, \"metadata\"][\"freq_indices_mf\"], :] = resample_feature(np.array(df_subset.loc[index, \"out_track\"][2]), df_subset.loc[index, \"metadata\"][\"mf_shape\"])\n",
        "out_spectrogram[df_subset.loc[index, \"metadata\"][\"freq_indices_lf\"], :] = resample_feature(np.array(df_subset.loc[index, \"out_track\"][3]), df_subset.loc[index, \"metadata\"][\"lf_shape\"])\n",
        "\n",
        "# out, with no join\n",
        "out_track = np.array(df_subset.loc[index, \"out_track\"])[0]\n",
        "\n",
        "# out spectrogram\n",
        "in_spectrogram = df_subset.loc[index, \"in_track\"][0]\n",
        "\n",
        "# target\n",
        "tar_track = np.array(df_subset.loc[index, \"tar_track\"])[0]\n",
        "\n",
        "# inverse normalisation to 0 - 1\n",
        "out_spectrogram = (out_spectrogram - 0.5) * 3\n",
        "out_track = (out_track - 0.5) * 3\n",
        "in_spectrogram = (in_spectrogram - 0.5) * 3\n",
        "tar_track = (tar_track - 0.5) * 3\n",
        "\n",
        "# Inverse standardisation\n",
        "input_temp = tar_track\n",
        "in_spectrogram = scalers[\"input_features_spectrogram\"].inverse_transform(in_spectrogram.reshape(1, -1)).reshape(input_temp.shape)\n",
        "\n",
        "out_spectrogram = scalers[\"target_features_spectrogram\"].inverse_transform(out_spectrogram.reshape(1, -1)).reshape(input_temp.shape)\n",
        "out_track = scalers[\"target_features_spectrogram\"].inverse_transform(out_track.reshape(1, -1)).reshape(input_temp.shape)\n",
        "\n",
        "tar_track = scalers[\"target_features_spectrogram\"].inverse_transform(tar_track.reshape(1, -1)).reshape(input_temp.shape)\n",
        "\n",
        "# plot things\n",
        "# Plot spectrograms\n",
        "fig, axes = plt.subplots(4, 1, figsize=(15, 15))\n",
        "\n",
        "axes[0].imshow(in_spectrogram, aspect=\"auto\", cmap=\"magma\", origin=\"lower\")\n",
        "axes[0].set_title(\"Noisy Input (Log Scale)\")\n",
        "axes[0].set_yscale(\"log\")\n",
        "axes[0].set_ylim((1, 1000))\n",
        "\n",
        "axes[1].imshow(out_spectrogram, aspect=\"auto\", cmap=\"magma\", origin=\"lower\")\n",
        "axes[1].set_title(\"Denoised Output (Log Scale) - reconstructed\")\n",
        "axes[1].set_yscale(\"log\")\n",
        "axes[1].set_ylim((1, 1000))\n",
        "\n",
        "axes[2].imshow(out_track, aspect=\"auto\", cmap=\"magma\", origin=\"lower\")\n",
        "axes[2].set_title(\"Denoised Output (Log Scale)\")\n",
        "axes[2].set_yscale(\"log\")\n",
        "axes[2].set_ylim((1, 1000))\n",
        "\n",
        "axes[3].imshow(tar_track, aspect=\"auto\", cmap=\"magma\", origin=\"lower\")\n",
        "axes[3].set_title(\"Clean Target (Log Scale)\")\n",
        "axes[3].set_yscale(\"log\")\n",
        "axes[3].set_ylim((1, 1000))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GNrvnkw4Yo04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "outputId": "1fa5ce02-3792-4d21-a341-156bc3b7804e"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5402016043663025\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'hf_shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'hf_shape'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-d630f41f7671>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mlf_spectrogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"out_track\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mout_spectrogram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"freq_indices_hf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"out_track\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hf_shape\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mout_spectrogram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"freq_indices_mf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"out_track\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mf_shape\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mout_spectrogram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"freq_indices_lf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"out_track\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lf_shape\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   4212\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4214\u001b[0;31m         \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4215\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   4636\u001b[0m             \u001b[0;31m#  pending resolution of GH#33047\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4638\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4639\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'hf_shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def magphase_to_waveform(magnitude, phase, audio_length=44100):\n",
        "    \"\"\"\n",
        "    Converts a spectrogram image back into an audio waveform.\n",
        "\n",
        "    Parameters:\n",
        "        image (np.array): Spectrogram image (3 channels).\n",
        "        sr (int): Sampling rate.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Reconstructed audio waveform.\n",
        "    \"\"\"\n",
        "    stft = magnitude * np.exp(1j * phase)\n",
        "    return librosa.istft(stft, length=audio_length)"
      ],
      "metadata": {
        "id": "XVYDvDepaM-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io.wavfile\n",
        "from google.colab import files\n",
        "import librosa\n",
        "\n",
        "# output waveform\n",
        "phase = df_subset.loc[index, \"phase\"]\n",
        "phase = scalers[\"input_features_phase\"].inverse_transform(phase.reshape(1, -1)).reshape(input_temp.shape)\n",
        "print(np.max(phase))\n",
        "print(np.min(phase))\n",
        "\n",
        "# reverse log scale\n",
        "out_spectrogram = librosa.db_to_amplitude(out_spectrogram)\n",
        "signal = magphase_to_waveform(out_spectrogram, phase, 44100 * 2)\n",
        "\n",
        "# Save as WAV file\n",
        "output_filename = f\"denoised_audio_{index}.wav\"\n",
        "scipy.io.wavfile.write(output_filename, rate=44100, data=signal)  # 16-bit PCM\n",
        "\n",
        "# Download the file\n",
        "files.download(output_filename)\n",
        "\n",
        "tar_track = librosa.db_to_amplitude(tar_track)\n",
        "signal = magphase_to_waveform(tar_track, phase, 44100 * 2)\n",
        "\n",
        "# Save as WAV file\n",
        "output_filename = f\"audio_{index}.wav\"\n",
        "scipy.io.wavfile.write(output_filename, rate=44100, data=signal)  # 16-bit PCM\n",
        "\n",
        "# Download the file\n",
        "files.download(output_filename)\n",
        "\n",
        "in_spectrogram = librosa.db_to_amplitude(in_spectrogram)\n",
        "signal = magphase_to_waveform(in_spectrogram, phase, 44100 * 2)\n",
        "\n",
        "# Save as WAV file\n",
        "output_filename = f\"noisy_audio_{index}.wav\"\n",
        "scipy.io.wavfile.write(output_filename, rate=44100, data=signal)  # 16-bit PCM\n",
        "\n",
        "# Download the file\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "id": "a9RpJcI7Zkw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a sample row to visualize\n",
        "index = 39  # Change this to visualize different rows\n",
        "\n",
        "# Extract spectrogram data (assuming they are stored as lists in the DataFrame)\n",
        "in_track = np.array(df_subset.loc[index, \"in_track\"])[1]\n",
        "in_track = in_track/np.max(in_track)\n",
        "out_track = np.array(df_subset.loc[index, \"out_track\"])[1]\n",
        "out_track = out_track/np.max(out_track)\n",
        "tar_track = np.array(df_subset.loc[index, \"tar_track\"])[1]\n",
        "tar_track = tar_track/np.max(tar_track)\n",
        "snr_db = np.array(df_subset.loc[index, \"snr_db\"])\n",
        "print(snr_db)\n",
        "\n",
        "# Plot spectrograms\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "axes[0].imshow(in_track, aspect=\"auto\", cmap=\"magma\", origin=\"lower\")\n",
        "axes[0].set_title(\"Noisy Input (Log Scale)\")\n",
        "axes[0].set_yscale(\"log\")\n",
        "axes[0].set_ylim((1, 1000))\n",
        "\n",
        "axes[1].imshow(out_track, aspect=\"auto\", cmap=\"magma\", origin=\"lower\")\n",
        "axes[1].set_title(\"Denoised Output (Log Scale)\")\n",
        "axes[1].set_yscale(\"log\")\n",
        "axes[1].set_ylim((1, 1000))\n",
        "\n",
        "axes[2].imshow(tar_track, aspect=\"auto\", cmap=\"magma\", origin=\"lower\")\n",
        "axes[2].set_title(\"Clean Target (Log Scale)\")\n",
        "axes[2].set_yscale(\"log\")\n",
        "axes[2].set_ylim((1, 1000))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SZ1Ha6v6wJ7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp03uLRHVuG9"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Set minimal theme\n",
        "sns.set_theme(style=\"white\", font_scale=1.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "df_eval = df_subset\n",
        "\n",
        "# Round SNR values to the nearest 0.5 to reduce noise\n",
        "df_eval[\"snr_db_rounded\"] = df_eval[\"snr_db\"].round(1)  # Rounds to 1 decimal place\n",
        "df_eval[\"snr_db_rounded\"] = (df_eval[\"snr_db_rounded\"] * 2).round() / 2  # Ensures nearest 0.5\n",
        "\n",
        "# Group by rounded SNR and audio/noise class, then compute mean improvement\n",
        "df_audio_avg = df_eval.groupby([\"snr_db_rounded\", \"audio_class\"], as_index=False)[\"Improvement_L1\"].mean()\n",
        "df_noise_avg = df_eval.groupby([\"snr_db_rounded\", \"noise_class\"], as_index=False)[\"Improvement_L1\"].mean()\n",
        "\n",
        "# Line plot colored by 'audio_class'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_audio_avg, x=\"snr_db_rounded\", y=\"Improvement_L1\", hue=\"audio_class\", palette=\"tab10\", marker=\"o\")\n",
        "plt.xlabel(\"SNR (dB)\")\n",
        "plt.ylabel(\"Mean Improvement (SDR)\")\n",
        "plt.title(\"Mean Improvement vs SNR (Rounded to 0.5, Colored by Audio Class)\")\n",
        "plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Line plot colored by 'noise_class'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_noise_avg, x=\"snr_db_rounded\", y=\"Improvement_L1\", hue=\"noise_class\", palette=\"tab10\", marker=\"o\")\n",
        "plt.xlabel(\"SNR (dB)\")\n",
        "plt.ylabel(\"Mean Improvement (SDR)\")\n",
        "plt.title(\"Mean Improvement vs SNR (Rounded to 0.5, Colored by Noise Class)\")\n",
        "plt.legend(title=\"Noise Class\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "x_9TP5JJnAmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "df_eval = df_subset\n",
        "\n",
        "# Round SNR values to the nearest 0.5 to reduce noise\n",
        "df_eval[\"snr_db_rounded\"] = df_eval[\"snr_db\"].round(1)  # Rounds to 1 decimal place\n",
        "df_eval[\"snr_db_rounded\"] = (df_eval[\"snr_db_rounded\"] * 2).round() / 2  # Ensures nearest 0.5\n",
        "\n",
        "# Group by rounded SNR and audio/noise class, then compute mean improvement\n",
        "df_audio_avg = df_eval.groupby([\"snr_db_rounded\", \"audio_class\"], as_index=False)[\"Improvement_L1_4k\"].mean()\n",
        "df_noise_avg = df_eval.groupby([\"snr_db_rounded\", \"noise_class\"], as_index=False)[\"Improvement_L1_4k\"].mean()\n",
        "\n",
        "# Line plot colored by 'audio_class'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_audio_avg, x=\"snr_db_rounded\", y=\"Improvement_L1_4k\", hue=\"audio_class\", palette=\"tab10\", marker=\"o\")\n",
        "plt.xlabel(\"SNR (dB)\")\n",
        "plt.ylabel(\"Mean Improvement (SDR)\")\n",
        "plt.title(\"Mean Improvement vs SNR (Rounded to 0.5, Colored by Audio Class)\")\n",
        "plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Line plot colored by 'noise_class'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_noise_avg, x=\"snr_db_rounded\", y=\"Improvement_L1_4k\", hue=\"noise_class\", palette=\"tab10\", marker=\"o\")\n",
        "plt.xlabel(\"SNR (dB)\")\n",
        "plt.ylabel(\"Mean Improvement (SDR)\")\n",
        "plt.title(\"Mean Improvement vs SNR (Rounded to 0.5, Colored by Noise Class)\")\n",
        "plt.legend(title=\"Noise Class\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1G_7pHjli0R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = df_subset\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax = sns.boxplot(x=\"noise_class\", y=\"Improvement_L1\", hue=\"audio_class\", data=df)\n",
        "\n",
        "# Customize the gridlines\n",
        "for line in ax.get_ygridlines():\n",
        "    if line.get_ydata()[0] == 0:  # Check if it's the gridline at y=0\n",
        "        line.set_color('grey')  # Set color to black\n",
        "        line.set_linewidth(2)  # Set line width to make it bolder\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.savefig(output_path + f\"boxplot_all_L1.png\")\n",
        "plt.show()\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter for 'crowd' noise class\n",
        "df_crowd = df_subset[df_subset[\"noise_class\"] == \"Crowd\"].copy()\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(7, 5))\n",
        "ax = sns.boxplot(x=\"audio_class\", y=\"Improvement_L1\", hue=\"audio_class\", data=df_crowd)\n",
        "\n",
        "# Customize the gridlines\n",
        "for line in ax.get_ygridlines():\n",
        "    if line.get_ydata()[0] == 0:  # Check if it's the gridline at y=0\n",
        "        line.set_color('grey')  # Set color to black\n",
        "        line.set_linewidth(2)  # Set line width to make it bolder\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks()\n",
        "#plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.savefig(output_path + f\"boxplot_crowd_L1.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "os6v-7ziieOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = df_subset\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax = sns.boxplot(x=\"noise_class\", y=\"Improvement_L1_4k\", hue=\"audio_class\", data=df)\n",
        "\n",
        "# Customize the gridlines\n",
        "for line in ax.get_ygridlines():\n",
        "    if line.get_ydata()[0] == 0:  # Check if it's the gridline at y=0\n",
        "        line.set_color('grey')  # Set color to black\n",
        "        line.set_linewidth(2)  # Set line width to make it bolder\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.savefig(output_path + f\"boxplot_all_L1_4k.png\")\n",
        "plt.show()\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter for 'crowd' noise class\n",
        "df_crowd = df_subset[df_subset[\"noise_class\"] == \"Crowd\"].copy()\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(7, 5))\n",
        "ax = sns.boxplot(x=\"audio_class\", y=\"Improvement_L1_4k\", hue=\"audio_class\", data=df_crowd)\n",
        "\n",
        "# Customize the gridlines\n",
        "for line in ax.get_ygridlines():\n",
        "    if line.get_ydata()[0] == 0:  # Check if it's the gridline at y=0\n",
        "        line.set_color('grey')  # Set color to black\n",
        "        line.set_linewidth(2)  # Set line width to make it bolder\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks()\n",
        "#plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.savefig(output_path + f\"boxplot_crowd_L1_4k.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Bo58Je9fjWtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = df_subset\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax = sns.boxplot(x=\"noise_class\", y=\"Improvement_L1_full\", hue=\"audio_class\", data=df)\n",
        "\n",
        "# Customize the gridlines\n",
        "for line in ax.get_ygridlines():\n",
        "    if line.get_ydata()[0] == 0:  # Check if it's the gridline at y=0\n",
        "        line.set_color('grey')  # Set color to black\n",
        "        line.set_linewidth(2)  # Set line width to make it bolder\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.savefig(output_path + f\"boxplot_all_L1_full.png\")\n",
        "plt.show()\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter for 'crowd' noise class\n",
        "df_crowd = df_subset[df_subset[\"noise_class\"] == \"Crowd\"].copy()\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(7, 5))\n",
        "ax = sns.boxplot(x=\"audio_class\", y=\"Improvement_L1_full\", hue=\"audio_class\", data=df_crowd)\n",
        "\n",
        "# Customize the gridlines\n",
        "for line in ax.get_ygridlines():\n",
        "    if line.get_ydata()[0] == 0:  # Check if it's the gridline at y=0\n",
        "        line.set_color('grey')  # Set color to black\n",
        "        line.set_linewidth(2)  # Set line width to make it bolder\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks()\n",
        "#plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.savefig(output_path + f\"boxplot_crowd_L1_full.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "L2DTNG0uoW_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3AQwpWnXgob"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you already have the dataframe loaded in `df`\n",
        "# df = pd.read_csv('your_data.csv')  # Uncomment if loading from CSV\n",
        "df = df_subset\n",
        "\n",
        "# You can also add visualization here if you want to dive deeper\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a more interpretable colormap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df[['sdr_invstar', 'sdr_outvstar', 'l1_invstar', 'l1_outvstar', 'snr_db', 'Improvement']].corr(),\n",
        "            annot=True, cmap='mako', fmt=\".2f\", vmin=-1, vmax=1, center=0)\n",
        "\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save subset dataframe\n",
        "df_subset.to_csv(output_path + f\"df_subset_SNRdB_{SNRdB[0]}-{SNRdB[1]}.csv\", index=False)"
      ],
      "metadata": {
        "id": "zVFDKMPLS42d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSQyZu_J5FnK"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "\n",
        "# Filter the dataframe for rows where Improvement is greater than 3\n",
        "filtered_df = df_eval[df_eval[\"Improvement\"] > 3]\n",
        "\n",
        "# Define the maximum number of files to write\n",
        "max_files = 5\n",
        "\n",
        "def norm(x):\n",
        "  # If x is a tensor, convert it to numpy array first\n",
        "  if isinstance(x, torch.Tensor):\n",
        "      x = x.detach().cpu().numpy()\n",
        "  return x / np.max(np.abs(x))\n",
        "\n",
        "# Loop through the filtered rows and save the chunks to WAV files, but stop at max_files\n",
        "file_count = 0\n",
        "for index, row in filtered_df.iterrows():\n",
        "    if file_count >= max_files:\n",
        "        break  # Stop if we have written the maximum number of files\n",
        "\n",
        "    input_chunk = row[\"input_chunk\"]\n",
        "    output_chunk = row[\"output_chunk\"]\n",
        "    target_chunk = row[\"target_chunk\"]\n",
        "\n",
        "    # Write to WAV files\n",
        "    input_filename = f\"/content/input_chunk_{index}.wav\"\n",
        "    output_filename = f\"/content/output_chunk_{index}.wav\"\n",
        "    target_filename = f\"/content/target_chunk_{index}.wav\"\n",
        "\n",
        "    sf.write(input_filename, norm(input_chunk), 44100)  # Assuming a sample rate of 44100 Hz\n",
        "    sf.write(output_filename, norm(output_chunk), 44100)\n",
        "    sf.write(target_filename, norm(target_chunk), 44100)\n",
        "\n",
        "    # Increment the file count\n",
        "    file_count += 1\n",
        "\n",
        "# Print how many files were written\n",
        "print(f\"Total WAV files written: {file_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvCNTO7up5e8"
      },
      "source": [
        "# Improvements that need to be made\n",
        "\n",
        "1. Metadata h5 column, including\n",
        "- Filename\n",
        "- SNR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFd_JY_vRFJY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cooVhUuEQ5aF"
      },
      "source": [
        "## Convert some entire songs and test some metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvNP8ZZwQ2xF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "whole_files = '/content/drive/MyDrive/Datasets/Music/MUSDB18/test/'\n",
        "song_files = []\n",
        "\n",
        "# Walk through the directory tree\n",
        "for root, dirs, files in os.walk(whole_files):\n",
        "    # Filter files with '.wav' extension and 'mixture' in their name\n",
        "    for f in files:\n",
        "        if f.endswith('.wav') and 'mixture' in f:\n",
        "            full_path = os.path.join(root, f)\n",
        "            song_files.append(full_path)\n",
        "\n",
        "print(f\"\\nTotal matching files: {len(song_files)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hbrf6o-X0xX"
      },
      "source": [
        "Generate Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQYQ8bH4mSai"
      },
      "outputs": [],
      "source": [
        "noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits/val/Crowd Noise (1)_zkChMb.wav'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_uxrcpp04wp"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.denoising import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWoHPPrI9dcG"
      },
      "outputs": [],
      "source": [
        "noisy_audio, sr = generate_audio_with_noise(song_files[0], noise_file, start_time=10, duration=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S5TJj7bZDnR"
      },
      "source": [
        "now the answer is to digest this waveform at 1s at a time, process those seconds, at intervals of 0.5s, window the outputs and put it back together for display on spectrograms and/or for output to .wav file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8UgbuKiWYXb"
      },
      "outputs": [],
      "source": [
        "denoiser = AudioDenoiser(model, output_path=output_path, chunk_duration=2, step_size=0.5)\n",
        "reconstructed_audio, reconstructed_input = denoiser.process_audio(noisy_audio, sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN4K8F4rrwMb"
      },
      "outputs": [],
      "source": [
        "!pip install mir_eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aorJVEypw2Ry"
      },
      "source": [
        "Use the SDR metric to compare signal to noise ratios of the generated output, and the standard output and demonstrate an increase in signal to noise ratio overall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJjf5BGKdTEe"
      },
      "outputs": [],
      "source": [
        "average_rec = np.log(np.average(Pxx_rec, axis=1))\n",
        "average_spec = np.log(np.average(Pxx_spec, axis=1))\n",
        "\n",
        "plt.plot(average_rec)\n",
        "plt.plot(average_spec)\n",
        "plt.xscale('log')\n",
        "plt.show()\n",
        "\n",
        "import numpy as np\n",
        "import mir_eval\n",
        "\n",
        "def compute_sdr(reference, estimated):\n",
        "    \"\"\"\n",
        "    Compute the Signal-to-Distortion Ratio (SDR) between reference and estimated signals.\n",
        "\n",
        "    :param reference: np.ndarray of shape (channels, samples), ground-truth clean signal\n",
        "    :param estimated: np.ndarray of shape (channels, samples), predicted separated signal\n",
        "    :return: float, SDR value in dB\n",
        "    \"\"\"\n",
        "    # Ensure inputs are 2D (stereo/multichannel) or 1D (mono)\n",
        "    reference = np.atleast_2d(reference)\n",
        "    estimated = np.atleast_2d(estimated)\n",
        "\n",
        "    # Compute SDR using mir_eval\n",
        "    sdr, _, _, _ = mir_eval.separation.bss_eval_sources(reference, estimated, compute_permutation=False)\n",
        "\n",
        "    return np.mean(sdr)  # Return average SDR across channels\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Fake reference and estimated signals (replace with actual signals)\n",
        "    ref_signal = reconstructed_audio_input  # 2 channels, 1 second at 44.1kHz\n",
        "    est_signal = reconstructed_audio  # Slightly noisy estimate\n",
        "\n",
        "    # max sdr\n",
        "    sdr_max = compute_sdr(audio, audio)\n",
        "    print(f\"SDR - : {sdr_max:.2f} dB -- Max\")\n",
        "\n",
        "    # reference sdr\n",
        "    sdr_ref = compute_sdr(audio, noisy_audio)\n",
        "    print(f\"SDR - : {sdr_ref:.2f} dB -- Reference\")\n",
        "\n",
        "    # computed srd\n",
        "    sdr_value = compute_sdr(audio, est_signal)\n",
        "    print(f\"SDR - : {sdr_value:.2f} dB -- Denoising\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}