{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8SELAW0r3K8"
      },
      "source": [
        "### Idea for denoising archetecture\n",
        "0. Add the other noise sources to a new dataset\n",
        "1. Come up with an example (so make the resampling and reconstruction a function)\n",
        "2.\n",
        "\n",
        "\n",
        "\n",
        "## 1. Using a Laplacian Filter (Edge Detection)\n",
        "```python\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Apply the Laplacian filter\n",
        "filtered_spectrogram = cv2.Laplacian(spectrogram, cv2.CV_64F, ksize=3)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And either use MFCC, or Cepstrum"
      ],
      "metadata": {
        "id": "pk0VVeyChttz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1B6SS0YdX-xs",
        "outputId": "6f769c7e-b67b-4b3e-94bd-71244f9a45a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE8QDLOXuiE7"
      },
      "source": [
        "Environmental Sound Classification 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ2cUzDwQQmi",
        "outputId": "143775d0-96d4-423b-c69d-f72b0c039ff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab...\n"
          ]
        }
      ],
      "source": [
        "# Detect Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    print(\"Running in Google Colab...\")\n",
        "    os.system(\"git clone https://github.com/CiaranMaloy/audioautoencoder\")\n",
        "    os.chdir(\"/content/audioautoencoder/\")\n",
        "    os.system(\"git pull\")\n",
        "    os.system(\"git checkout bandchannels\")\n",
        "    os.system(\"git pull origin bandchannels\")\n",
        "    #os.system(\"pip install --upgrade torchmetrics\")\n",
        "else:\n",
        "    print(\"Running locally...\")\n",
        "    os.system(\"git pull origin bandchannels\")\n",
        "    os.system(\"pip install --upgrade torchmetrics\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "av62pZLzSUjo"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/audioautoencoder')\n",
        "sys.path.append('/content/audioautoencoder/audioautoencoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJxpYCvxUBZZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N23OLmvVQr6V"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.data import *\n",
        "from audioautoencoder.outputs import *\n",
        "from audioautoencoder.processing import *\n",
        "from audioautoencoder.training import *\n",
        "from audioautoencoder.datasets.loaders import *\n",
        "from audioautoencoder.generate_dataset import *\n",
        "from audioautoencoder.plotting import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBX7YjnSJpLC"
      },
      "source": [
        "test get item input and output to see if i can use the log magnitude and then reverse it on the output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Niz7eKR1xy18"
      },
      "source": [
        "## Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4jLmXnwg8YL-"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.data_management import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qIB1M7xDLIFR"
      },
      "outputs": [],
      "source": [
        "GENERATE=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vHikpu86MO3s"
      },
      "outputs": [],
      "source": [
        "# Example Usage\n",
        "if GENERATE:\n",
        "  dataset_dirs = [\"/content/drive/MyDrive/Datasets/Noise/All_Noise\"]\n",
        "  output_dir = \"/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2\"\n",
        "  splits = create_datasets(dataset_dirs, output_dir)\n",
        "  print(\"Training Set:\", len(splits[\"train\"]))\n",
        "  print(\"Validation Set:\", len(splits[\"val\"]))\n",
        "  print(\"Testing Set:\", len(splits[\"test\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_C4AGBiBPsQ8"
      },
      "outputs": [],
      "source": [
        "if GENERATE:\n",
        "  save_splits_to_directories(splits, output_dir, max_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KT9U_mkQtxzx"
      },
      "outputs": [],
      "source": [
        "# generate audio files for noise and music (2s)\n",
        "if GENERATE:\n",
        "  noise_test = output_dir + \"/test\"\n",
        "  noise_train = output_dir + \"/train\"\n",
        "\n",
        "  noise_test_output = noise_test + \"-2s-44100\"\n",
        "  noise_train_output = noise_train + \"-2s-44100\"\n",
        "\n",
        "  for input_path, output_path in [(noise_test, noise_test_output), (noise_train, noise_train_output)]:\n",
        "    print(input_path, output_path)\n",
        "    generate_audio_files(input_path, output_path, t=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wf3uGYyGwS0S"
      },
      "outputs": [],
      "source": [
        "# generate audio files for noise and music (2s)\n",
        "if False:\n",
        "  music_test = \"/content/drive/MyDrive/Datasets/Music/MUSDB18/test\"\n",
        "  music_train = \"/content/drive/MyDrive/Datasets/Music/MUSDB18/train\"\n",
        "\n",
        "  music_test_output = music_test + \"-2s-44100\"\n",
        "  music_train_output = music_train + \"-2s-44100\"\n",
        "\n",
        "  for input_path, output_path in [(music_test, music_test_output), (music_train, music_train_output)]:\n",
        "    print(input_path, output_path)\n",
        "    generate_audio_files(input_path, output_path, t=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxxKjl_JOAKE"
      },
      "source": [
        "## Process files to H5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "X82E9Uf6WdX_"
      },
      "outputs": [],
      "source": [
        "GENERATE_H5_FILES = False\n",
        "\n",
        "if GENERATE_H5_FILES:\n",
        "  checkpoint_file_size=50000\n",
        "  processor = DatasetProcessor(\n",
        "          train_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/train-2s-44100',\n",
        "          train_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/train-2s-44100',\n",
        "          test_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/test-2s-44100',\n",
        "          test_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test-2s-44100',\n",
        "          output_dir='/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_sep_features',\n",
        "          SNRdB=[-10, 10],\n",
        "          process_train=True,\n",
        "          process_test=True,\n",
        "          checkpoint_file_size=checkpoint_file_size\n",
        "      )\n",
        "  processor.process()\n",
        "\n",
        "  processor = DatasetProcessor(\n",
        "          train_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/train-2s-44100',\n",
        "          train_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/train-2s-44100',\n",
        "          test_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/test-2s-44100',\n",
        "          test_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test-2s-44100',\n",
        "          output_dir='/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_sep_features',\n",
        "          SNRdB=[0, 20],\n",
        "          process_train=True,\n",
        "          process_test=True,\n",
        "          checkpoint_file_size=checkpoint_file_size\n",
        "      )\n",
        "  processor.process()\n",
        "\n",
        "  processor = DatasetProcessor(\n",
        "          train_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/train-2s-44100',\n",
        "          train_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/train-2s-44100',\n",
        "          test_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/test-2s-44100',\n",
        "          test_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test-2s-44100',\n",
        "          output_dir='/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_sep_features',\n",
        "          SNRdB=[10, 30],\n",
        "          process_train=True,\n",
        "          process_test=True,\n",
        "          checkpoint_file_size=checkpoint_file_size\n",
        "      )\n",
        "  processor.process()\n",
        "\n",
        "  processor = DatasetProcessor(\n",
        "          train_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/train-2s-44100',\n",
        "          train_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/train-2s-44100',\n",
        "          test_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/test-2s-44100',\n",
        "          test_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test-2s-44100',\n",
        "          output_dir='/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_mix_features',\n",
        "          SNRdB=[-10, 30],\n",
        "          process_train=True,\n",
        "          process_test=True,\n",
        "          mix_only=True,\n",
        "          checkpoint_file_size=checkpoint_file_size\n",
        "      )\n",
        "  processor.process()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2m4ztn1qxC3"
      },
      "source": [
        "## Define Autoencoder structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep3iZRJC0tJi"
      },
      "source": [
        "## Improvement to UnetAutoencoder\n",
        "\n",
        "1. changed padding from explicit to on the output of the convolutional layer\n",
        "2. added batch normalisation between layers\n",
        "3. changed fro elu to leaky_relu for efficiency\n",
        "4. in the other one there is no pooling... and the skip connections are done by concatenation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOXAuPserTGv"
      },
      "source": [
        "#### For v3, remove pooling layers, and add attention onto skip connections\n",
        "\n",
        "In addition, try only music and only crowd as a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "zyXtMpe1PvID"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# too big without.... lets try, halfing the expansion\n",
        "# apternaitvely use max pooling instead of average pooling\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)  # Avg Pooling\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)  # Max Pooling\n",
        "        attn = self.conv(torch.cat([avg_out, max_out], dim=1))  # Convolution\n",
        "        return x * self.sigmoid(attn)  # Apply Attention Map\n",
        "\n",
        "class UNetConv9(nn.Module):\n",
        "    # Update from UnetConv6, moving to a masking model, which hopefully works better\n",
        "    def __init__(self, in_channels=9, out_channels=4):\n",
        "        super(UNetConv9, self).__init__()\n",
        "\n",
        "        a = 2\n",
        "        A, B, C, D = 64, 128, 256, 512\n",
        "        bottleneck_channels = 1024\n",
        "\n",
        "        # function\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Encoder (Downsampling)\n",
        "        enc_channels = [in_channels, A, B, C, D]\n",
        "        self.enc1 = self.conv_block(enc_channels[0], enc_channels[1], (10, 3), 2)\n",
        "        self.enc2 = self.conv_block(enc_channels[1], enc_channels[2], 5, 2)\n",
        "        self.enc3 = self.conv_block(enc_channels[2], enc_channels[3], 3, 2)\n",
        "        self.enc4 = self.conv_block(enc_channels[3], enc_channels[4], 3, 2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(enc_channels[4], bottleneck_channels, 3, 2)\n",
        "\n",
        "        # Decoder (Upsampling)\n",
        "        dec_channels = [bottleneck_channels, D, C, B, A]\n",
        "        self.dec4 = self.upconv_block(dec_channels[0], dec_channels[1], 3, 2)\n",
        "        self.dec3 = self.upconv_block(dec_channels[1] + enc_channels[4], dec_channels[2], 3, 2)\n",
        "        self.dec2 = self.upconv_block(dec_channels[2] + enc_channels[3], dec_channels[3], 5, 2)\n",
        "        self.dec1 = self.upconv_block(dec_channels[3] + enc_channels[2], dec_channels[4], (10, 3), 2)\n",
        "\n",
        "        # Final Output Layer\n",
        "        self.final = nn.Conv2d(dec_channels[4] + enc_channels[1], out_channels, kernel_size=(10, 3), padding=1, stride=1)\n",
        "\n",
        "        # Initialize Spatial Attention Modules\n",
        "        self.spatial_attn4 = SpatialAttention()\n",
        "        self.spatial_attn3 = SpatialAttention()\n",
        "        self.spatial_attn2 = SpatialAttention()\n",
        "        self.spatial_attn1 = SpatialAttention()\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels, kernel_size, stride, dropout=0.2):\n",
        "        \"\"\"Convolutional Block with Dropout in Deeper Layers Only\"\"\"\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1, stride=stride), # In the next iteration i should introduce a stride here\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        ]\n",
        "\n",
        "        # Dropout only for deeper encoder layers\n",
        "        if out_channels >= 256:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def upconv_block(self, in_channels, out_channels, kernel_size, stride, dropout=0.2):\n",
        "        \"\"\"Upsampling Block with Dropout in First Few Decoder Layers\"\"\"\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        ]\n",
        "\n",
        "        # Dropout only for first few decoder layers\n",
        "        if in_channels >= 256:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass with skip connections\"\"\"\n",
        "        # Encoding\n",
        "        e1 = self.enc1(x)  # (batch, 64, 1028, 175)\n",
        "        e2 = self.enc2(e1) # (batch, 128, 514, 87)\n",
        "        e3 = self.enc3(e2)  # (batch, 256, 257, 43)\n",
        "        e4 = self.enc4(e3) # (batch, 512, 128, 21)\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(e4)  # (batch, 1024, 64, 10)\n",
        "\n",
        "        # Decoding + Skip Connections with Spatial Attention\n",
        "        d4 = self.dec4(b)  # (batch, 512, ?, ?)\n",
        "        d4 = F.interpolate(d4, size=e4.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "        e4_attn = self.spatial_attn4(e4)  # Apply Spatial Attention\n",
        "        d4 = torch.cat([d4, e4_attn], dim=1)\n",
        "\n",
        "        d3 = self.dec3(d4)  # (batch, 256, ?, ?)\n",
        "        d3 = F.interpolate(d3, size=e3.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "        e3_attn = self.spatial_attn3(e3)  # Apply Spatial Attention\n",
        "        d3 = torch.cat([d3, e3_attn], dim=1)\n",
        "\n",
        "        d2 = self.dec2(d3)  # (batch, 128, ?, ?)\n",
        "        d2 = F.interpolate(d2, size=e2.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "        e2_attn = self.spatial_attn2(e2)  # Apply Spatial Attention\n",
        "        d2 = torch.cat([d2, e2_attn], dim=1)\n",
        "\n",
        "        d1 = self.dec1(d2)  # (batch, 64, ?, ?)\n",
        "        d1 = F.interpolate(d1, size=e1.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "        e1_attn = self.spatial_attn1(e1)  # Apply Spatial Attention\n",
        "        d1 = torch.cat([d1, e1_attn], dim=1)\n",
        "\n",
        "        # Final Convolution (output denoised spectrogram)\n",
        "        output = F.interpolate(self.final(d1), size=(1025, 175), mode=\"bilinear\", align_corners=False)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4\n",
        "TEST_MODEL = True"
      ],
      "metadata": {
        "id": "dwpBexNA9X0Q"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "xgzwlHooHoiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0fb5af-b10a-40f7-9bc3-9e10a917279b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output....\n",
            "torch.Size([4, 4, 1025, 175])\n"
          ]
        }
      ],
      "source": [
        "if TEST_MODEL:\n",
        "  if __name__ == \"__main__\":\n",
        "      x = torch.randn((BATCH_SIZE, 9, 1025, 175))\n",
        "      model = UNetConv9()\n",
        "      output = model(x)\n",
        "\n",
        "      print('output....')\n",
        "      print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "zB2SGHy6BEby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f873f3-6b44-4439-c243-7cabea3ba17b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running autoencoder tests...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "...\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 3.424s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 64, 509, 88]          17,344\n",
            "       BatchNorm2d-2          [-1, 64, 509, 88]             128\n",
            "         LeakyReLU-3          [-1, 64, 509, 88]               0\n",
            "            Conv2d-4          [-1, 64, 502, 88]         122,944\n",
            "       BatchNorm2d-5          [-1, 64, 502, 88]             128\n",
            "         LeakyReLU-6          [-1, 64, 502, 88]               0\n",
            "            Conv2d-7         [-1, 128, 250, 43]         204,928\n",
            "       BatchNorm2d-8         [-1, 128, 250, 43]             256\n",
            "         LeakyReLU-9         [-1, 128, 250, 43]               0\n",
            "           Conv2d-10         [-1, 128, 248, 41]         409,728\n",
            "      BatchNorm2d-11         [-1, 128, 248, 41]             256\n",
            "        LeakyReLU-12         [-1, 128, 248, 41]               0\n",
            "           Conv2d-13         [-1, 256, 124, 21]         295,168\n",
            "      BatchNorm2d-14         [-1, 256, 124, 21]             512\n",
            "        LeakyReLU-15         [-1, 256, 124, 21]               0\n",
            "           Conv2d-16         [-1, 256, 124, 21]         590,080\n",
            "      BatchNorm2d-17         [-1, 256, 124, 21]             512\n",
            "        LeakyReLU-18         [-1, 256, 124, 21]               0\n",
            "          Dropout-19         [-1, 256, 124, 21]               0\n",
            "           Conv2d-20          [-1, 512, 62, 11]       1,180,160\n",
            "      BatchNorm2d-21          [-1, 512, 62, 11]           1,024\n",
            "        LeakyReLU-22          [-1, 512, 62, 11]               0\n",
            "           Conv2d-23          [-1, 512, 62, 11]       2,359,808\n",
            "      BatchNorm2d-24          [-1, 512, 62, 11]           1,024\n",
            "        LeakyReLU-25          [-1, 512, 62, 11]               0\n",
            "          Dropout-26          [-1, 512, 62, 11]               0\n",
            "           Conv2d-27          [-1, 1024, 31, 6]       4,719,616\n",
            "      BatchNorm2d-28          [-1, 1024, 31, 6]           2,048\n",
            "        LeakyReLU-29          [-1, 1024, 31, 6]               0\n",
            "           Conv2d-30          [-1, 1024, 31, 6]       9,438,208\n",
            "      BatchNorm2d-31          [-1, 1024, 31, 6]           2,048\n",
            "        LeakyReLU-32          [-1, 1024, 31, 6]               0\n",
            "          Dropout-33          [-1, 1024, 31, 6]               0\n",
            "  ConvTranspose2d-34          [-1, 512, 63, 13]       4,719,104\n",
            "      BatchNorm2d-35          [-1, 512, 63, 13]           1,024\n",
            "        LeakyReLU-36          [-1, 512, 63, 13]               0\n",
            "          Dropout-37          [-1, 512, 63, 13]               0\n",
            "           Conv2d-38            [-1, 1, 62, 11]              98\n",
            "          Sigmoid-39            [-1, 1, 62, 11]               0\n",
            " SpatialAttention-40          [-1, 512, 62, 11]               0\n",
            "  ConvTranspose2d-41         [-1, 256, 125, 23]       2,359,552\n",
            "      BatchNorm2d-42         [-1, 256, 125, 23]             512\n",
            "        LeakyReLU-43         [-1, 256, 125, 23]               0\n",
            "          Dropout-44         [-1, 256, 125, 23]               0\n",
            "           Conv2d-45           [-1, 1, 124, 21]              98\n",
            "          Sigmoid-46           [-1, 1, 124, 21]               0\n",
            " SpatialAttention-47         [-1, 256, 124, 21]               0\n",
            "  ConvTranspose2d-48         [-1, 128, 251, 45]       1,638,528\n",
            "      BatchNorm2d-49         [-1, 128, 251, 45]             256\n",
            "        LeakyReLU-50         [-1, 128, 251, 45]               0\n",
            "          Dropout-51         [-1, 128, 251, 45]               0\n",
            "           Conv2d-52           [-1, 1, 248, 41]              98\n",
            "          Sigmoid-53           [-1, 1, 248, 41]               0\n",
            " SpatialAttention-54         [-1, 128, 248, 41]               0\n",
            "  ConvTranspose2d-55          [-1, 64, 504, 83]         491,584\n",
            "      BatchNorm2d-56          [-1, 64, 504, 83]             128\n",
            "        LeakyReLU-57          [-1, 64, 504, 83]               0\n",
            "          Dropout-58          [-1, 64, 504, 83]               0\n",
            "           Conv2d-59           [-1, 1, 502, 88]              98\n",
            "          Sigmoid-60           [-1, 1, 502, 88]               0\n",
            " SpatialAttention-61          [-1, 64, 502, 88]               0\n",
            "           Conv2d-62           [-1, 4, 495, 88]          15,364\n",
            "================================================================\n",
            "Total params: 28,572,364\n",
            "Trainable params: 28,572,364\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 6.16\n",
            "Forward/backward pass size (MB): 458.57\n",
            "Params size (MB): 108.99\n",
            "Estimated Total Size (MB): 573.72\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import unittest\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "\n",
        "class TestAutoencoder(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.model = UNetConv9()\n",
        "        self.input_channels = 9\n",
        "        self.output_channels = 4\n",
        "        self.input_height = 1025\n",
        "        self.input_width = 175\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def test_model_initialization(self):\n",
        "        self.assertIsInstance(self.model, UNetConv9, \"Model initialization failed\")\n",
        "\n",
        "    def test_forward_pass(self):\n",
        "        x = torch.randn(self.batch_size, self.input_channels, self.input_height, self.input_width, device=self.device)\n",
        "        output = self.model(x)\n",
        "        self.assertEqual(\n",
        "            output.shape,\n",
        "            (self.batch_size, self.output_channels, self.input_height, self.input_width),\n",
        "            f\"Expected output shape {(self.batch_size, self.output_channels, self.input_height, self.input_width)}, but got {output.shape}\"\n",
        "        )\n",
        "\n",
        "    def test_model_summary(self):\n",
        "        try:\n",
        "            summary(self.model, input_size=(self.input_channels, self.input_height, self.input_width))\n",
        "        except Exception as e:\n",
        "            self.fail(f\"Model summary failed: {str(e)}\")\n",
        "\n",
        "# This allows running tests externally\n",
        "def suite():\n",
        "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestAutoencoder)\n",
        "    return test_suite\n",
        "\n",
        "# runner\n",
        "class TestRunner:\n",
        "    def __init__(self):\n",
        "        self.runner = unittest.TextTestRunner()\n",
        "\n",
        "    def run(self):\n",
        "        print(\"Running autoencoder tests...\")\n",
        "        self.runner.run(suite())\n",
        "\n",
        "if TEST_MODEL:\n",
        "  if __name__ == \"__main__\":\n",
        "      runner = TestRunner()\n",
        "      runner.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihXSxknjq_Ks"
      },
      "source": [
        "## First train as an autoencoder for music"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q-mqM2myd0F"
      },
      "source": [
        "## Download file to local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "hNOpNkUFSL3C"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.plotting import *\n",
        "from audioautoencoder.datasets.utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzePP5Urw927",
        "outputId": "a69f756e-db7a-4b1a-8abd-812984f26fb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr: 0.001\n",
            "SNRdB: [0, 20]\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "train = True\n",
        "LOAD_DATA = True\n",
        "load_model = False\n",
        "\n",
        "# --------------- Main Execution parameters ---------------\n",
        "model_name = 'UNetConv9'\n",
        "SNRdB_load = [0, 20]\n",
        "SNRdBs = [[-10, 10]] # SNR random range\n",
        "load_trigger = [load_model]\n",
        "load_file = 'Autoencodermodel_earlystopping.pth'\n",
        "#load_file = 'Autoencodermodel_checkpoint.pth'\n",
        "\n",
        "folder = ['sep_features'][i] # sep\n",
        "\n",
        "# parameters\n",
        "learning_rates = [1e-3] # 1e-4 for re0training?, 1e-3 for training?\n",
        "\n",
        "base_lr=1e-5\n",
        "max_lr=learning_rates[i]\n",
        "gamma=0.8\n",
        "\n",
        "# data params\n",
        "max_file_size_gb = 100\n",
        "IMPORT_TRAIN_NOISY = train\n",
        "batch_size = 32\n",
        "\n",
        "# training params\n",
        "load = load_trigger[i]\n",
        "warm_start = False\n",
        "epochs = 100\n",
        "accumulation_steps = int(512/batch_size)\n",
        "\n",
        "SNRdB = SNRdBs[i]\n",
        "learning_rate = learning_rates[i]\n",
        "eta_min = 1e-5\n",
        "\n",
        "print('lr:', learning_rate)\n",
        "print('SNRdB:', SNRdB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7DKs1xyWxBdX"
      },
      "outputs": [],
      "source": [
        "# --------------- In Loop Parameters --------------\n",
        "output_path = f'/content/drive/MyDrive/Projects/ML_Projects/De-noising-autoencoder/Models_Denoising/Checkpoints_{model_name}_{SNRdB[0]}-{SNRdB[1]}/'\n",
        "load_path = f'/content/drive/MyDrive/Projects/ML_Projects/De-noising-autoencoder/Models_Denoising/Checkpoints_{model_name}_{SNRdB_load[0]}-{SNRdB_load[1]}/{load_file}'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import joblib  # or use pickle if you prefer\n",
        "\n",
        "def save_scalers(scalers, save_path):\n",
        "    \"\"\"Save scalers to a file.\"\"\"\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    joblib.dump(scalers, save_path)\n",
        "\n",
        "def load_scalers(save_path):\n",
        "    \"\"\"Load scalers from a file.\"\"\"\n",
        "    return joblib.load(save_path)"
      ],
      "metadata": {
        "id": "ISQ1XFZQuV2z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"1ciaran.maloy@gmail.com\"\n",
        "!git config --global user.name \"ciaran.maloy\""
      ],
      "metadata": {
        "id": "GNbLPBvRSKqB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "id": "MbBgApe4THxP",
        "outputId": "39ca3216-90bc-47f2-f331-362fc5213720",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch bandchannels\n",
            "Your branch is up to date with 'origin/bandchannels'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2Ju-nK9SmZr",
        "outputId": "5dab18d0-e2cd-40bb-f946-4fa8e11ae579"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5MW2Gsna7ku_"
      },
      "outputs": [],
      "source": [
        "# Define the source and destination file paths\n",
        "if LOAD_DATA:\n",
        "  scaler_file = output_path + \"scalers.pkl\"  # Static filename since it's unique per run\n",
        "  source_folder = f\"/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_{folder}/SNRdB_{SNRdB[0]}-{SNRdB[1]}/\"\n",
        "  source_path = source_folder + \"train/\"\n",
        "  destination_path = f\"/content/SNRdB_{SNRdB[0]}-{SNRdB[1]}/train/\"\n",
        "  save_path = source_folder + \"combined_000.h5\"\n",
        "  subset = False\n",
        "\n",
        "  if IMPORT_TRAIN_NOISY:\n",
        "    dataset_path = f\"/content/SNRdB_{SNRdB[0]}-{SNRdB[1]}/train/combined_000.h5\"\n",
        "    if not os.path.exists(destination_path):\n",
        "      combine_h5_files_features(source_path, destination_path, max_file_size_gb=max_file_size_gb)\n",
        "\n",
        "    if os.path.exists(scaler_file):\n",
        "        print(\"Loading existing scalers...\")\n",
        "        scalers = load_scalers(scaler_file)\n",
        "    else:\n",
        "        print(\"Training new scalers...\")\n",
        "        scalers = train_scalers_separation(dataset_path, sample_size=8000)\n",
        "        save_scalers(scalers, scaler_file)\n",
        "\n",
        "    print(scalers)\n",
        "\n",
        "    train_loader = ChannelDatasetLoader(\n",
        "          dataset_path=dataset_path,\n",
        "          scalers=scalers,\n",
        "          output_time_length=175,\n",
        "          channels=1,\n",
        "          snr_db=SNRdB,\n",
        "          subset=subset,\n",
        "          batch_size=batch_size\n",
        "      )\n",
        "\n",
        "    print(f\"Training set size: {len(train_loader.train_dataset)}\")\n",
        "    print(f\"Validation set size: {len(train_loader.val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_DATA:\n",
        "  input, output, medatata = train_loader.train_dataset[200]\n",
        "  print(input.shape)\n",
        "  print(output.shape)"
      ],
      "metadata": {
        "id": "l6-sIxJ-qEDC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_DATA:\n",
        "  import matplotlib.pyplot as plt\n",
        "  import torch\n",
        "\n",
        "  # Fetch a sample\n",
        "  input_tensor, output_tensor, metadata = train_loader.train_dataset[50]\n",
        "\n",
        "  # Convert to NumPy for plotting\n",
        "  input_array = input_tensor.numpy()\n",
        "  output_array = output_tensor.numpy()\n",
        "\n",
        "  # Number of input channels\n",
        "  num_channels_in = input_array.shape[0]\n",
        "  num_channels_out = output_array.shape[0]\n",
        "\n",
        "  # Create subplots\n",
        "  fig, axes = plt.subplots(num_channels_in + num_channels_out, 1, figsize=(15, 30))\n",
        "\n",
        "  # Plot each input channel\n",
        "  for i in range(num_channels_in):\n",
        "      input = input_array[i]\n",
        "      print('Min, Max: ', np.min(input), np.max(input))\n",
        "      im = axes[i].imshow(input, aspect='auto', cmap='magma')\n",
        "      axes[i].invert_yaxis()\n",
        "\n",
        "      axes[i].set_title(f\"Input Channel {i+1}\")\n",
        "      axes[i].axis(\"off\")\n",
        "\n",
        "      # Add colorbar\n",
        "      cbar = fig.colorbar(im, ax=axes[i], orientation=\"vertical\")\n",
        "      cbar.set_label(\"Amplitude\")\n",
        "\n",
        "    # Plot each input channel\n",
        "  for i in range(num_channels_out):\n",
        "      output = output_array[i]\n",
        "      print('Min, Max: ', np.min(output), np.max(output))\n",
        "      im = axes[num_channels_in + i].imshow(output, aspect='auto', cmap='magma')\n",
        "      axes[num_channels_in + i].invert_yaxis()\n",
        "\n",
        "      axes[num_channels_in + i].set_title(f\"Output Channel {i+1}\")\n",
        "      axes[num_channels_in + i].axis(\"off\")\n",
        "\n",
        "      # Add colorbar\n",
        "      cbar = fig.colorbar(im, ax=axes[num_channels_in + i], orientation=\"vertical\")\n",
        "      cbar.set_label(\"Amplitude\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "YO8_4lXBtXim"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsVtpS6ksh_9"
      },
      "source": [
        "# Retrain Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MNVGzk5jK856"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.loss import *\n",
        "from audioautoencoder.utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "v7M1VlCI5WHr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "R2BLTePNsRoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5f97286-c1a6-467d-8bdb-a783e3c4eb9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the model, define loss function and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNetConv8().to(device)\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "SGiVzAgVD0Ra"
      },
      "outputs": [],
      "source": [
        "if load:\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "  #scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=eta_min)\n",
        "  scheduler_loss = False\n",
        "else:\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=eta_min)\n",
        "  scheduler_loss = False\n",
        "\n",
        "  #optimizer = None #torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "  #scheduler = None #torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
        "  #scheduler_loss = False #True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clears all allocated GPU memory in PyTorch.\"\"\"\n",
        "    torch.cuda.empty_cache()  # Clears cache\n",
        "    gc.collect()  # Runs Python garbage collector\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        torch.cuda.reset_peak_memory_stats(i)  # Resets peak memory tracking\n",
        "\n",
        "clear_gpu_memory()\n"
      ],
      "metadata": {
        "id": "6pxw01Uptmvj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tv3swntPn-kj"
      },
      "outputs": [],
      "source": [
        "if train:\n",
        "  trainer = DenoisingTrainer(\n",
        "      model=model, noisy_train_loader=train_loader.train_loader, noisy_val_loader=train_loader.val_loader,\n",
        "      SNRdB=SNRdB, output_path=output_path, epochs=epochs, learning_rate=learning_rate,\n",
        "      load=load, warm_start=warm_start, train=train, verbose=False, accumulation_steps=accumulation_steps, load_path=load_path,\n",
        "      base_lr=base_lr, max_lr=max_lr, gamma=gamma, optimizer=optimizer, scheduler=scheduler, scheduler_loss=scheduler_loss,\n",
        "      max_noise=0.01, noise_epochs=5\n",
        "  )\n",
        "  trainer.train_or_evaluate()\n",
        "  model = trainer.get_model()\n",
        "\n",
        "  # I need a flat load model function somewhere, as now I need to define a train loader before I can load a model\n",
        "  csv_file_path = output_path + \"training_log.csv\"\n",
        "  plot_training_log(csv_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VFPK5D5Xab2"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNetConv8().to(device)\n",
        "denoiser = DenoisingLoader(model, load_path)\n",
        "model = denoiser.model\n",
        "print('Loaded Model')\n",
        "\n",
        "# Example input (batch_size=1, channels=2, height=1025, width=175)\n",
        "noisy_input = torch.randn(1, 9, 1025, 175)\n",
        "\n",
        "denoised_output = denoiser.denoise(noisy_input)\n",
        "print(denoised_output.shape)"
      ],
      "metadata": {
        "id": "U9A8qCuyqbam",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77635293-4681-4778-8ad8-036ed2f48d21"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/audioautoencoder/audioautoencoder/training.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=self.device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model from /content/drive/MyDrive/Projects/ML_Projects/De-noising-autoencoder/Models_Denoising/Checkpoints_UNetConv8_0-20/Autoencodermodel_earlystopping.pth\n",
            "Loaded Model\n",
            "torch.Size([1, 4, 1025, 175])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r-L1IZ-l9O7"
      },
      "source": [
        "## Testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "GAcVayA_pesy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb869c69-5cf1-44c7-c027-2e5399d3c094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 20]\n"
          ]
        }
      ],
      "source": [
        "# Define the source and destination file paths\n",
        "SNRdB = SNRdB # SNR random range\n",
        "print(SNRdB)\n",
        "#filename = f\"train-SNRdB_{SNRdB}-1s-44-1khz-magnitude-freqweightmagnitude-phase.h5\"\n",
        "source_folder = f\"/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_{folder}/SNRdB_{SNRdB[0]}-{SNRdB[1]}/\"\n",
        "source_path = source_folder + \"test/\"\n",
        "destination_path = f\"/content/SNRdB_{SNRdB[0]}-{SNRdB[1]}/test/\"\n",
        "scaler_file = output_path + \"scalers.pkl\"  # Static filename since it's unique per run\n",
        "\n",
        "save_path = destination_path + \"combined_000.h5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "cQwjhSZJpesz"
      },
      "outputs": [],
      "source": [
        "IMPORT_TEST_NOISY = False\n",
        "load_dataframe = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "EEucu19Apesz"
      },
      "outputs": [],
      "source": [
        "max_file_size_gb = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "E_338U2bpesz"
      },
      "outputs": [],
      "source": [
        "destination_path = f\"/content/SNRdB_{SNRdB[0]}-{SNRdB[1]}/test/\"\n",
        "\n",
        "if IMPORT_TEST_NOISY:\n",
        "  if not os.path.exists(destination_path):\n",
        "    combine_h5_files_features(source_path, destination_path, max_file_size_gb=max_file_size_gb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_YpNU8IjC8V"
      },
      "source": [
        "View Pretrained Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "jq8WWxdalRhK"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.datasets.utils import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(scaler_file):\n",
        "    print(\"Loading existing scalers...\")\n",
        "    scalers = load_scalers(scaler_file)\n",
        "else:\n",
        "    print(\"Training new scalers...\")\n",
        "    scalers = train_scalers_separation(dataset_path, sample_size=8000)\n",
        "    save_scalers(scalers, scaler_file)"
      ],
      "metadata": {
        "id": "y6qyNPFoPIu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f102800-7eb0-4bfb-f7a9-04039c2f626f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing scalers...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "dbFb6LzXlRhL"
      },
      "outputs": [],
      "source": [
        "if IMPORT_TEST_NOISY:\n",
        "    print(\"Loading existing scalers...\")\n",
        "    scalers = load_scalers(scaler_file)\n",
        "    test_loader = ChannelDatasetLoader(\n",
        "          dataset_path=save_path,\n",
        "          scalers=scalers,\n",
        "          output_time_length=175,\n",
        "          channels=1,\n",
        "          snr_db=SNRdB,\n",
        "          subset=False,\n",
        "          batch_size=4\n",
        "      )\n",
        "\n",
        "    print(f\"Training set size: {len(test_loader.train_dataset)}\")\n",
        "    print(f\"Validation set size: {len(test_loader.val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "YLtXGJboq-vp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8821bbc-bc41-4401-abca-44e8a5786ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m4TfuxEJ0ti"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.system(\"pip install --upgrade torchmetrics\")"
      ],
      "metadata": {
        "id": "mycn_B1xfe36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f1b712-8a7b-489a-9382-bb0ace2be723"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "LyyEUEWoLXYs",
        "outputId": "8ceb8200-c5a9-4440-92e6-18af53d84e91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchmetrics'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-f5e3a382aad4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0maudioautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/audioautoencoder/audioautoencoder/testing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSignalDistortionRatio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mEvaluation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchmetrics'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from audioautoencoder.testing import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTS6rjFFHy5T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from loss import *\n",
        "\n",
        "# Testing loop\n",
        "def test_model(model, test_loader, criterion, scalers):\n",
        "    evaluation = Evaluation(scalers)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0.0\n",
        "        progress_bar = tqdm(test_loader, desc=\"Testing\", unit=\"batch\")\n",
        "        for inputs, targets, metadata in progress_bar:\n",
        "\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "          test_loss += loss.item()\n",
        "\n",
        "          # evaluation\n",
        "          evaluation.evaluate(inputs, targets, outputs, metadata)\n",
        "\n",
        "        test_loss /= len(test_loader)\n",
        "\n",
        "    return test_loss, evaluation.process()\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torchmetrics.audio import SignalDistortionRatio\n",
        "\n",
        "class Evaluation:\n",
        "    def __init__(self, scalers, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "        \"\"\"Initialize storage for evaluation metrics.\"\"\"\n",
        "        self.results = []\n",
        "        self.device = torch.device(device)  # Store the device\n",
        "        self.sdr = SignalDistortionRatio().to(self.device)  # Move SDR metric to the device\n",
        "        self.scalers = scalers\n",
        "\n",
        "\n",
        "    def evaluate(self, inputs, targets, outputs, metadata):\n",
        "        \"\"\"\n",
        "        Compute SDR and L1 loss for input vs. target and input vs. output.\n",
        "        \"\"\"\n",
        "        batch_size = inputs.shape[0]\n",
        "        chunk_length = 44100 * 2\n",
        "\n",
        "        # I need to inverse the standardisation for this, as it is skewing the results\n",
        "\n",
        "        # Convert tensors to NumPy\n",
        "        inputs = inputs.detach().cpu().numpy()\n",
        "        targets = targets.detach().cpu().numpy()\n",
        "        outputs = outputs.detach().cpu().numpy()\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            filename = metadata[i][\"filename\"]\n",
        "            snr_db = metadata[i][\"snr_db\"]\n",
        "            phase = metadata[i][\"phase\"]\n",
        "            lf_shape = metadata[i][\"lf_shape\"]\n",
        "\n",
        "            input = inputs[i]\n",
        "            output = outputs[i]\n",
        "            target = targets[i]\n",
        "\n",
        "            in_track = input.copy()\n",
        "            out_track = output.copy()\n",
        "            tar_track = target.copy()\n",
        "\n",
        "            # Inverse standardisation\n",
        "            #input_temp = input[0]\n",
        "            #input[0] = self.scalers[\"input_features_spectrogram\"].transform(input_temp.reshape(1, -1)).reshape(input_temp.shape)\n",
        "            #output_temp = output[0]\n",
        "            #output[0] = self.scalers[\"target_features_spectrogram\"].transform(output_temp.reshape(1, -1)).reshape(output_temp.shape)\n",
        "            #target_temp = target[0]\n",
        "            #target[0] = self.scalers[\"target_features_spectrogram\"].transform(target_temp.reshape(1, -1)).reshape(target_temp.shape)\n",
        "\n",
        "            #input_chunk = magphase_to_waveform(input[0], input[1], chunk_length)\n",
        "            #output_chunk = magphase_to_waveform(output[0], input[1], chunk_length)\n",
        "            #target_chunk = magphase_to_waveform(target[0], input[1], chunk_length)\n",
        "\n",
        "            # Move tensors to the correct device\n",
        "            #input_chunk = torch.from_numpy(input_chunk).to(self.device).float()\n",
        "            #output_chunk = torch.from_numpy(output_chunk).to(self.device).float()\n",
        "            #target_chunk = torch.from_numpy(target_chunk).to(self.device).float()\n",
        "\n",
        "            input = torch.from_numpy(input).to(self.device).float()\n",
        "            output = torch.from_numpy(output).to(self.device).float()\n",
        "            target = torch.from_numpy(target).to(self.device).float()\n",
        "\n",
        "            # Compute SDR (using torchaudio)\n",
        "            #sdr_invstar = self.sdr(input_chunk, target_chunk).item()\n",
        "            #sdr_outvstar = self.sdr(output_chunk, target_chunk).item()\n",
        "\n",
        "            # Compute L1 loss\n",
        "            l1_invstar = F.l1_loss(input[0:4, :, :], target[0:4, :, :]).item()\n",
        "            l1_outvstar = F.l1_loss(output[0:4, :, :], target[0:4, :, :]).item()\n",
        "\n",
        "            l1_invstar_4k = F.l1_loss(input[1:2, :, :], target[1:2, :, :]).item()\n",
        "            l1_outvstar_4k = F.l1_loss(output[1:2, :, :], target[1:2, :, :]).item()\n",
        "\n",
        "            l1_invstar_full = F.l1_loss(input[0:1, :, :], target[0:1, :, :]).item()\n",
        "            l1_outvstar_full = F.l1_loss(output[0:1, :, :], target[0:1, :, :]).item()\n",
        "\n",
        "            # Store results\n",
        "            self.results.append({\n",
        "                \"instance\": len(self.results),\n",
        "                #\"sdr_invstar\": sdr_invstar,\n",
        "                #\"sdr_outvstar\": sdr_outvstar,\n",
        "                \"l1_invstar\": l1_invstar,\n",
        "                \"l1_outvstar\": l1_outvstar,\n",
        "                \"l1_invstar_4k\": l1_invstar_4k,\n",
        "                \"l1_outvstar_4k\": l1_outvstar_4k,\n",
        "                \"l1_invstar_full\": l1_invstar_full,\n",
        "                \"l1_outvstar_full\": l1_outvstar_full,\n",
        "                \"filename\": filename,\n",
        "                \"snr_db\": snr_db,\n",
        "                \"in_track\":in_track,\n",
        "                \"out_track\":out_track,\n",
        "                \"tar_track\":tar_track,\n",
        "                \"metadata\": metadata[i],\n",
        "            })\n",
        "\n",
        "    def process(self):\n",
        "        \"\"\"Return the stored evaluation results as a Pandas DataFrame.\"\"\"\n",
        "        return pd.DataFrame(self.results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if load_dataframe:\n",
        "  df_subset = pd.read_csv(output_path + f\"df_subset_SNRdB_{SNRdB[0]}-{SNRdB[1]}.csv\")"
      ],
      "metadata": {
        "id": "OthiH-tDgRC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ8jaUW4j3Pr"
      },
      "outputs": [],
      "source": [
        "if not load_dataframe:\n",
        "  criterion = nn.L1Loss()\n",
        "  loss, df_eval = test_model(model, test_loader.train_loader, criterion, scalers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrOfxekHV0C3"
      },
      "outputs": [],
      "source": [
        "if not load_dataframe:\n",
        "  # Assuming `df` is your original dataframe\n",
        "  #df_eval[\"Improvement\"] = df_eval[\"l1_outvstar\"] df_eval[\"l1_invstar\"]  # Higher SDR is better\n",
        "  subset_columns = [\"instance\", \"l1_invstar\", \"l1_outvstar\", \"l1_invstar_4k\", \"l1_outvstar_4k\", \"l1_invstar_full\", \"l1_outvstar_full\",  \"filename\", \"snr_db\"] #\"Improvement\"]\n",
        "  df_subset = df_eval#[subset_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "istycCl4X5bQ"
      },
      "outputs": [],
      "source": [
        "if not load_dataframe:\n",
        "  # Create a function to map filename to a class\n",
        "  def get_class_from_filename(filename, classes):\n",
        "      for keyword in classes:\n",
        "          if keyword in filename:\n",
        "              return keyword\n",
        "      return 'Unknown'  # Default if no match found\n",
        "\n",
        "  df_subset[['filename_audio', 'filename_noise']] = pd.DataFrame(df_subset['filename'].tolist(), index=df_subset.index)\n",
        "  df_subset['filename_audio'] = df_subset['filename_audio'].apply(lambda x: x.decode('utf-8'))\n",
        "  df_subset['filename_noise'] = df_subset['filename_noise'].apply(lambda x: x.decode('utf-8'))\n",
        "\n",
        "  classes = ['mixture', 'vocals', 'drums', 'guitar', 'bass', 'piano', 'electric_guitar', 'acoustic_guitar', 'synthesizer', 'strings', 'brass']\n",
        "  df_subset['audio_class'] = df_subset['filename_audio'].apply(lambda x: get_class_from_filename(x, classes))\n",
        "\n",
        "  classes = ['0707', 'Rain', 'Crowd', 'Water', 'Ice']\n",
        "  df_subset['noise_class'] = df_subset['filename_noise'].apply(lambda x: get_class_from_filename(x, classes))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_subset[\"Improvement_L1\"] = df_subset[\"l1_invstar\"] - df_subset[\"l1_outvstar\"]  # Lower L1 loss is better\n",
        "df_subset[\"Improvement_L1_4k\"] = df_subset[\"l1_invstar_4k\"] - df_subset[\"l1_outvstar_4k\"]  # Lower L1 loss is better\n",
        "df_subset[\"Improvement_L1_full\"] = df_subset[\"l1_invstar_full\"] - df_subset[\"l1_outvstar_full\"]  # Lower L1 loss is better"
      ],
      "metadata": {
        "id": "XQ9SAAWnm1fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def threshold_spectrogram(spectrogram, threshold):\n",
        "    \"\"\"\n",
        "    Zeroes out all values in the spectrogram that are below the given threshold.\n",
        "\n",
        "    Args:\n",
        "        spectrogram (np.ndarray): Input 2D array.\n",
        "        threshold (float): The threshold value.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The processed spectrogram with values below threshold set to zero.\n",
        "    \"\"\"\n",
        "    spectrogram = np.where(spectrogram >= threshold, spectrogram, 0)\n",
        "    return spectrogram"
      ],
      "metadata": {
        "id": "yqV50cKVMA-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_rate = 44100  # 44.1 kHz audio\n",
        "n_fft = 2048  # Adjust this for better resolution\n",
        "freqs = np.linspace(0, sampling_rate / 2, n_fft // 2 + 1)  # STFT frequency bins\n",
        "\n",
        "# Find indices corresponding to 0–4000 Hz\n",
        "min_freq, max_freq = 0, 4000\n",
        "freq_indices = np.where((freqs >= min_freq) & (freqs <= max_freq))[0]\n",
        "\n",
        "# in spectrogram\n",
        "index = 9\n",
        "\n",
        "snr_db = np.array(df_subset.loc[index, \"snr_db\"])\n",
        "print(snr_db)\n",
        "\n",
        "# lets evaluate this from a l1 loss perspective\n",
        "# reconstruct spectrogram\n",
        "out_spectrogram = np.array(df_subset.loc[index, \"out_track\"][0])\n",
        "out_spectrogram[df_subset.loc[index, \"metadata\"][\"freq_indices_hf\"], :] = resample_feature(np.array(df_subset.loc[index, \"out_track\"][1]), df_subset.loc[index, \"metadata\"][\"hf_shape\"])\n",
        "out_spectrogram[df_subset.loc[index, \"metadata\"][\"freq_indices_mf\"], :] = resample_feature(np.array(df_subset.loc[index, \"out_track\"][2]), df_subset.loc[index, \"metadata\"][\"mf_shape\"])\n",
        "out_spectrogram[df_subset.loc[index, \"metadata\"][\"freq_indices_lf\"], :] = resample_feature(np.array(df_subset.loc[index, \"out_track\"][3]), df_subset.loc[index, \"metadata\"][\"lf_shape\"])\n",
        "out_spec_copy = out_spectrogram\n",
        "\n",
        "out_spectrogram = threshold_spectrogram(out_spectrogram, np.mean(out_spectrogram)*0.75)\n",
        "\n",
        "# out, with no join\n",
        "out_track = np.array(df_subset.loc[index, \"out_track\"])[0]\n",
        "\n",
        "# out spectrogram\n",
        "in_spectrogram = df_subset.loc[index, \"in_track\"][0]\n",
        "\n",
        "# target\n",
        "tar_track = np.array(df_subset.loc[index, \"tar_track\"])[0]\n",
        "\n",
        "# inverse normalisation to 0 - 1\n",
        "out_spectrogram = (out_spectrogram - 0.5) * 3\n",
        "out_track = (out_track - 0.5) * 3\n",
        "in_spectrogram = (in_spectrogram - 0.5) * 3\n",
        "tar_track = (tar_track - 0.5) * 3\n",
        "\n",
        "# Inverse standardisation\n",
        "input_temp = tar_track\n",
        "in_spectrogram = scalers[\"input_features_spectrogram\"].inverse_transform(in_spectrogram.reshape(1, -1)).reshape(input_temp.shape)\n",
        "\n",
        "out_spectrogram = scalers[\"target_features_spectrogram\"].inverse_transform(out_spectrogram.reshape(1, -1)).reshape(input_temp.shape)\n",
        "out_track = scalers[\"target_features_spectrogram\"].inverse_transform(out_track.reshape(1, -1)).reshape(input_temp.shape)\n",
        "\n",
        "tar_track = scalers[\"target_features_spectrogram\"].inverse_transform(tar_track.reshape(1, -1)).reshape(input_temp.shape)\n",
        "\n",
        "# plot things\n",
        "# Plot spectrograms\n",
        "fig, axes = plt.subplots(4, 1, figsize=(15, 15))\n",
        "\n",
        "axes[0].imshow(in_spectrogram, aspect=\"auto\", cmap=\"magma\", origin=\"lower\")\n",
        "axes[0].set_title(\"Noisy Input (Log Scale)\")\n",
        "axes[0].set_yscale(\"log\")\n",
        "axes[0].set_ylim((1, 1000))\n",
        "\n",
        "axes[1].imshow(out_spectrogram, aspect=\"auto\", cmap=\"magma\", origin=\"lower\")\n",
        "axes[1].set_title(\"Denoised Output (Log Scale) - reconstructed\")\n",
        "axes[1].set_yscale(\"log\")\n",
        "axes[1].set_ylim((1, 1000))\n",
        "\n",
        "axes[2].imshow(out_track, aspect=\"auto\", cmap=\"magma\", origin=\"lower\")\n",
        "axes[2].set_title(\"Denoised Output (Log Scale)\")\n",
        "axes[2].set_yscale(\"log\")\n",
        "axes[2].set_ylim((1, 1000))\n",
        "\n",
        "axes[3].imshow(tar_track, aspect=\"auto\", cmap=\"magma\", origin=\"lower\")\n",
        "axes[3].set_title(\"Clean Target (Log Scale)\")\n",
        "axes[3].set_yscale(\"log\")\n",
        "axes[3].set_ylim((1, 1000))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GNrvnkw4Yo04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def magphase_to_waveform(magnitude, phase, audio_length=44100):\n",
        "    \"\"\"\n",
        "    Converts a spectrogram image back into an audio waveform.\n",
        "\n",
        "    Parameters:\n",
        "        image (np.array): Spectrogram image (3 channels).\n",
        "        sr (int): Sampling rate.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Reconstructed audio waveform.\n",
        "    \"\"\"\n",
        "    stft = magnitude * np.exp(1j * phase)\n",
        "    return librosa.istft(stft, length=audio_length)"
      ],
      "metadata": {
        "id": "XVYDvDepaM-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io.wavfile\n",
        "from google.colab import files\n",
        "import librosa\n",
        "\n",
        "# output waveform\n",
        "phase = df_subset.loc[index, \"metadata\"][\"phase\"]\n",
        "#phase = scalers[\"input_features_phase\"].inverse_transform(phase.reshape(1, -1)).reshape(input_temp.shape)\n",
        "print(np.max(phase))\n",
        "print(np.min(phase))\n",
        "\n",
        "# reverse log scale\n",
        "out_spectrogram = librosa.db_to_amplitude(out_spectrogram)\n",
        "signal = magphase_to_waveform(out_spectrogram, phase, 44100 * 2)\n",
        "\n",
        "# Save as WAV file\n",
        "output_filename = f\"denoised_audio_{index}:{snr_db}.wav\"\n",
        "scipy.io.wavfile.write(output_filename, rate=44100, data=signal)  # 16-bit PCM\n",
        "\n",
        "# Download the file\n",
        "files.download(output_filename)\n",
        "\n",
        "tar_track = librosa.db_to_amplitude(tar_track)\n",
        "signal = magphase_to_waveform(tar_track, phase, 44100 * 2)\n",
        "\n",
        "# Save as WAV file\n",
        "output_filename = f\"audio_{index}:{snr_db}.wav\"\n",
        "scipy.io.wavfile.write(output_filename, rate=44100, data=signal)  # 16-bit PCM\n",
        "\n",
        "# Download the file\n",
        "files.download(output_filename)\n",
        "\n",
        "in_spectrogram = librosa.db_to_amplitude(in_spectrogram)\n",
        "signal = magphase_to_waveform(in_spectrogram, phase, 44100 * 2)\n",
        "\n",
        "# Save as WAV file\n",
        "output_filename = f\"noisy_audio_{index}:{snr_db}.wav\"\n",
        "scipy.io.wavfile.write(output_filename, rate=44100, data=signal)  # 16-bit PCM\n",
        "\n",
        "# Download the file\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "id": "a9RpJcI7Zkw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp03uLRHVuG9"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Set minimal theme\n",
        "sns.set_theme(style=\"white\", font_scale=1.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "df_eval = df_subset\n",
        "\n",
        "# Round SNR values to the nearest 0.5 to reduce noise\n",
        "df_eval[\"snr_db_rounded\"] = df_eval[\"snr_db\"].round(1)  # Rounds to 1 decimal place\n",
        "df_eval[\"snr_db_rounded\"] = (df_eval[\"snr_db_rounded\"] * 2).round() / 2  # Ensures nearest 0.5\n",
        "\n",
        "# Group by rounded SNR and audio/noise class, then compute mean improvement\n",
        "df_audio_avg = df_eval.groupby([\"snr_db_rounded\", \"audio_class\"], as_index=False)[\"Improvement_L1\"].mean()\n",
        "df_noise_avg = df_eval.groupby([\"snr_db_rounded\", \"noise_class\"], as_index=False)[\"Improvement_L1\"].mean()\n",
        "\n",
        "# Line plot colored by 'audio_class'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_audio_avg, x=\"snr_db_rounded\", y=\"Improvement_L1\", hue=\"audio_class\", palette=\"tab10\", marker=\"o\")\n",
        "plt.xlabel(\"SNR (dB)\")\n",
        "plt.ylabel(\"Mean Improvement (SDR)\")\n",
        "plt.title(\"Mean Improvement vs SNR (Rounded to 0.5, Colored by Audio Class)\")\n",
        "plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Line plot colored by 'noise_class'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_noise_avg, x=\"snr_db_rounded\", y=\"Improvement_L1\", hue=\"noise_class\", palette=\"tab10\", marker=\"o\")\n",
        "plt.xlabel(\"SNR (dB)\")\n",
        "plt.ylabel(\"Mean Improvement (SDR)\")\n",
        "plt.title(\"Mean Improvement vs SNR (Rounded to 0.5, Colored by Noise Class)\")\n",
        "plt.legend(title=\"Noise Class\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "x_9TP5JJnAmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "df_eval = df_subset\n",
        "\n",
        "# Round SNR values to the nearest 0.5 to reduce noise\n",
        "df_eval[\"snr_db_rounded\"] = df_eval[\"snr_db\"].round(1)  # Rounds to 1 decimal place\n",
        "df_eval[\"snr_db_rounded\"] = (df_eval[\"snr_db_rounded\"] * 2).round() / 2  # Ensures nearest 0.5\n",
        "\n",
        "# Group by rounded SNR and audio/noise class, then compute mean improvement\n",
        "df_audio_avg = df_eval.groupby([\"snr_db_rounded\", \"audio_class\"], as_index=False)[\"Improvement_L1_4k\"].mean()\n",
        "df_noise_avg = df_eval.groupby([\"snr_db_rounded\", \"noise_class\"], as_index=False)[\"Improvement_L1_4k\"].mean()\n",
        "\n",
        "# Line plot colored by 'audio_class'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_audio_avg, x=\"snr_db_rounded\", y=\"Improvement_L1_4k\", hue=\"audio_class\", palette=\"tab10\", marker=\"o\")\n",
        "plt.xlabel(\"SNR (dB)\")\n",
        "plt.ylabel(\"Mean Improvement (SDR)\")\n",
        "plt.title(\"Mean Improvement vs SNR (Rounded to 0.5, Colored by Audio Class)\")\n",
        "plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Line plot colored by 'noise_class'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_noise_avg, x=\"snr_db_rounded\", y=\"Improvement_L1_4k\", hue=\"noise_class\", palette=\"tab10\", marker=\"o\")\n",
        "plt.xlabel(\"SNR (dB)\")\n",
        "plt.ylabel(\"Mean Improvement (SDR)\")\n",
        "plt.title(\"Mean Improvement vs SNR (Rounded to 0.5, Colored by Noise Class)\")\n",
        "plt.legend(title=\"Noise Class\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1G_7pHjli0R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = df_subset\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax = sns.boxplot(x=\"noise_class\", y=\"Improvement_L1\", hue=\"audio_class\", data=df)\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.ylim(-0.5, 0.5)\n",
        "plt.savefig(output_path + f\"boxplot_all_L1.png\")\n",
        "plt.show()\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter for 'crowd' noise class\n",
        "df_crowd = df_subset[df_subset[\"noise_class\"] == \"Crowd\"].copy()\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(7, 5))\n",
        "ax = sns.boxplot(x=\"audio_class\", y=\"Improvement_L1\", hue=\"audio_class\", data=df_crowd)\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks()\n",
        "#plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.ylim(-0.5, 0.5)\n",
        "plt.savefig(output_path + f\"boxplot_crowd_L1.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "os6v-7ziieOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = df_subset\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax = sns.boxplot(x=\"noise_class\", y=\"Improvement_L1_4k\", hue=\"audio_class\", data=df)\n",
        "\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.ylim(-0.5, 0.5)\n",
        "plt.savefig(output_path + f\"boxplot_all_L1_4k.png\")\n",
        "plt.show()\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter for 'crowd' noise class\n",
        "df_crowd = df_subset[df_subset[\"noise_class\"] == \"Crowd\"].copy()\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(7, 5))\n",
        "ax = sns.boxplot(x=\"audio_class\", y=\"Improvement_L1_4k\", hue=\"audio_class\", data=df_crowd)\n",
        "\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks()\n",
        "#plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.ylim(-0.5, 0.5)\n",
        "plt.savefig(output_path + f\"boxplot_crowd_L1_4k.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Bo58Je9fjWtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = df_subset\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax = sns.boxplot(x=\"noise_class\", y=\"Improvement_L1_full\", hue=\"audio_class\", data=df)\n",
        "\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.savefig(output_path + f\"boxplot_all_L1_full.png\")\n",
        "plt.show()\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter for 'crowd' noise class\n",
        "df_crowd = df_subset[df_subset[\"noise_class\"] == \"Crowd\"].copy()\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(7, 5))\n",
        "ax = sns.boxplot(x=\"audio_class\", y=\"Improvement_L1_full\", hue=\"audio_class\", data=df_crowd)\n",
        "\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks()\n",
        "#plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.savefig(output_path + f\"boxplot_crowd_L1_full.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "L2DTNG0uoW_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3AQwpWnXgob"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you already have the dataframe loaded in `df`\n",
        "# df = pd.read_csv('your_data.csv')  # Uncomment if loading from CSV\n",
        "df = df_subset\n",
        "\n",
        "# You can also add visualization here if you want to dive deeper\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a more interpretable colormap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df[['l1_invstar', 'l1_outvstar', 'snr_db', 'Improvement_L1']].corr(),\n",
        "            annot=True, cmap='mako', fmt=\".2f\", vmin=-1, vmax=1, center=0)\n",
        "\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save subset dataframe\n",
        "df_subset.to_csv(output_path + f\"df_subset_SNRdB_{SNRdB[0]}-{SNRdB[1]}.csv\", index=False)"
      ],
      "metadata": {
        "id": "zVFDKMPLS42d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "# Delete large variables\n",
        "del df_subset, df_eval, df\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "HWdVoElDc6IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvCNTO7up5e8"
      },
      "source": [
        "# Improvements that need to be made\n",
        "\n",
        "1. Metadata h5 column, including\n",
        "- Filename\n",
        "- SNR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cooVhUuEQ5aF"
      },
      "source": [
        "## Convert some entire songs and test some metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvNP8ZZwQ2xF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "whole_files = '/content/drive/MyDrive/Datasets/Music/MUSDB18/test/'\n",
        "song_files = []\n",
        "\n",
        "# Walk through the directory tree\n",
        "for root, dirs, files in os.walk(whole_files):\n",
        "    # Filter files with '.wav' extension and 'mixture' in their name\n",
        "    for f in files:\n",
        "        if f.endswith('.wav') and 'mixture' in f:\n",
        "            full_path = os.path.join(root, f)\n",
        "            song_files.append(full_path)\n",
        "\n",
        "print(f\"\\nTotal matching files: {len(song_files)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hbrf6o-X0xX"
      },
      "source": [
        "Generate Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQYQ8bH4mSai"
      },
      "outputs": [],
      "source": [
        "#noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/val/crowd noise (4)_xDoJJ9.wav'\n",
        "#noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/val/plane-noise-passengers-sound_s8OrJQ.mp3'\n",
        "#noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/val/Crowd - Mall ambience_UdLE4r.wav'\n",
        "#noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test/Crowd - Cheering - Strong cheering and soft rhythmic cheering_Pxj5eZ.wav'\n",
        "#noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test/Crowd - Street parade with music_FgF6cW.wav'\n",
        "noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test/Robocup 2019 4.1_vTYYoj.mp3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_uxrcpp04wp"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.denoising import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_audio_with_noise(audio_file, noise_file, start_time=10, duration=10,\n",
        "                             signal_level=1, noise_level=0.1, sr=44100, plot=False):\n",
        "    \"\"\"\n",
        "    Loads an audio file and a noise file, trims them, normalizes, and adds Gaussian noise.\n",
        "\n",
        "    Parameters:\n",
        "        audio_file (str): Path to the main audio file.\n",
        "        noise_file (str): Path to the noise file.\n",
        "        start_time (int): Start time (in seconds) for trimming.\n",
        "        duration (int): Total duration (in seconds).\n",
        "        signal_level (float): Scaling factor for the audio signal.\n",
        "        noise_level (float): Scaling factor for the noise.\n",
        "        sr (int): Expected sample rate (default: 44100 Hz).\n",
        "\n",
        "    Returns:\n",
        "        noisy_audio (np.array): Processed noisy audio.\n",
        "        snr (float): Signal-to-noise ratio in dB.\n",
        "    \"\"\"\n",
        "    # Load audio and noise\n",
        "    audio, audio_sr = load_audio_file(audio_file)\n",
        "    noise_waveform, noise_sr = load_audio_file(noise_file)\n",
        "\n",
        "    if len(audio) == 2:\n",
        "      audio = audio[0]\n",
        "\n",
        "    if len(noise_waveform) == 2:\n",
        "      noise_waveform = noise_waveform[0]\n",
        "\n",
        "    # Trim audio and noise to the specified start time and duration\n",
        "\n",
        "    audio = audio.cpu().numpy() if isinstance(audio, torch.Tensor) else audio\n",
        "    noise_waveform = noise_waveform.cpu().numpy() if isinstance(noise_waveform, torch.Tensor) else noise_waveform\n",
        "\n",
        "    print('Noise Sample Rate:', noise_sr)\n",
        "\n",
        "    assert audio_sr == sr, f\"Expected sample rate {sr}, but got {audio_sr}\"\n",
        "\n",
        "    # Trim audio and noise to the specified start time and duration\n",
        "    audio = audio[start_time * sr : (start_time + duration) * sr]\n",
        "    noise_waveform = noise_waveform[start_time * noise_sr : (start_time + duration) * noise_sr]\n",
        "\n",
        "    # Normalize audio to [-1, 1]\n",
        "    audio = np.clip((audio / np.max(np.abs(audio))) * signal_level, -1, 1)\n",
        "    noise_waveform = np.clip((noise_waveform / np.max(np.abs(noise_waveform))) * noise_level, -1, 1)\n",
        "\n",
        "    # Add noise to the signal\n",
        "    noisy_audio = np.clip(audio + noise_waveform, -1, 1)\n",
        "\n",
        "    # Compute SNR\n",
        "    signal_power = np.mean(audio**2)\n",
        "    noise_power = np.mean(noise_waveform**2)\n",
        "    snr = 10 * np.log10(signal_power / noise_power)\n",
        "\n",
        "    print(f\"SNR: {snr:.2f} dB\")\n",
        "\n",
        "    # Plot results\n",
        "    if plot:\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(noise_waveform, label=\"Noise\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(audio, label=\"Clean Audio\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(noisy_audio, label=\"Noisy Audio\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    return noisy_audio, sr"
      ],
      "metadata": {
        "id": "IFR0Qq2Ijwou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWoHPPrI9dcG"
      },
      "outputs": [],
      "source": [
        "file_number = 4\n",
        "noisy_audio, sr = generate_audio_with_noise(song_files[file_number], noise_file, start_time=20, duration=10, noise_level=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S5TJj7bZDnR"
      },
      "source": [
        "now the answer is to digest this waveform at 1s at a time, process those seconds, at intervals of 0.5s, window the outputs and put it back together for display on spectrograms and/or for output to .wav file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import soundfile as sf\n",
        "\n",
        "class AudioDenoiser:\n",
        "    def __init__(self, model, scalers, output_path, sample_rate=44100, chunk_duration=2, step_size=0.5, device=None):\n",
        "        \"\"\"\n",
        "        Audio Denoising Pipeline using AI model.\n",
        "\n",
        "        Parameters:\n",
        "            model (torch.nn.Module): AI model for denoising.\n",
        "            output_path (str): Directory to save output files.\n",
        "            sample_rate (int): Sample rate (default 44100 Hz).\n",
        "            chunk_duration (int): Duration of each chunk in seconds.\n",
        "            step_size (float): Step size for overlap-add in seconds.\n",
        "            device (str, optional): Device for PyTorch computation (\"cuda\" or \"cpu\").\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.output_path = output_path\n",
        "        self.sample_rate = sample_rate\n",
        "        self.chunk_samples = sample_rate * chunk_duration\n",
        "        self.scalers = scalers\n",
        "        self.step_samples = int(self.chunk_samples * step_size)\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def process_audio(self, waveform, sr):\n",
        "        \"\"\"\n",
        "        Processes audio by adding noise, chunking, denoising, and reconstructing.\n",
        "\n",
        "        Parameters:\n",
        "            input_path (str): Path to the input song file.\n",
        "            noise_path (str): Path to the noise file.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (reconstructed_audio, reconstructed_audio_input).\n",
        "        \"\"\"\n",
        "        # Load audio\n",
        "        self.waveform = waveform\n",
        "        assert sr == self.sample_rate, f\"Sample rate mismatch: expected {self.sample_rate}, got {sr}\"\n",
        "\n",
        "        # Process in chunks\n",
        "        processed_audio, processed_input = [], []\n",
        "        for start in range(0, len(waveform) - self.chunk_samples + 1, self.step_samples):\n",
        "            chunk = waveform[start:start + self.chunk_samples]\n",
        "\n",
        "            # --- start denoising\n",
        "            features = extract_features(chunk, sr, audio_length=self.chunk_samples)\n",
        "            transformed_features, metadata = transform_features(features, scalers)\n",
        "\n",
        "            # AI denoising\n",
        "            input_tensor = torch.tensor(np.array([transformed_features]), dtype=torch.float32).to(self.device)\n",
        "            denoised = self.model(input_tensor)\n",
        "\n",
        "            input = input_tensor.detach().cpu().numpy()[0]\n",
        "            denoised = denoised.detach().cpu().numpy()[0]\n",
        "\n",
        "            denoised_spectrogram = reconstruct_spectrogram(denoised, metadata)\n",
        "            input_spectrogram = reconstruct_spectrogram(input, metadata)\n",
        "\n",
        "            # remove lower than average from the spectrogram\n",
        "            denoised_spectrogram = threshold_spectrogram(denoised_spectrogram, np.mean(denoised_spectrogram))\n",
        "\n",
        "            # de-normalise\n",
        "            denoised_spectrogram = inverse_scale(denoised_spectrogram, scalers)\n",
        "            denoised_spectrogram = librosa.db_to_amplitude(denoised_spectrogram)\n",
        "            output_chunk = magphase_to_waveform(denoised_spectrogram, features['phase'], self.chunk_samples)\n",
        "\n",
        "            # denormalise input\n",
        "            input_spectrogram = inverse_scale(input_spectrogram, scalers)\n",
        "            input_spectrogram = librosa.db_to_amplitude(input_spectrogram)\n",
        "\n",
        "            # Convert back to waveform\n",
        "            output_chunk = magphase_to_waveform(denoised_spectrogram, features['phase'], self.chunk_samples)\n",
        "            input_chunk = magphase_to_waveform(input_spectrogram, features['phase'], self.chunk_samples)\n",
        "\n",
        "            processed_input.append(input_chunk)\n",
        "            processed_audio.append(output_chunk)\n",
        "\n",
        "        # Reconstruct waveform with overlap-add\n",
        "        reconstructed_audio = self._overlap_add(processed_audio)\n",
        "        reconstructed_audio_input = waveform #self._overlap_add(processed_input)\n",
        "\n",
        "        # Save output\n",
        "        self._save_audio(reconstructed_audio, \"output_audio_song.wav\")\n",
        "        self._save_audio(reconstructed_audio_input, \"input_audio_song.wav\")\n",
        "\n",
        "        # Plot spectrograms\n",
        "        self._plot_spectrograms(reconstructed_audio, reconstructed_audio_input)\n",
        "\n",
        "        return reconstructed_audio, reconstructed_audio_input\n",
        "\n",
        "    def _overlap_add(self, chunks):\n",
        "        \"\"\"Reconstructs the waveform using overlap-add method.\"\"\"\n",
        "        reconstructed = np.zeros(len(self.waveform))\n",
        "        weight = np.zeros(len(self.waveform))\n",
        "\n",
        "        for i, start in enumerate(range(0, len(self.waveform) - self.chunk_samples + 1, self.step_samples)):\n",
        "            reconstructed[start:start + self.chunk_samples] += chunks[i]\n",
        "            weight[start:start + self.chunk_samples] += np.hanning(self.chunk_samples)\n",
        "\n",
        "        reconstructed /= np.maximum(weight, 1e-6)\n",
        "        reconstructed = np.clip(reconstructed, -1, 1)\n",
        "\n",
        "        fade_in = int(self.sample_rate / 2)\n",
        "        reconstructed[:fade_in] *= np.hanning(self.sample_rate)[:fade_in]\n",
        "        reconstructed[-fade_in:] *= np.hanning(self.sample_rate)[-fade_in:]\n",
        "\n",
        "        return reconstructed\n",
        "\n",
        "    def _save_audio(self, audio, filename):\n",
        "        \"\"\"Saves the audio file.\"\"\"\n",
        "        output_filename = os.path.join(self.output_path, add_datetime_to_filename(filename))\n",
        "        sf.write(output_filename, audio / np.max(audio), self.sample_rate)\n",
        "        print(f\"Saved: {output_filename}\")\n",
        "\n",
        "    def _plot_spectrograms(self, reconstructed_audio, reconstructed_audio_input):\n",
        "        \"\"\"Plots spectrograms of processed and input audio with consistent color scale.\"\"\"\n",
        "\n",
        "        import librosa\n",
        "        import librosa.display\n",
        "        import numpy as np\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Compute Mel spectrograms\n",
        "        Sxx1 = librosa.feature.melspectrogram(y=reconstructed_audio, sr=self.sample_rate, n_fft=2048, hop_length=1024)\n",
        "        Sxx2 = librosa.feature.melspectrogram(y=reconstructed_audio_input, sr=self.sample_rate, n_fft=2048, hop_length=1024)\n",
        "\n",
        "        # Convert to log scale (dB)\n",
        "        Sxx1_db = librosa.amplitude_to_db(Sxx1, ref=np.max)\n",
        "        Sxx2_db = librosa.amplitude_to_db(Sxx2, ref=np.max)\n",
        "\n",
        "        # Compute shared color limits\n",
        "        vmin, vmax = min(Sxx1_db.min(), Sxx2_db.min()), max(Sxx1_db.max(), Sxx2_db.max())\n",
        "\n",
        "        # Create figure\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
        "\n",
        "        # Plot processed audio spectrogram\n",
        "        img1 = librosa.display.specshow(Sxx1_db, sr=self.sample_rate, hop_length=1024, cmap=\"viridis\", ax=axes[0], vmin=vmin, vmax=vmax)\n",
        "        axes[0].set_title(\"Spectrogram of Processed Audio\")\n",
        "        axes[0].set_xlabel(\"Time (s)\")\n",
        "        axes[0].set_ylabel(\"Frequency (Hz)\")\n",
        "\n",
        "        # Plot input spectrogram\n",
        "        img2 = librosa.display.specshow(Sxx2_db, sr=self.sample_rate, hop_length=1024, cmap=\"viridis\", ax=axes[1], vmin=vmin, vmax=vmax)\n",
        "        axes[1].set_title(\"Spectrogram of Input Audio\")\n",
        "        axes[1].set_xlabel(\"Time (s)\")\n",
        "\n",
        "        # Add shared colorbar\n",
        "        fig.colorbar(img1, ax=axes, orientation=\"vertical\", fraction=0.02, pad=0.02, label=\"Amplitude (dB)\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def resample_feature(feature, target_shape):\n",
        "    \"\"\"Resamples a 2D numpy feature array to match target shape using torch.nn.functional.interpolate.\"\"\"\n",
        "    feature_tensor = torch.tensor(feature, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, H, W)\n",
        "    target_size = (target_shape[0], target_shape[1])  # (new_H, new_W)\n",
        "\n",
        "    resized_feature = F.interpolate(feature_tensor, size=target_size, mode=\"bilinear\", align_corners=False)\n",
        "    return resized_feature.squeeze(0).squeeze(0).numpy()  # Remove batch/channel dim and return as numpy\n",
        "\n",
        "def transform_features(features, scalers):\n",
        "    input_spectrogram = features['spectrogram']\n",
        "    input_edges = features['edges']\n",
        "    input_cepstrum = features['cepstrum']\n",
        "\n",
        "    # function to transform the extracted features to an input to the\n",
        "    target_shape = input_spectrogram.shape\n",
        "    # Apply scalers\n",
        "        #input_phase = self.scalers[\"input_features_phase\"].transform(input_phase.reshape(1, -1)).reshape(input_phase.shape)\n",
        "    input_spectrogram = scalers[\"input_features_spectrogram\"].transform(input_spectrogram.reshape(1, -1)).reshape(input_spectrogram.shape)\n",
        "    input_edges = scalers[\"input_features_edges\"].transform(input_edges.reshape(1, -1)).reshape(input_edges.shape)\n",
        "    input_cepstrum = scalers[\"input_features_cepstrum\"].transform(input_cepstrum.reshape(1, -1)).reshape(input_cepstrum.shape)\n",
        "    #input_cepstrum_edges = self.scalers[\"input_features_cepstrum_edges\"].transform(input_cepstrum_edges.reshape(1, -1)).reshape(input_cepstrum_edges.shape)\n",
        "\n",
        "    # resample mfcc featues so theyre the same shape as the spectrogram and phase features\n",
        "    # Define frequency bins\n",
        "    sampling_rate = 44100  # 44.1 kHz audio\n",
        "    n_fft = 2048  # Adjust this for better resolution\n",
        "    freqs = np.linspace(0, sampling_rate / 2, n_fft // 2 + 1)  # STFT frequency bins\n",
        "\n",
        "    # Find indices corresponding to 0–4000 Hz\n",
        "    min_freq, hf, mf, lf = 0, 4000, 1000, 200\n",
        "    freq_indices_hf = np.where((freqs >= min_freq) & (freqs <= hf))[0]\n",
        "    freq_indices_mf = np.where((freqs >= min_freq) & (freqs <= mf))[0]\n",
        "    freq_indices_lf = np.where((freqs >= min_freq) & (freqs <= lf))[0]\n",
        "    # input spectrogram\n",
        "    input_spectrogram_hf = resample_feature(input_spectrogram[freq_indices_hf, :], target_shape)\n",
        "    input_spectrogram_mf = resample_feature(input_spectrogram[freq_indices_mf, :], target_shape)\n",
        "    input_spectrogram_lf = resample_feature(input_spectrogram[freq_indices_lf, :], target_shape)\n",
        "    # edges\n",
        "    input_edges_hf = resample_feature(input_edges[freq_indices_hf, :], target_shape)\n",
        "    input_edges_mf = resample_feature(input_edges[freq_indices_mf, :], target_shape)\n",
        "    input_edges_lf = resample_feature(input_edges[freq_indices_lf, :], target_shape)\n",
        "\n",
        "    # now input indices for 0-1000 and 0-200 to add as channels and as freq_indicies for reconstruction\n",
        "\n",
        "    # Resample MFCC features\n",
        "    input_cepstrum = resample_feature(input_cepstrum, target_shape)\n",
        "\n",
        "    # Convert to tensors - input_phase, is missing,..... it's too confusing\n",
        "    inputs = torch.tensor(np.stack([\n",
        "        input_spectrogram, input_spectrogram_hf, input_spectrogram_mf, input_spectrogram_lf,\n",
        "        input_edges, input_edges_hf, input_edges_mf, input_edges_lf,\n",
        "        input_cepstrum\n",
        "    ], axis=0), dtype=torch.float32)  # Shape: (6, H, W)\n",
        "\n",
        "    a = 3\n",
        "    inputs = (inputs/a) + 0.5\n",
        "\n",
        "    # metadata\n",
        "    # Extract metadata\n",
        "    metadata = {\n",
        "        \"hf_shape\": input_spectrogram[freq_indices_hf, :].shape,\n",
        "        \"mf_shape\": input_spectrogram[freq_indices_mf, :].shape,\n",
        "        \"lf_shape\": input_spectrogram[freq_indices_lf, :].shape,\n",
        "        \"freq_indices_hf\": freq_indices_hf,\n",
        "        \"freq_indices_mf\": freq_indices_mf,\n",
        "        \"freq_indices_lf\": freq_indices_lf\n",
        "    }\n",
        "\n",
        "    return inputs, metadata\n",
        "\n",
        "def reconstruct_spectrogram(outputs, metadata):\n",
        "    # lets evaluate this from a l1 loss perspective\n",
        "    # reconstruct spectrogram\n",
        "    out_spectrogram = np.array(outputs[0])\n",
        "    out_spectrogram[metadata[\"freq_indices_hf\"], :] = resample_feature(outputs[1], metadata[\"hf_shape\"])\n",
        "    out_spectrogram[metadata[\"freq_indices_mf\"], :] = resample_feature(outputs[2], metadata[\"mf_shape\"])\n",
        "    out_spectrogram[metadata[\"freq_indices_lf\"], :] = resample_feature(outputs[3], metadata[\"lf_shape\"])\n",
        "    return out_spectrogram\n",
        "\n",
        "def inverse_scale(out_spectrogram, scalers):\n",
        "    # inverse scale the\n",
        "    # transform back to 0 centred and\n",
        "    out_spectrogram = (out_spectrogram - 0.5) * 3\n",
        "    out_spec_shape = out_spectrogram.shape\n",
        "\n",
        "    # undo scaler\n",
        "    out_spectrogram = scalers[\"input_features_spectrogram\"].inverse_transform(np.array([out_spectrogram]).reshape(1, -1)).reshape(out_spec_shape)\n",
        "    return out_spectrogram\n",
        "\n",
        "def threshold_spectrogram(spectrogram, threshold, percentage=0.8):\n",
        "    \"\"\"\n",
        "    Zeroes out all values in the spectrogram that are below the given threshold.\n",
        "\n",
        "    Args:\n",
        "        spectrogram (np.ndarray): Input 2D array.\n",
        "        threshold (float): The threshold value.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The processed spectrogram with values below threshold set to zero.\n",
        "    \"\"\"\n",
        "    spectrogram = np.where(spectrogram >= threshold * percentage, spectrogram, 0)\n",
        "    return spectrogram\n",
        "\n",
        "def magphase_to_waveform(magnitude, phase, audio_length=44100):\n",
        "    \"\"\"\n",
        "    Converts a spectrogram image back into an audio waveform.\n",
        "\n",
        "    Parameters:\n",
        "        image (np.array): Spectrogram image (3 channels).\n",
        "        sr (int): Sampling rate.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Reconstructed audio waveform.\n",
        "    \"\"\"\n",
        "    stft = magnitude * np.exp(1j * phase)\n",
        "    return librosa.istft(stft, length=audio_length)"
      ],
      "metadata": {
        "id": "Z2nkgmDifvri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8UgbuKiWYXb"
      },
      "outputs": [],
      "source": [
        "denoiser = AudioDenoiser(model, scalers, output_path=output_path, chunk_duration=2, step_size=0.5)\n",
        "reconstructed_audio, reconstructed_input = denoiser.process_audio(noisy_audio, sr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "from google.colab import files\n",
        "\n",
        "def save_wav_sf(file_path, audio_array, sample_rate):\n",
        "    \"\"\"Saves a NumPy array as a WAV file using soundfile.\"\"\"\n",
        "    sf.write(file_path, audio_array, sample_rate, subtype=\"PCM_16\")  # Can be PCM_24, PCM_32, FLOAT\n",
        "    files.download(file_path)  # Trigger download in Colab\n",
        "\n",
        "save_wav_sf(f\"output_{file_number}.wav\", reconstructed_audio, sr)\n",
        "save_wav_sf(f\"input_{file_number}.wav\", reconstructed_input, sr)"
      ],
      "metadata": {
        "id": "_WGNAvPmi0qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN4K8F4rrwMb"
      },
      "outputs": [],
      "source": [
        "!pip install mir_eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aorJVEypw2Ry"
      },
      "source": [
        "Use the SDR metric to compare signal to noise ratios of the generated output, and the standard output and demonstrate an increase in signal to noise ratio overall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJjf5BGKdTEe"
      },
      "outputs": [],
      "source": [
        "average_rec = np.log(np.average(Pxx_rec, axis=1))\n",
        "average_spec = np.log(np.average(Pxx_spec, axis=1))\n",
        "\n",
        "plt.plot(average_rec)\n",
        "plt.plot(average_spec)\n",
        "plt.xscale('log')\n",
        "plt.show()\n",
        "\n",
        "import numpy as np\n",
        "import mir_eval\n",
        "\n",
        "def compute_sdr(reference, estimated):\n",
        "    \"\"\"\n",
        "    Compute the Signal-to-Distortion Ratio (SDR) between reference and estimated signals.\n",
        "\n",
        "    :param reference: np.ndarray of shape (channels, samples), ground-truth clean signal\n",
        "    :param estimated: np.ndarray of shape (channels, samples), predicted separated signal\n",
        "    :return: float, SDR value in dB\n",
        "    \"\"\"\n",
        "    # Ensure inputs are 2D (stereo/multichannel) or 1D (mono)\n",
        "    reference = np.atleast_2d(reference)\n",
        "    estimated = np.atleast_2d(estimated)\n",
        "\n",
        "    # Compute SDR using mir_eval\n",
        "    sdr, _, _, _ = mir_eval.separation.bss_eval_sources(reference, estimated, compute_permutation=False)\n",
        "\n",
        "    return np.mean(sdr)  # Return average SDR across channels\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Fake reference and estimated signals (replace with actual signals)\n",
        "    ref_signal = reconstructed_audio_input  # 2 channels, 1 second at 44.1kHz\n",
        "    est_signal = reconstructed_audio  # Slightly noisy estimate\n",
        "\n",
        "    # max sdr\n",
        "    sdr_max = compute_sdr(audio, audio)\n",
        "    print(f\"SDR - : {sdr_max:.2f} dB -- Max\")\n",
        "\n",
        "    # reference sdr\n",
        "    sdr_ref = compute_sdr(audio, noisy_audio)\n",
        "    print(f\"SDR - : {sdr_ref:.2f} dB -- Reference\")\n",
        "\n",
        "    # computed srd\n",
        "    sdr_value = compute_sdr(audio, est_signal)\n",
        "    print(f\"SDR - : {sdr_value:.2f} dB -- Denoising\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8pgR2SVFiqdV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}