{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8SELAW0r3K8"
      },
      "source": [
        "### Idea for denoising archetecture\n",
        "0. Add the other noise sources to a new dataset\n",
        "1. Come up with an example (so make the resampling and reconstruction a function)\n",
        "2.\n",
        "\n",
        "\n",
        "\n",
        "## 1. Using a Laplacian Filter (Edge Detection)\n",
        "```python\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Apply the Laplacian filter\n",
        "filtered_spectrogram = cv2.Laplacian(spectrogram, cv2.CV_64F, ksize=3)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And either use MFCC, or Cepstrum"
      ],
      "metadata": {
        "id": "pk0VVeyChttz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1B6SS0YdX-xs",
        "outputId": "bebb40fb-6ba1-4893-db76-e943fd1751fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE8QDLOXuiE7"
      },
      "source": [
        "Environmental Sound Classification 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ2cUzDwQQmi",
        "outputId": "16cae447-8d42-428c-a728-208d36d4983d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab...\n"
          ]
        }
      ],
      "source": [
        "# Detect Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    print(\"Running in Google Colab...\")\n",
        "    os.system(\"git clone https://github.com/CiaranMaloy/audioautoencoder\")\n",
        "    os.chdir(\"/content/audioautoencoder/\")\n",
        "    os.system(\"git pull\")\n",
        "    os.system(\"git checkout bandchannels\")\n",
        "    os.system(\"git pull origin bandchannels\")\n",
        "    #os.system(\"pip install --upgrade torchmetrics\")\n",
        "else:\n",
        "    print(\"Running locally...\")\n",
        "    os.system(\"git pull origin bandchannels\")\n",
        "    os.system(\"pip install --upgrade torchmetrics\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "av62pZLzSUjo"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/audioautoencoder')\n",
        "sys.path.append('/content/audioautoencoder/audioautoencoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJxpYCvxUBZZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N23OLmvVQr6V"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.data import *\n",
        "from audioautoencoder.outputs import *\n",
        "from audioautoencoder.processing import *\n",
        "from audioautoencoder.training import *\n",
        "from audioautoencoder.datasets.loaders import *\n",
        "from audioautoencoder.generate_dataset import *\n",
        "from audioautoencoder.plotting import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBX7YjnSJpLC"
      },
      "source": [
        "test get item input and output to see if i can use the log magnitude and then reverse it on the output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Niz7eKR1xy18"
      },
      "source": [
        "## Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4jLmXnwg8YL-"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.data_management import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qIB1M7xDLIFR"
      },
      "outputs": [],
      "source": [
        "GENERATE=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vHikpu86MO3s"
      },
      "outputs": [],
      "source": [
        "# Example Usage\n",
        "if GENERATE:\n",
        "  dataset_dirs = [\"/content/drive/MyDrive/Datasets/Noise/All_Noise\"]\n",
        "  output_dir = \"/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2\"\n",
        "  splits = create_datasets(dataset_dirs, output_dir)\n",
        "  print(\"Training Set:\", len(splits[\"train\"]))\n",
        "  print(\"Validation Set:\", len(splits[\"val\"]))\n",
        "  print(\"Testing Set:\", len(splits[\"test\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_C4AGBiBPsQ8"
      },
      "outputs": [],
      "source": [
        "if GENERATE:\n",
        "  save_splits_to_directories(splits, output_dir, max_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KT9U_mkQtxzx"
      },
      "outputs": [],
      "source": [
        "# generate audio files for noise and music (2s)\n",
        "if GENERATE:\n",
        "  noise_test = output_dir + \"/test\"\n",
        "  noise_train = output_dir + \"/train\"\n",
        "\n",
        "  noise_test_output = noise_test + \"-2s-44100\"\n",
        "  noise_train_output = noise_train + \"-2s-44100\"\n",
        "\n",
        "  for input_path, output_path in [(noise_test, noise_test_output), (noise_train, noise_train_output)]:\n",
        "    print(input_path, output_path)\n",
        "    generate_audio_files(input_path, output_path, t=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wf3uGYyGwS0S"
      },
      "outputs": [],
      "source": [
        "# generate audio files for noise and music (2s)\n",
        "if False:\n",
        "  music_test = \"/content/drive/MyDrive/Datasets/Music/MUSDB18/test\"\n",
        "  music_train = \"/content/drive/MyDrive/Datasets/Music/MUSDB18/train\"\n",
        "\n",
        "  music_test_output = music_test + \"-2s-44100\"\n",
        "  music_train_output = music_train + \"-2s-44100\"\n",
        "\n",
        "  for input_path, output_path in [(music_test, music_test_output), (music_train, music_train_output)]:\n",
        "    print(input_path, output_path)\n",
        "    generate_audio_files(input_path, output_path, t=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxxKjl_JOAKE"
      },
      "source": [
        "## Process files to H5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "X82E9Uf6WdX_"
      },
      "outputs": [],
      "source": [
        "GENERATE_H5_FILES = False\n",
        "\n",
        "if GENERATE_H5_FILES:\n",
        "  checkpoint_file_size=50000\n",
        "  processor = DatasetProcessor(\n",
        "          train_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/train-2s-44100',\n",
        "          train_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/train-2s-44100',\n",
        "          test_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/test-2s-44100',\n",
        "          test_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test-2s-44100',\n",
        "          output_dir='/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_sep_features',\n",
        "          SNRdB=[-10, 10],\n",
        "          process_train=True,\n",
        "          process_test=True,\n",
        "          checkpoint_file_size=checkpoint_file_size\n",
        "      )\n",
        "  processor.process()\n",
        "\n",
        "  processor = DatasetProcessor(\n",
        "          train_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/train-2s-44100',\n",
        "          train_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/train-2s-44100',\n",
        "          test_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/test-2s-44100',\n",
        "          test_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test-2s-44100',\n",
        "          output_dir='/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_sep_features',\n",
        "          SNRdB=[0, 20],\n",
        "          process_train=True,\n",
        "          process_test=True,\n",
        "          checkpoint_file_size=checkpoint_file_size\n",
        "      )\n",
        "  processor.process()\n",
        "\n",
        "  processor = DatasetProcessor(\n",
        "          train_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/train-2s-44100',\n",
        "          train_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/train-2s-44100',\n",
        "          test_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/test-2s-44100',\n",
        "          test_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test-2s-44100',\n",
        "          output_dir='/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_sep_features',\n",
        "          SNRdB=[10, 30],\n",
        "          process_train=True,\n",
        "          process_test=True,\n",
        "          checkpoint_file_size=checkpoint_file_size\n",
        "      )\n",
        "  processor.process()\n",
        "\n",
        "  processor = DatasetProcessor(\n",
        "          train_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/train-2s-44100',\n",
        "          train_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/train-2s-44100',\n",
        "          test_music_dir='/content/drive/MyDrive/Datasets/Music/MUSDB18/test-2s-44100',\n",
        "          test_noise_dir='/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test-2s-44100',\n",
        "          output_dir='/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_mix_features',\n",
        "          SNRdB=[-10, 30],\n",
        "          process_train=True,\n",
        "          process_test=True,\n",
        "          mix_only=True,\n",
        "          checkpoint_file_size=checkpoint_file_size\n",
        "      )\n",
        "  processor.process()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2m4ztn1qxC3"
      },
      "source": [
        "## Define Autoencoder structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep3iZRJC0tJi"
      },
      "source": [
        "## Improvement to UnetAutoencoder\n",
        "\n",
        "1. changed padding from explicit to on the output of the convolutional layer\n",
        "2. added batch normalisation between layers\n",
        "3. changed fro elu to leaky_relu for efficiency\n",
        "4. in the other one there is no pooling... and the skip connections are done by concatenation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOXAuPserTGv"
      },
      "source": [
        "#### For v3, remove pooling layers, and add attention onto skip connections\n",
        "\n",
        "In addition, try only music and only crowd as a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "zyXtMpe1PvID"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "\n",
        "## Masking Unet\n",
        "# enhanced attention on skip connections\n",
        "# deep 3 headded self attention in bottleneck\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channels:int, num_groups:int, dropout_prob:float):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.gnorm1 = nn.GroupNorm(num_groups=num_groups, num_channels=channels)\n",
        "        self.gnorm2 = nn.GroupNorm(num_groups=num_groups, num_channels=channels)\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.dropout = nn.Dropout(p=dropout_prob, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r = self.conv1(self.relu(self.gnorm1(x)))\n",
        "        r = self.dropout(r)\n",
        "        r = self.conv2(self.relu(self.gnorm2(x)))\n",
        "        return r + x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, channels: int, num_heads:int , dropout_prob: float):\n",
        "        super().__init__()\n",
        "        self.proj1 = nn.Linear(channels, channels*3)\n",
        "        self.proj2 = nn.Linear(channels, channels)\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, w = x.shape[2:]\n",
        "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "        x = self.proj1(x)\n",
        "        x = rearrange(x, 'b L (C H K) -> K b H L C', K=3, H=self.num_heads)\n",
        "        q,k,v = x[0], x[1], x[2]\n",
        "        x = F.scaled_dot_product_attention(q,k,v, is_causal=False, dropout_p=self.dropout_prob)\n",
        "        x = rearrange(x, 'b H (h w) C -> b h w (C H)', h=h, w=w)\n",
        "        x = self.proj2(x)\n",
        "        return rearrange(x, 'b h w C -> b C h w')\n",
        "\n",
        "\n",
        "class ResAttentionLayer(nn.Module):\n",
        "    def __init__(self, channels, attention=True, num_groups=32, dropout_prob=0.1, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.ResBlock1 = ResBlock(channels=channels, num_groups=num_groups, dropout_prob=dropout_prob)\n",
        "        self.ResBlock2 = ResBlock(channels=channels, num_groups=num_groups, dropout_prob=dropout_prob)\n",
        "\n",
        "        if attention:\n",
        "            self.attention_layer = Attention(channels, num_heads=num_heads, dropout_prob=dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ResBlock1(x)\n",
        "        if hasattr(self, 'attention_layer'):\n",
        "            x = self.attention_layer(x)\n",
        "        x = self.ResBlock2(x)\n",
        "        return x\n",
        "\n",
        "class EnhancedSkipAttention(nn.Module):\n",
        "    def __init__(self, encoder_channels, decoder_channels, reduction_ratio=4):\n",
        "        super().__init__()\n",
        "        self.channels = encoder_channels\n",
        "\n",
        "        # Channel attention for encoder features\n",
        "        self.encoder_channel_attn = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(encoder_channels, encoder_channels // reduction_ratio, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(encoder_channels // reduction_ratio, encoder_channels, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Projection for decoder features to match encoder dimensions if needed\n",
        "        self.decoder_proj = None\n",
        "        if encoder_channels != decoder_channels:\n",
        "            self.decoder_proj = nn.Conv2d(decoder_channels, encoder_channels, kernel_size=1)\n",
        "\n",
        "        # Cross-attention between encoder and decoder features\n",
        "        self.cross_attn = nn.Sequential(\n",
        "            nn.Conv2d(encoder_channels*2, 2, kernel_size=3, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, encoder_features, decoder_features):\n",
        "        # Process decoder features if dimensions don't match\n",
        "        if self.decoder_proj is not None:\n",
        "            decoder_features = self.decoder_proj(decoder_features)\n",
        "\n",
        "        # Apply channel attention to encoder features\n",
        "        channel_attn = self.encoder_channel_attn(encoder_features)\n",
        "        encoder_features = encoder_features * channel_attn\n",
        "\n",
        "        # Concatenate encoder and decoder features\n",
        "        combined = torch.cat([encoder_features, decoder_features], dim=1)\n",
        "\n",
        "        # Generate attention weights for each feature set\n",
        "        attn_weights = self.cross_attn(combined)\n",
        "        encoder_weight, decoder_weight = torch.split(attn_weights, 1, dim=1)\n",
        "\n",
        "        # Apply weights and combine features\n",
        "        result = encoder_features * encoder_weight + decoder_features * decoder_weight\n",
        "\n",
        "        return result\n",
        "\n",
        "class UNetConv10(nn.Module):\n",
        "    # Update from UnetConv6, moving to a masking model, which hopefully works better\n",
        "    def __init__(self, in_channels=9, out_channels=4):\n",
        "        super().__init__()\n",
        "\n",
        "        a = 2\n",
        "        A, B, C, D = 64, 128, 256, 512\n",
        "        bottleneck_channels = 1024\n",
        "\n",
        "        # function\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Encoder (Downsampling)\n",
        "        enc_channels = [A, B, C, D]\n",
        "        dec_channels = [D, C, B, A]\n",
        "\n",
        "        self.enc1 = self.conv_block(in_channels, enc_channels[0], (10, 3), 2)\n",
        "        self.enc2 = self.conv_block(enc_channels[0], enc_channels[1], 5, 2)\n",
        "        self.enc3 = self.conv_block(enc_channels[1], enc_channels[2], 3, 2)\n",
        "        self.enc4 = self.conv_block(enc_channels[2], enc_channels[3], 3, 2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck_in = self.conv_block(enc_channels[3], bottleneck_channels, 3, 2)\n",
        "        self.resattention = ResAttentionLayer(bottleneck_channels, attention=True)\n",
        "        self.bottleneck_out = self.upconv_block(bottleneck_channels, dec_channels[0], 3, 2)\n",
        "\n",
        "        # Decoder (Upsampling)\n",
        "        self.dec4 = self.upconv_block(dec_channels[0], dec_channels[1], 3, 2)\n",
        "        self.dec3 = self.upconv_block(dec_channels[1], dec_channels[2], 3, 2)\n",
        "        self.dec2 = self.upconv_block(dec_channels[2], dec_channels[3], 5, 2)\n",
        "        self.dec1 = self.upconv_block(dec_channels[3], out_channels, (10, 3), 2)\n",
        "\n",
        "        # Initialize Spatial Attention Modules\n",
        "        self.attn4 = EnhancedSkipAttention(enc_channels[3], dec_channels[0])\n",
        "        self.attn3 = EnhancedSkipAttention(enc_channels[2], dec_channels[1])\n",
        "        self.attn2 = EnhancedSkipAttention(enc_channels[1], dec_channels[2])\n",
        "        self.attn1 = EnhancedSkipAttention(enc_channels[0], dec_channels[3])\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels, kernel_size, stride, dropout=0.2):\n",
        "        \"\"\"Convolutional Block with Dropout in Deeper Layers Only\"\"\"\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1, stride=stride), # In the next iteration i should introduce a stride here\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        ]\n",
        "\n",
        "        # Dropout only for deeper encoder layers\n",
        "        if out_channels >= 256:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def upconv_block(self, in_channels, out_channels, kernel_size, stride, dropout=0.2):\n",
        "        \"\"\"Upsampling Block with Dropout in First Few Decoder Layers\"\"\"\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        ]\n",
        "\n",
        "        # Dropout only for first few decoder layers\n",
        "        if in_channels >= 256:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass with skip connections\"\"\"\n",
        "        # Encoding\n",
        "        e1 = self.enc1(x)  # (batch, 64, 1028, 175)\n",
        "        e2 = self.enc2(e1) # (batch, 128, 514, 87)\n",
        "        e3 = self.enc3(e2)  # (batch, 256, 257, 43)\n",
        "        e4 = self.enc4(e3) # (batch, 512, 128, 21)\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.bottleneck_in(e4)  # (batch, 1024, 64, 10)\n",
        "        b = self.resattention(b)  # (batch, 1024, 64, 10)\n",
        "        b = self.bottleneck_out(b)  # (batch, 512, 64, 10)\n",
        "\n",
        "        # Decoding + Skip Connections with Spatial Attention\n",
        "        b = F.interpolate(b, size=e4.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "        d4_attn = self.attn4(e4, b)  # Apply Spatial Attention\n",
        "        d4 = self.dec4(d4_attn)  # (batch, 512, ?, ?)\n",
        "\n",
        "        d4 = F.interpolate(d4, size=e3.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "        d3_attn = self.attn3(e3, d4)  # Apply Spatial Attention\n",
        "        d3 = self.dec3(d3_attn)  # (batch, 256, ?, ?)\n",
        "\n",
        "        d3 = F.interpolate(d3, size=e2.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "        d2_attn = self.attn2(e2, d3)  # Apply Spatial Attention\n",
        "        d2 = self.dec2(d2_attn)  # (batch, 128, ?, ?)\n",
        "\n",
        "        d2 = F.interpolate(d2, size=e1.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "        d1_attn = self.attn1(e1, d2)  # Apply Spatial Attention\n",
        "        d1 = self.dec1(d1_attn)  # (batch, 64, ?, ?)\n",
        "\n",
        "        # Final Convolution (output denoised spectrogram)\n",
        "        mask = F.interpolate(d1, size=(1025, 175), mode=\"bilinear\", align_corners=False)\n",
        "        return x[:, :4] * self.sigmoid(mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4\n",
        "TEST_MODEL = True"
      ],
      "metadata": {
        "id": "dwpBexNA9X0Q"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "xgzwlHooHoiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40eed408-a5e5-4550-a087-2ff69a82a9c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output....\n",
            "torch.Size([4, 4, 1025, 175])\n"
          ]
        }
      ],
      "source": [
        "if TEST_MODEL:\n",
        "  if __name__ == \"__main__\":\n",
        "      x = torch.randn((BATCH_SIZE, 9, 1025, 175))\n",
        "      model = UNetConv10()\n",
        "      output = model(x)\n",
        "\n",
        "      print('output....')\n",
        "      print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "zB2SGHy6BEby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b69ab327-af15-4b23-b82f-f45c6ccbf960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running autoencoder tests...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "...\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 2.851s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 64, 509, 88]          17,344\n",
            "       BatchNorm2d-2          [-1, 64, 509, 88]             128\n",
            "         LeakyReLU-3          [-1, 64, 509, 88]               0\n",
            "            Conv2d-4          [-1, 64, 502, 88]         122,944\n",
            "       BatchNorm2d-5          [-1, 64, 502, 88]             128\n",
            "         LeakyReLU-6          [-1, 64, 502, 88]               0\n",
            "            Conv2d-7         [-1, 128, 250, 43]         204,928\n",
            "       BatchNorm2d-8         [-1, 128, 250, 43]             256\n",
            "         LeakyReLU-9         [-1, 128, 250, 43]               0\n",
            "           Conv2d-10         [-1, 128, 248, 41]         409,728\n",
            "      BatchNorm2d-11         [-1, 128, 248, 41]             256\n",
            "        LeakyReLU-12         [-1, 128, 248, 41]               0\n",
            "           Conv2d-13         [-1, 256, 124, 21]         295,168\n",
            "      BatchNorm2d-14         [-1, 256, 124, 21]             512\n",
            "        LeakyReLU-15         [-1, 256, 124, 21]               0\n",
            "           Conv2d-16         [-1, 256, 124, 21]         590,080\n",
            "      BatchNorm2d-17         [-1, 256, 124, 21]             512\n",
            "        LeakyReLU-18         [-1, 256, 124, 21]               0\n",
            "          Dropout-19         [-1, 256, 124, 21]               0\n",
            "           Conv2d-20          [-1, 512, 62, 11]       1,180,160\n",
            "      BatchNorm2d-21          [-1, 512, 62, 11]           1,024\n",
            "        LeakyReLU-22          [-1, 512, 62, 11]               0\n",
            "           Conv2d-23          [-1, 512, 62, 11]       2,359,808\n",
            "      BatchNorm2d-24          [-1, 512, 62, 11]           1,024\n",
            "        LeakyReLU-25          [-1, 512, 62, 11]               0\n",
            "          Dropout-26          [-1, 512, 62, 11]               0\n",
            "           Conv2d-27          [-1, 1024, 31, 6]       4,719,616\n",
            "      BatchNorm2d-28          [-1, 1024, 31, 6]           2,048\n",
            "        LeakyReLU-29          [-1, 1024, 31, 6]               0\n",
            "           Conv2d-30          [-1, 1024, 31, 6]       9,438,208\n",
            "      BatchNorm2d-31          [-1, 1024, 31, 6]           2,048\n",
            "        LeakyReLU-32          [-1, 1024, 31, 6]               0\n",
            "          Dropout-33          [-1, 1024, 31, 6]               0\n",
            "        GroupNorm-34          [-1, 1024, 31, 6]           2,048\n",
            "             ReLU-35          [-1, 1024, 31, 6]               0\n",
            "           Conv2d-36          [-1, 1024, 31, 6]       9,438,208\n",
            "          Dropout-37          [-1, 1024, 31, 6]               0\n",
            "        GroupNorm-38          [-1, 1024, 31, 6]           2,048\n",
            "             ReLU-39          [-1, 1024, 31, 6]               0\n",
            "           Conv2d-40          [-1, 1024, 31, 6]       9,438,208\n",
            "         ResBlock-41          [-1, 1024, 31, 6]               0\n",
            "           Linear-42            [-1, 186, 3072]       3,148,800\n",
            "           Linear-43          [-1, 31, 6, 1024]       1,049,600\n",
            "        Attention-44          [-1, 1024, 31, 6]               0\n",
            "        GroupNorm-45          [-1, 1024, 31, 6]           2,048\n",
            "             ReLU-46          [-1, 1024, 31, 6]               0\n",
            "           Conv2d-47          [-1, 1024, 31, 6]       9,438,208\n",
            "          Dropout-48          [-1, 1024, 31, 6]               0\n",
            "        GroupNorm-49          [-1, 1024, 31, 6]           2,048\n",
            "             ReLU-50          [-1, 1024, 31, 6]               0\n",
            "           Conv2d-51          [-1, 1024, 31, 6]       9,438,208\n",
            "         ResBlock-52          [-1, 1024, 31, 6]               0\n",
            "ResAttentionLayer-53          [-1, 1024, 31, 6]               0\n",
            "  ConvTranspose2d-54          [-1, 512, 63, 13]       4,719,104\n",
            "      BatchNorm2d-55          [-1, 512, 63, 13]           1,024\n",
            "        LeakyReLU-56          [-1, 512, 63, 13]               0\n",
            "          Dropout-57          [-1, 512, 63, 13]               0\n",
            "AdaptiveAvgPool2d-58            [-1, 512, 1, 1]               0\n",
            "           Conv2d-59            [-1, 128, 1, 1]          65,664\n",
            "             ReLU-60            [-1, 128, 1, 1]               0\n",
            "           Conv2d-61            [-1, 512, 1, 1]          66,048\n",
            "          Sigmoid-62            [-1, 512, 1, 1]               0\n",
            "           Conv2d-63            [-1, 2, 62, 11]          18,434\n",
            "          Sigmoid-64            [-1, 2, 62, 11]               0\n",
            "EnhancedSkipAttention-65          [-1, 512, 62, 11]               0\n",
            "  ConvTranspose2d-66         [-1, 256, 125, 23]       1,179,904\n",
            "      BatchNorm2d-67         [-1, 256, 125, 23]             512\n",
            "        LeakyReLU-68         [-1, 256, 125, 23]               0\n",
            "          Dropout-69         [-1, 256, 125, 23]               0\n",
            "AdaptiveAvgPool2d-70            [-1, 256, 1, 1]               0\n",
            "           Conv2d-71             [-1, 64, 1, 1]          16,448\n",
            "             ReLU-72             [-1, 64, 1, 1]               0\n",
            "           Conv2d-73            [-1, 256, 1, 1]          16,640\n",
            "          Sigmoid-74            [-1, 256, 1, 1]               0\n",
            "           Conv2d-75           [-1, 2, 124, 21]           9,218\n",
            "          Sigmoid-76           [-1, 2, 124, 21]               0\n",
            "EnhancedSkipAttention-77         [-1, 256, 124, 21]               0\n",
            "  ConvTranspose2d-78         [-1, 128, 249, 43]         295,040\n",
            "      BatchNorm2d-79         [-1, 128, 249, 43]             256\n",
            "        LeakyReLU-80         [-1, 128, 249, 43]               0\n",
            "          Dropout-81         [-1, 128, 249, 43]               0\n",
            "AdaptiveAvgPool2d-82            [-1, 128, 1, 1]               0\n",
            "           Conv2d-83             [-1, 32, 1, 1]           4,128\n",
            "             ReLU-84             [-1, 32, 1, 1]               0\n",
            "           Conv2d-85            [-1, 128, 1, 1]           4,224\n",
            "          Sigmoid-86            [-1, 128, 1, 1]               0\n",
            "           Conv2d-87           [-1, 2, 248, 41]           4,610\n",
            "          Sigmoid-88           [-1, 2, 248, 41]               0\n",
            "EnhancedSkipAttention-89         [-1, 128, 248, 41]               0\n",
            "  ConvTranspose2d-90          [-1, 64, 499, 85]         204,864\n",
            "      BatchNorm2d-91          [-1, 64, 499, 85]             128\n",
            "        LeakyReLU-92          [-1, 64, 499, 85]               0\n",
            "AdaptiveAvgPool2d-93             [-1, 64, 1, 1]               0\n",
            "           Conv2d-94             [-1, 16, 1, 1]           1,040\n",
            "             ReLU-95             [-1, 16, 1, 1]               0\n",
            "           Conv2d-96             [-1, 64, 1, 1]           1,088\n",
            "          Sigmoid-97             [-1, 64, 1, 1]               0\n",
            "           Conv2d-98           [-1, 2, 502, 88]           2,306\n",
            "          Sigmoid-99           [-1, 2, 502, 88]               0\n",
            "EnhancedSkipAttention-100          [-1, 64, 502, 88]               0\n",
            " ConvTranspose2d-101         [-1, 4, 1012, 177]           7,684\n",
            "     BatchNorm2d-102         [-1, 4, 1012, 177]               8\n",
            "       LeakyReLU-103         [-1, 4, 1012, 177]               0\n",
            "         Sigmoid-104         [-1, 4, 1025, 175]               0\n",
            "================================================================\n",
            "Total params: 67,923,716\n",
            "Trainable params: 67,923,716\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 6.16\n",
            "Forward/backward pass size (MB): 490.12\n",
            "Params size (MB): 259.11\n",
            "Estimated Total Size (MB): 755.39\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import unittest\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "\n",
        "class TestAutoencoder(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.model = UNetConv10()\n",
        "        self.input_channels = 9\n",
        "        self.output_channels = 4\n",
        "        self.input_height = 1025\n",
        "        self.input_width = 175\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def test_model_initialization(self):\n",
        "        self.assertIsInstance(self.model, UNetConv10, \"Model initialization failed\")\n",
        "\n",
        "    def test_forward_pass(self):\n",
        "        x = torch.randn(self.batch_size, self.input_channels, self.input_height, self.input_width, device=self.device)\n",
        "        output = self.model(x)\n",
        "        self.assertEqual(\n",
        "            output.shape,\n",
        "            (self.batch_size, self.output_channels, self.input_height, self.input_width),\n",
        "            f\"Expected output shape {(self.batch_size, self.output_channels, self.input_height, self.input_width)}, but got {output.shape}\"\n",
        "        )\n",
        "\n",
        "    def test_model_summary(self):\n",
        "        try:\n",
        "            summary(self.model, input_size=(self.input_channels, self.input_height, self.input_width))\n",
        "        except Exception as e:\n",
        "            self.fail(f\"Model summary failed: {str(e)}\")\n",
        "\n",
        "# This allows running tests externally\n",
        "def suite():\n",
        "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestAutoencoder)\n",
        "    return test_suite\n",
        "\n",
        "# runner\n",
        "class TestRunner:\n",
        "    def __init__(self):\n",
        "        self.runner = unittest.TextTestRunner()\n",
        "\n",
        "    def run(self):\n",
        "        print(\"Running autoencoder tests...\")\n",
        "        self.runner.run(suite())\n",
        "\n",
        "if TEST_MODEL:\n",
        "  if __name__ == \"__main__\":\n",
        "      runner = TestRunner()\n",
        "      runner.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihXSxknjq_Ks"
      },
      "source": [
        "## First train as an autoencoder for music"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q-mqM2myd0F"
      },
      "source": [
        "## Download file to local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNOpNkUFSL3C"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.plotting import *\n",
        "from audioautoencoder.datasets.utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzePP5Urw927"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "train = True\n",
        "LOAD_DATA = True\n",
        "load_model = False\n",
        "\n",
        "# --------------- Main Execution parameters ---------------\n",
        "model_name = 'UNetConv10_mask'\n",
        "SNRdB_load = [-10, 10]\n",
        "SNRdBs = [[-10, 10]] # SNR random range\n",
        "load_trigger = [load_model]\n",
        "load_file = 'Autoencodermodel_earlystopping.pth'\n",
        "#load_file = 'Autoencodermodel_checkpoint.pth'\n",
        "\n",
        "folder = ['sep_features'][i] # sep\n",
        "\n",
        "# parameters\n",
        "learning_rates = [1e-3] # 1e-4 for re0training?, 1e-3 for training?\n",
        "\n",
        "base_lr=1e-5\n",
        "max_lr=learning_rates[i]\n",
        "gamma=0.8\n",
        "\n",
        "# data params\n",
        "max_file_size_gb = 100\n",
        "IMPORT_TRAIN_NOISY = train\n",
        "batch_size = 32\n",
        "\n",
        "# training params\n",
        "load = load_trigger[i]\n",
        "warm_start = False\n",
        "epochs = 100\n",
        "accumulation_steps = int(512/batch_size)\n",
        "\n",
        "SNRdB = SNRdBs[i]\n",
        "learning_rate = learning_rates[i]\n",
        "eta_min = 1e-5\n",
        "\n",
        "print('lr:', learning_rate)\n",
        "print('SNRdB:', SNRdB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DKs1xyWxBdX"
      },
      "outputs": [],
      "source": [
        "# --------------- In Loop Parameters --------------\n",
        "output_path = f'/content/drive/MyDrive/Projects/ML_Projects/De-noising-autoencoder/Models_Denoising/Checkpoints_{model_name}_{SNRdB[0]}-{SNRdB[1]}/'\n",
        "load_path = f'/content/drive/MyDrive/Projects/ML_Projects/De-noising-autoencoder/Models_Denoising/Checkpoints_{model_name}_{SNRdB_load[0]}-{SNRdB_load[1]}/{load_file}'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import joblib  # or use pickle if you prefer\n",
        "\n",
        "def save_scalers(scalers, save_path):\n",
        "    \"\"\"Save scalers to a file.\"\"\"\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    joblib.dump(scalers, save_path)\n",
        "\n",
        "def load_scalers(save_path):\n",
        "    \"\"\"Load scalers from a file.\"\"\"\n",
        "    return joblib.load(save_path)"
      ],
      "metadata": {
        "id": "ISQ1XFZQuV2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MW2Gsna7ku_"
      },
      "outputs": [],
      "source": [
        "# Define the source and destination file paths\n",
        "if LOAD_DATA:\n",
        "  scaler_file = output_path + \"scalers.pkl\"  # Static filename since it's unique per run\n",
        "  source_folder = f\"/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_{folder}/SNRdB_{SNRdB[0]}-{SNRdB[1]}/\"\n",
        "  source_path = source_folder + \"train/\"\n",
        "  destination_path = f\"/content/SNRdB_{SNRdB[0]}-{SNRdB[1]}/train/\"\n",
        "  save_path = source_folder + \"combined_000.h5\"\n",
        "  subset = False\n",
        "\n",
        "  if IMPORT_TRAIN_NOISY:\n",
        "    dataset_path = f\"/content/SNRdB_{SNRdB[0]}-{SNRdB[1]}/train/combined_000.h5\"\n",
        "    if not os.path.exists(destination_path):\n",
        "      combine_h5_files_features(source_path, destination_path, max_file_size_gb=max_file_size_gb)\n",
        "\n",
        "    if os.path.exists(scaler_file):\n",
        "        print(\"Loading existing scalers...\")\n",
        "        scalers = load_scalers(scaler_file)\n",
        "    else:\n",
        "        print(\"Training new scalers...\")\n",
        "        scalers = train_scalers_separation(dataset_path, sample_size=8000)\n",
        "        save_scalers(scalers, scaler_file)\n",
        "\n",
        "    print(scalers)\n",
        "\n",
        "    train_loader = ChannelDatasetLoader(\n",
        "          dataset_path=dataset_path,\n",
        "          scalers=scalers,\n",
        "          output_time_length=175,\n",
        "          channels=1,\n",
        "          snr_db=SNRdB,\n",
        "          subset=subset,\n",
        "          batch_size=batch_size\n",
        "      )\n",
        "\n",
        "    print(f\"Training set size: {len(train_loader.train_dataset)}\")\n",
        "    print(f\"Validation set size: {len(train_loader.val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_DATA:\n",
        "  input, output, medatata = train_loader.train_dataset[200]\n",
        "  print(input.shape)\n",
        "  print(output.shape)"
      ],
      "metadata": {
        "id": "l6-sIxJ-qEDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_DATA:\n",
        "  import matplotlib.pyplot as plt\n",
        "  import torch\n",
        "\n",
        "  # Fetch a sample\n",
        "  input_tensor, output_tensor, metadata = train_loader.train_dataset[50]\n",
        "\n",
        "  # Convert to NumPy for plotting\n",
        "  input_array = input_tensor.numpy()\n",
        "  output_array = output_tensor.numpy()\n",
        "\n",
        "  # Number of input channels\n",
        "  num_channels_in = input_array.shape[0]\n",
        "  num_channels_out = output_array.shape[0]\n",
        "\n",
        "  # Create subplots\n",
        "  fig, axes = plt.subplots(num_channels_in + num_channels_out, 1, figsize=(15, 30))\n",
        "\n",
        "  # Plot each input channel\n",
        "  for i in range(num_channels_in):\n",
        "      input = input_array[i]\n",
        "      print('Min, Max: ', np.min(input), np.max(input))\n",
        "      im = axes[i].imshow(input, aspect='auto', cmap='magma')\n",
        "      axes[i].invert_yaxis()\n",
        "\n",
        "      axes[i].set_title(f\"Input Channel {i+1}\")\n",
        "      axes[i].axis(\"off\")\n",
        "\n",
        "      # Add colorbar\n",
        "      cbar = fig.colorbar(im, ax=axes[i], orientation=\"vertical\")\n",
        "      cbar.set_label(\"Amplitude\")\n",
        "\n",
        "    # Plot each input channel\n",
        "  for i in range(num_channels_out):\n",
        "      output = output_array[i]\n",
        "      print('Min, Max: ', np.min(output), np.max(output))\n",
        "      im = axes[num_channels_in + i].imshow(output, aspect='auto', cmap='magma')\n",
        "      axes[num_channels_in + i].invert_yaxis()\n",
        "\n",
        "      axes[num_channels_in + i].set_title(f\"Output Channel {i+1}\")\n",
        "      axes[num_channels_in + i].axis(\"off\")\n",
        "\n",
        "      # Add colorbar\n",
        "      cbar = fig.colorbar(im, ax=axes[num_channels_in + i], orientation=\"vertical\")\n",
        "      cbar.set_label(\"Amplitude\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "YO8_4lXBtXim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsVtpS6ksh_9"
      },
      "source": [
        "# Retrain Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNVGzk5jK856"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.loss import *\n",
        "from audioautoencoder.utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7M1VlCI5WHr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2BLTePNsRoG"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model, define loss function and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNetConv10().to(device)\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGiVzAgVD0Ra"
      },
      "outputs": [],
      "source": [
        "if load:\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "  #scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=eta_min)\n",
        "  scheduler_loss = False\n",
        "else:\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=eta_min)\n",
        "  scheduler_loss = False\n",
        "\n",
        "  #optimizer = None #torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "  #scheduler = None #torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
        "  #scheduler_loss = False #True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clears all allocated GPU memory in PyTorch.\"\"\"\n",
        "    torch.cuda.empty_cache()  # Clears cache\n",
        "    gc.collect()  # Runs Python garbage collector\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        torch.cuda.reset_peak_memory_stats(i)  # Resets peak memory tracking\n",
        "\n",
        "clear_gpu_memory()\n"
      ],
      "metadata": {
        "id": "6pxw01Uptmvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv3swntPn-kj"
      },
      "outputs": [],
      "source": [
        "if train:\n",
        "  trainer = DenoisingTrainer(\n",
        "      model=model, noisy_train_loader=train_loader.train_loader, noisy_val_loader=train_loader.val_loader,\n",
        "      SNRdB=SNRdB, output_path=output_path, epochs=epochs, learning_rate=learning_rate,\n",
        "      load=load, warm_start=warm_start, train=train, verbose=False, accumulation_steps=accumulation_steps, load_path=load_path,\n",
        "      base_lr=base_lr, max_lr=max_lr, gamma=gamma, optimizer=optimizer, scheduler=scheduler, scheduler_loss=scheduler_loss,\n",
        "      max_noise=0.01, noise_epochs=5\n",
        "  )\n",
        "  trainer.train_or_evaluate()\n",
        "  model = trainer.get_model()\n",
        "\n",
        "  # I need a flat load model function somewhere, as now I need to define a train loader before I can load a model\n",
        "  csv_file_path = output_path + \"training_log.csv\"\n",
        "  plot_training_log(csv_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VFPK5D5Xab2"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNetConv9().to(device)\n",
        "denoiser = DenoisingLoader(model, load_path)\n",
        "model = denoiser.model\n",
        "print('Loaded Model')\n",
        "\n",
        "# Example input (batch_size=1, channels=2, height=1025, width=175)\n",
        "noisy_input = torch.randn(1, 9, 1025, 175)\n",
        "\n",
        "denoised_output = denoiser.denoise(noisy_input)\n",
        "print(denoised_output.shape)"
      ],
      "metadata": {
        "id": "U9A8qCuyqbam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r-L1IZ-l9O7"
      },
      "source": [
        "## Testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAcVayA_pesy"
      },
      "outputs": [],
      "source": [
        "# Define the source and destination file paths\n",
        "SNRdB = SNRdB # SNR random range\n",
        "print(SNRdB)\n",
        "#filename = f\"train-SNRdB_{SNRdB}-1s-44-1khz-magnitude-freqweightmagnitude-phase.h5\"\n",
        "source_folder = f\"/content/drive/MyDrive/Datasets/Music-Noise/SNRdB_{folder}/SNRdB_{SNRdB[0]}-{SNRdB[1]}/\"\n",
        "source_path = source_folder + \"test/\"\n",
        "destination_path = f\"/content/SNRdB_{SNRdB[0]}-{SNRdB[1]}/test/\"\n",
        "scaler_file = output_path + \"scalers.pkl\"  # Static filename since it's unique per run\n",
        "\n",
        "save_path = destination_path + \"combined_000.h5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQwjhSZJpesz"
      },
      "outputs": [],
      "source": [
        "IMPORT_TEST_NOISY = True\n",
        "load_dataframe = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEucu19Apesz"
      },
      "outputs": [],
      "source": [
        "max_file_size_gb = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_338U2bpesz"
      },
      "outputs": [],
      "source": [
        "destination_path = f\"/content/SNRdB_{SNRdB[0]}-{SNRdB[1]}/test/\"\n",
        "\n",
        "if IMPORT_TEST_NOISY:\n",
        "  if not os.path.exists(destination_path):\n",
        "    combine_h5_files_features(source_path, destination_path, max_file_size_gb=max_file_size_gb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_YpNU8IjC8V"
      },
      "source": [
        "View Pretrained Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq8WWxdalRhK"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.datasets.utils import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(scaler_file):\n",
        "    print(\"Loading existing scalers...\")\n",
        "    scalers = load_scalers(scaler_file)\n",
        "else:\n",
        "    print(\"Training new scalers...\")\n",
        "    scalers = train_scalers_separation(dataset_path, sample_size=8000)\n",
        "    save_scalers(scalers, scaler_file)"
      ],
      "metadata": {
        "id": "y6qyNPFoPIu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbFb6LzXlRhL"
      },
      "outputs": [],
      "source": [
        "if IMPORT_TEST_NOISY:\n",
        "    print(\"Loading existing scalers...\")\n",
        "    scalers = load_scalers(scaler_file)\n",
        "    test_loader = ChannelDatasetLoader(\n",
        "          dataset_path=save_path,\n",
        "          scalers=scalers,\n",
        "          output_time_length=175,\n",
        "          channels=1,\n",
        "          snr_db=SNRdB,\n",
        "          subset=False,\n",
        "          batch_size=4\n",
        "      )\n",
        "\n",
        "    print(f\"Training set size: {len(test_loader.train_dataset)}\")\n",
        "    print(f\"Validation set size: {len(test_loader.val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLtXGJboq-vp"
      },
      "outputs": [],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m4TfuxEJ0ti"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.system(\"pip install --upgrade torchmetrics\")"
      ],
      "metadata": {
        "id": "mycn_B1xfe36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyyEUEWoLXYs"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.testing import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if load_dataframe:\n",
        "  df_subset = pd.read_csv(output_path + f\"df_subset_SNRdB_{SNRdB[0]}-{SNRdB[1]}.csv\")"
      ],
      "metadata": {
        "id": "OthiH-tDgRC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ8jaUW4j3Pr"
      },
      "outputs": [],
      "source": [
        "if not load_dataframe:\n",
        "  criterion = nn.L1Loss()\n",
        "  loss, df_eval = test_model(model, test_loader.train_loader, criterion, scalers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrOfxekHV0C3"
      },
      "outputs": [],
      "source": [
        "if not load_dataframe:\n",
        "  # Assuming `df` is your original dataframe\n",
        "  #df_eval[\"Improvement\"] = df_eval[\"l1_outvstar\"] df_eval[\"l1_invstar\"]  # Higher SDR is better\n",
        "  subset_columns = [\"instance\", \"l1_invstar\", \"l1_outvstar\", \"l1_invstar_4k\", \"l1_outvstar_4k\", \"l1_invstar_full\", \"l1_outvstar_full\",  \"filename\", \"snr_db\"] #\"Improvement\"]\n",
        "  df_subset = df_eval#[subset_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "istycCl4X5bQ"
      },
      "outputs": [],
      "source": [
        "if not load_dataframe:\n",
        "  # Create a function to map filename to a class\n",
        "  def get_class_from_filename(filename, classes):\n",
        "      for keyword in classes:\n",
        "          if keyword in filename:\n",
        "              return keyword\n",
        "      return 'Unknown'  # Default if no match found\n",
        "\n",
        "  df_subset[['filename_audio', 'filename_noise']] = pd.DataFrame(df_subset['filename'].tolist(), index=df_subset.index)\n",
        "  df_subset['filename_audio'] = df_subset['filename_audio'].apply(lambda x: x.decode('utf-8'))\n",
        "  df_subset['filename_noise'] = df_subset['filename_noise'].apply(lambda x: x.decode('utf-8'))\n",
        "\n",
        "  classes = ['mixture', 'vocals', 'drums', 'guitar', 'bass', 'piano', 'electric_guitar', 'acoustic_guitar', 'synthesizer', 'strings', 'brass']\n",
        "  df_subset['audio_class'] = df_subset['filename_audio'].apply(lambda x: get_class_from_filename(x, classes))\n",
        "\n",
        "  classes = ['0707', 'Rain', 'Crowd', 'Water', 'Ice']\n",
        "  df_subset['noise_class'] = df_subset['filename_noise'].apply(lambda x: get_class_from_filename(x, classes))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_subset[\"Improvement_L1\"] = df_subset[\"l1_invstar\"] - df_subset[\"l1_outvstar\"]  # Lower L1 loss is better\n",
        "df_subset[\"Improvement_L1_4k\"] = df_subset[\"l1_invstar_4k\"] - df_subset[\"l1_outvstar_4k\"]  # Lower L1 loss is better\n",
        "df_subset[\"Improvement_L1_full\"] = df_subset[\"l1_invstar_full\"] - df_subset[\"l1_outvstar_full\"]  # Lower L1 loss is better"
      ],
      "metadata": {
        "id": "XQ9SAAWnm1fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def threshold_spectrogram(spectrogram, threshold):\n",
        "    \"\"\"\n",
        "    Zeroes out all values in the spectrogram that are below the given threshold.\n",
        "\n",
        "    Args:\n",
        "        spectrogram (np.ndarray): Input 2D array.\n",
        "        threshold (float): The threshold value.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The processed spectrogram with values below threshold set to zero.\n",
        "    \"\"\"\n",
        "    spectrogram = np.where(spectrogram >= threshold, spectrogram, 0)\n",
        "    return spectrogram"
      ],
      "metadata": {
        "id": "yqV50cKVMA-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_rate = 44100  # 44.1 kHz audio\n",
        "n_fft = 2048  # Adjust this for better resolution\n",
        "freqs = np.linspace(0, sampling_rate / 2, n_fft // 2 + 1)  # STFT frequency bins\n",
        "\n",
        "# Find indices corresponding to 0–4000 Hz\n",
        "min_freq, max_freq = 0, 4000\n",
        "freq_indices = np.where((freqs >= min_freq) & (freqs <= max_freq))[0]\n",
        "\n",
        "# in spectrogram\n",
        "index = 18\n",
        "\n",
        "snr_db = np.array(df_subset.loc[index, \"snr_db\"])\n",
        "print(snr_db)\n",
        "\n",
        "# lets evaluate this from a l1 loss perspective\n",
        "# reconstruct spectrogram\n",
        "out_spectrogram = np.array(df_subset.loc[index, \"out_track\"][0])\n",
        "out_spectrogram[df_subset.loc[index, \"metadata\"][\"freq_indices_hf\"], :] = resample_feature(np.array(df_subset.loc[index, \"out_track\"][1]), df_subset.loc[index, \"metadata\"][\"hf_shape\"])\n",
        "out_spectrogram[df_subset.loc[index, \"metadata\"][\"freq_indices_mf\"], :] = resample_feature(np.array(df_subset.loc[index, \"out_track\"][2]), df_subset.loc[index, \"metadata\"][\"mf_shape\"])\n",
        "out_spectrogram[df_subset.loc[index, \"metadata\"][\"freq_indices_lf\"], :] = resample_feature(np.array(df_subset.loc[index, \"out_track\"][3]), df_subset.loc[index, \"metadata\"][\"lf_shape\"])\n",
        "out_spec_copy = out_spectrogram\n",
        "\n",
        "out_spectrogram = threshold_spectrogram(out_spectrogram, np.mean(out_spectrogram)*0.75)\n",
        "\n",
        "# out, with no join\n",
        "out_track = np.array(df_subset.loc[index, \"out_track\"])[0]\n",
        "\n",
        "# out spectrogram\n",
        "in_spectrogram = df_subset.loc[index, \"in_track\"][0]\n",
        "\n",
        "# target\n",
        "tar_track = np.array(df_subset.loc[index, \"tar_track\"])[0]\n",
        "\n",
        "# inverse normalisation to 0 - 1\n",
        "out_spectrogram = (out_spectrogram - 0.5) * 3\n",
        "out_track = (out_track - 0.5) * 3\n",
        "in_spectrogram = (in_spectrogram - 0.5) * 3\n",
        "tar_track = (tar_track - 0.5) * 3\n",
        "\n",
        "# Inverse standardisation\n",
        "input_temp = tar_track\n",
        "in_spectrogram = scalers[\"input_features_spectrogram\"].inverse_transform(in_spectrogram.reshape(1, -1)).reshape(input_temp.shape)\n",
        "\n",
        "out_spectrogram = scalers[\"target_features_spectrogram\"].inverse_transform(out_spectrogram.reshape(1, -1)).reshape(input_temp.shape)\n",
        "out_track = scalers[\"target_features_spectrogram\"].inverse_transform(out_track.reshape(1, -1)).reshape(input_temp.shape)\n",
        "\n",
        "tar_track = scalers[\"target_features_spectrogram\"].inverse_transform(tar_track.reshape(1, -1)).reshape(input_temp.shape)\n",
        "\n",
        "# plot things\n",
        "# Plot spectrograms\n",
        "fig, axes = plt.subplots(4, 1, figsize=(15, 15))\n",
        "\n",
        "axes[0].imshow(in_spectrogram, aspect=\"auto\", cmap=\"magma\", origin=\"lower\")\n",
        "axes[0].set_title(\"Noisy Input (Log Scale)\")\n",
        "axes[0].set_yscale(\"log\")\n",
        "axes[0].set_ylim((1, 1000))\n",
        "\n",
        "axes[1].imshow(out_spectrogram, aspect=\"auto\", cmap=\"magma\", origin=\"lower\")\n",
        "axes[1].set_title(\"Denoised Output (Log Scale) - reconstructed\")\n",
        "axes[1].set_yscale(\"log\")\n",
        "axes[1].set_ylim((1, 1000))\n",
        "\n",
        "axes[2].imshow(out_track, aspect=\"auto\", cmap=\"magma\", origin=\"lower\")\n",
        "axes[2].set_title(\"Denoised Output (Log Scale)\")\n",
        "axes[2].set_yscale(\"log\")\n",
        "axes[2].set_ylim((1, 1000))\n",
        "\n",
        "axes[3].imshow(tar_track, aspect=\"auto\", cmap=\"magma\", origin=\"lower\")\n",
        "axes[3].set_title(\"Clean Target (Log Scale)\")\n",
        "axes[3].set_yscale(\"log\")\n",
        "axes[3].set_ylim((1, 1000))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GNrvnkw4Yo04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def magphase_to_waveform(magnitude, phase, audio_length=44100):\n",
        "    \"\"\"\n",
        "    Converts a spectrogram image back into an audio waveform.\n",
        "\n",
        "    Parameters:\n",
        "        image (np.array): Spectrogram image (3 channels).\n",
        "        sr (int): Sampling rate.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Reconstructed audio waveform.\n",
        "    \"\"\"\n",
        "    stft = magnitude * np.exp(1j * phase)\n",
        "    return librosa.istft(stft, length=audio_length)"
      ],
      "metadata": {
        "id": "XVYDvDepaM-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io.wavfile\n",
        "from google.colab import files\n",
        "import librosa\n",
        "\n",
        "# output waveform\n",
        "phase = df_subset.loc[index, \"metadata\"][\"phase\"]\n",
        "#phase = scalers[\"input_features_phase\"].inverse_transform(phase.reshape(1, -1)).reshape(input_temp.shape)\n",
        "print(np.max(phase))\n",
        "print(np.min(phase))\n",
        "\n",
        "# reverse log scale\n",
        "out_spectrogram = librosa.db_to_amplitude(out_spectrogram)\n",
        "signal = magphase_to_waveform(out_spectrogram, phase, 44100 * 2)\n",
        "\n",
        "# Save as WAV file\n",
        "output_filename = f\"denoised_audio_{index}:{snr_db}.wav\"\n",
        "scipy.io.wavfile.write(output_filename, rate=44100, data=signal)  # 16-bit PCM\n",
        "\n",
        "# Download the file\n",
        "files.download(output_filename)\n",
        "\n",
        "tar_track = librosa.db_to_amplitude(tar_track)\n",
        "signal = magphase_to_waveform(tar_track, phase, 44100 * 2)\n",
        "\n",
        "# Save as WAV file\n",
        "output_filename = f\"audio_{index}:{snr_db}.wav\"\n",
        "scipy.io.wavfile.write(output_filename, rate=44100, data=signal)  # 16-bit PCM\n",
        "\n",
        "# Download the file\n",
        "files.download(output_filename)\n",
        "\n",
        "in_spectrogram = librosa.db_to_amplitude(in_spectrogram)\n",
        "signal = magphase_to_waveform(in_spectrogram, phase, 44100 * 2)\n",
        "\n",
        "# Save as WAV file\n",
        "output_filename = f\"noisy_audio_{index}:{snr_db}.wav\"\n",
        "scipy.io.wavfile.write(output_filename, rate=44100, data=signal)  # 16-bit PCM\n",
        "\n",
        "# Download the file\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "id": "a9RpJcI7Zkw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp03uLRHVuG9"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Set minimal theme\n",
        "sns.set_theme(style=\"white\", font_scale=1.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "df_eval = df_subset\n",
        "\n",
        "# Round SNR values to the nearest 0.5 to reduce noise\n",
        "df_eval[\"snr_db_rounded\"] = df_eval[\"snr_db\"].round(1)  # Rounds to 1 decimal place\n",
        "df_eval[\"snr_db_rounded\"] = (df_eval[\"snr_db_rounded\"] * 2).round() / 2  # Ensures nearest 0.5\n",
        "\n",
        "# Group by rounded SNR and audio/noise class, then compute mean improvement\n",
        "df_audio_avg = df_eval.groupby([\"snr_db_rounded\", \"audio_class\"], as_index=False)[\"Improvement_L1\"].mean()\n",
        "df_noise_avg = df_eval.groupby([\"snr_db_rounded\", \"noise_class\"], as_index=False)[\"Improvement_L1\"].mean()\n",
        "\n",
        "# Line plot colored by 'audio_class'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_audio_avg, x=\"snr_db_rounded\", y=\"Improvement_L1\", hue=\"audio_class\", palette=\"tab10\", marker=\"o\")\n",
        "plt.xlabel(\"SNR (dB)\")\n",
        "plt.ylabel(\"Mean Improvement (SDR)\")\n",
        "plt.title(\"Mean Improvement vs SNR (Rounded to 0.5, Colored by Audio Class)\")\n",
        "plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Line plot colored by 'noise_class'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_noise_avg, x=\"snr_db_rounded\", y=\"Improvement_L1\", hue=\"noise_class\", palette=\"tab10\", marker=\"o\")\n",
        "plt.xlabel(\"SNR (dB)\")\n",
        "plt.ylabel(\"Mean Improvement (SDR)\")\n",
        "plt.title(\"Mean Improvement vs SNR (Rounded to 0.5, Colored by Noise Class)\")\n",
        "plt.legend(title=\"Noise Class\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "x_9TP5JJnAmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "df_eval = df_subset\n",
        "\n",
        "# Round SNR values to the nearest 0.5 to reduce noise\n",
        "df_eval[\"snr_db_rounded\"] = df_eval[\"snr_db\"].round(1)  # Rounds to 1 decimal place\n",
        "df_eval[\"snr_db_rounded\"] = (df_eval[\"snr_db_rounded\"] * 2).round() / 2  # Ensures nearest 0.5\n",
        "\n",
        "# Group by rounded SNR and audio/noise class, then compute mean improvement\n",
        "df_audio_avg = df_eval.groupby([\"snr_db_rounded\", \"audio_class\"], as_index=False)[\"Improvement_L1_4k\"].mean()\n",
        "df_noise_avg = df_eval.groupby([\"snr_db_rounded\", \"noise_class\"], as_index=False)[\"Improvement_L1_4k\"].mean()\n",
        "\n",
        "# Line plot colored by 'audio_class'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_audio_avg, x=\"snr_db_rounded\", y=\"Improvement_L1_4k\", hue=\"audio_class\", palette=\"tab10\", marker=\"o\")\n",
        "plt.xlabel(\"SNR (dB)\")\n",
        "plt.ylabel(\"Mean Improvement (SDR)\")\n",
        "plt.title(\"Mean Improvement vs SNR (Rounded to 0.5, Colored by Audio Class)\")\n",
        "plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Line plot colored by 'noise_class'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_noise_avg, x=\"snr_db_rounded\", y=\"Improvement_L1_4k\", hue=\"noise_class\", palette=\"tab10\", marker=\"o\")\n",
        "plt.xlabel(\"SNR (dB)\")\n",
        "plt.ylabel(\"Mean Improvement (SDR)\")\n",
        "plt.title(\"Mean Improvement vs SNR (Rounded to 0.5, Colored by Noise Class)\")\n",
        "plt.legend(title=\"Noise Class\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1G_7pHjli0R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = df_subset\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax = sns.boxplot(x=\"noise_class\", y=\"Improvement_L1\", hue=\"audio_class\", data=df)\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.ylim(-0.5, 0.5)\n",
        "plt.savefig(output_path + f\"boxplot_all_L1.png\")\n",
        "plt.show()\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter for 'crowd' noise class\n",
        "df_crowd = df_subset[df_subset[\"noise_class\"] == \"Crowd\"].copy()\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(7, 5))\n",
        "ax = sns.boxplot(x=\"audio_class\", y=\"Improvement_L1\", hue=\"audio_class\", data=df_crowd)\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks()\n",
        "#plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.ylim(-0.5, 0.5)\n",
        "plt.savefig(output_path + f\"boxplot_crowd_L1.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "os6v-7ziieOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = df_subset\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax = sns.boxplot(x=\"noise_class\", y=\"Improvement_L1_4k\", hue=\"audio_class\", data=df)\n",
        "\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.ylim(-0.5, 0.5)\n",
        "plt.savefig(output_path + f\"boxplot_all_L1_4k.png\")\n",
        "plt.show()\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter for 'crowd' noise class\n",
        "df_crowd = df_subset[df_subset[\"noise_class\"] == \"Crowd\"].copy()\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(7, 5))\n",
        "ax = sns.boxplot(x=\"audio_class\", y=\"Improvement_L1_4k\", hue=\"audio_class\", data=df_crowd)\n",
        "\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks()\n",
        "#plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.ylim(-0.5, 0.5)\n",
        "plt.savefig(output_path + f\"boxplot_crowd_L1_4k.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Bo58Je9fjWtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = df_subset\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax = sns.boxplot(x=\"noise_class\", y=\"Improvement_L1_full\", hue=\"audio_class\", data=df)\n",
        "\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.savefig(output_path + f\"boxplot_all_L1_full.png\")\n",
        "plt.show()\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter for 'crowd' noise class\n",
        "df_crowd = df_subset[df_subset[\"noise_class\"] == \"Crowd\"].copy()\n",
        "\n",
        "# Create a grouped boxplot\n",
        "plt.figure(figsize=(7, 5))\n",
        "ax = sns.boxplot(x=\"audio_class\", y=\"Improvement_L1_full\", hue=\"audio_class\", data=df_crowd)\n",
        "\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f\"Improvement by Noise Class and Audio Class: SNRdB {SNRdB[0]} to {SNRdB[1]}\")\n",
        "plt.xticks()\n",
        "#plt.legend(title=\"Audio Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.savefig(output_path + f\"boxplot_crowd_L1_full.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "L2DTNG0uoW_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3AQwpWnXgob"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you already have the dataframe loaded in `df`\n",
        "# df = pd.read_csv('your_data.csv')  # Uncomment if loading from CSV\n",
        "df = df_subset\n",
        "\n",
        "# You can also add visualization here if you want to dive deeper\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a more interpretable colormap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df[['l1_invstar', 'l1_outvstar', 'snr_db', 'Improvement_L1']].corr(),\n",
        "            annot=True, cmap='mako', fmt=\".2f\", vmin=-1, vmax=1, center=0)\n",
        "\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save subset dataframe\n",
        "df_subset.to_csv(output_path + f\"df_subset_SNRdB_{SNRdB[0]}-{SNRdB[1]}.csv\", index=False)"
      ],
      "metadata": {
        "id": "zVFDKMPLS42d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "# Delete large variables\n",
        "del df_subset, df_eval, df\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "HWdVoElDc6IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvCNTO7up5e8"
      },
      "source": [
        "# Improvements that need to be made\n",
        "\n",
        "1. Metadata h5 column, including\n",
        "- Filename\n",
        "- SNR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cooVhUuEQ5aF"
      },
      "source": [
        "## Convert some entire songs and test some metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from audioautoencoder.models.UNetConv8 import *"
      ],
      "metadata": {
        "id": "xYzsD7_8qofh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------- In Loop Parameters --------------\n",
        "model_name = 'UNetConv8'\n",
        "SNRdB_load = [-10, 10]\n",
        "load_file = 'Autoencodermodel_earlystopping.pth'\n",
        "load_path = f'/content/drive/MyDrive/Projects/ML_Projects/De-noising-autoencoder/Models_Denoising/Checkpoints_{model_name}_{SNRdB_load[0]}-{SNRdB_load[1]}/{load_file}'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_one = UNetConv8().to(device)\n",
        "denoiser = DenoisingLoader(model_one, load_path)\n",
        "model_one = denoiser.model\n",
        "print('Loaded Model')\n",
        "\n",
        "model_name = 'UNetConv9'\n",
        "SNRdB_load = [-10, 10]\n",
        "load_file = 'Autoencodermodel_earlystopping.pth'\n",
        "load_path = f'/content/drive/MyDrive/Projects/ML_Projects/De-noising-autoencoder/Models_Denoising/Checkpoints_{model_name}_{SNRdB_load[0]}-{SNRdB_load[1]}/{load_file}'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_two = UNetConv9().to(device)\n",
        "denoiser = DenoisingLoader(model_two, load_path)\n",
        "model_two = denoiser.model\n",
        "print('Loaded Model')"
      ],
      "metadata": {
        "id": "zx4nvHUAnjfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvNP8ZZwQ2xF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "whole_files = '/content/drive/MyDrive/Datasets/Music/MUSDB18/test/'\n",
        "song_files = []\n",
        "\n",
        "# Walk through the directory tree\n",
        "for root, dirs, files in os.walk(whole_files):\n",
        "    # Filter files with '.wav' extension and 'mixture' in their name\n",
        "    for f in files:\n",
        "        if f.endswith('.wav') and 'mixture' in f:\n",
        "            full_path = os.path.join(root, f)\n",
        "            song_files.append(full_path)\n",
        "\n",
        "print(f\"\\nTotal matching files: {len(song_files)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hbrf6o-X0xX"
      },
      "source": [
        "Generate Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQYQ8bH4mSai"
      },
      "outputs": [],
      "source": [
        "#noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/val/crowd noise (4)_xDoJJ9.wav'\n",
        "#noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/val/plane-noise-passengers-sound_s8OrJQ.mp3'\n",
        "#noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/val/Crowd - Mall ambience_UdLE4r.wav'\n",
        "#noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test/Crowd - Cheering - Strong cheering and soft rhythmic cheering_Pxj5eZ.wav'\n",
        "#noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test/Crowd - Street parade with music_FgF6cW.wav'\n",
        "noise_file = '/content/drive/MyDrive/Datasets/Noise/All_Noise/splits_v2/test/Robocup 2019 4.1_vTYYoj.mp3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_uxrcpp04wp"
      },
      "outputs": [],
      "source": [
        "from audioautoencoder.denoising import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_audio_with_noise(audio_file, noise_file, start_time=10, duration=10,\n",
        "                             signal_level=1, noise_level=0.1, sr=44100, plot=False):\n",
        "    \"\"\"\n",
        "    Loads an audio file and a noise file, trims them, normalizes, and adds Gaussian noise.\n",
        "\n",
        "    Parameters:\n",
        "        audio_file (str): Path to the main audio file.\n",
        "        noise_file (str): Path to the noise file.\n",
        "        start_time (int): Start time (in seconds) for trimming.\n",
        "        duration (int): Total duration (in seconds).\n",
        "        signal_level (float): Scaling factor for the audio signal.\n",
        "        noise_level (float): Scaling factor for the noise.\n",
        "        sr (int): Expected sample rate (default: 44100 Hz).\n",
        "\n",
        "    Returns:\n",
        "        noisy_audio (np.array): Processed noisy audio.\n",
        "        snr (float): Signal-to-noise ratio in dB.\n",
        "    \"\"\"\n",
        "    # Load audio and noise\n",
        "    audio, audio_sr = load_audio_file(audio_file)\n",
        "    noise_waveform, noise_sr = load_audio_file(noise_file)\n",
        "\n",
        "    if len(audio) == 2:\n",
        "      audio = audio[0]\n",
        "\n",
        "    if len(noise_waveform) == 2:\n",
        "      noise_waveform = noise_waveform[0]\n",
        "\n",
        "    # Trim audio and noise to the specified start time and duration\n",
        "\n",
        "    audio = audio.cpu().numpy() if isinstance(audio, torch.Tensor) else audio\n",
        "    noise_waveform = noise_waveform.cpu().numpy() if isinstance(noise_waveform, torch.Tensor) else noise_waveform\n",
        "\n",
        "    print('Noise Sample Rate:', noise_sr)\n",
        "\n",
        "    assert audio_sr == sr, f\"Expected sample rate {sr}, but got {audio_sr}\"\n",
        "\n",
        "    # Trim audio and noise to the specified start time and duration\n",
        "    audio = audio[start_time * sr : (start_time + duration) * sr]\n",
        "    noise_waveform = noise_waveform[start_time * noise_sr : (start_time + duration) * noise_sr]\n",
        "\n",
        "    # Normalize audio to [-1, 1]\n",
        "    audio = np.clip((audio / np.max(np.abs(audio))) * signal_level, -1, 1)\n",
        "    noise_waveform = np.clip((noise_waveform / np.max(np.abs(noise_waveform))) * noise_level, -1, 1)\n",
        "\n",
        "    # Add noise to the signal\n",
        "    noisy_audio = np.clip(audio + noise_waveform, -1, 1)\n",
        "\n",
        "    # Compute SNR\n",
        "    signal_power = np.mean(audio**2)\n",
        "    noise_power = np.mean(noise_waveform**2)\n",
        "    snr = 10 * np.log10(signal_power / noise_power)\n",
        "\n",
        "    print(f\"SNR: {snr:.2f} dB\")\n",
        "\n",
        "    # Plot results\n",
        "    if plot:\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(noise_waveform, label=\"Noise\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(audio, label=\"Clean Audio\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(noisy_audio, label=\"Noisy Audio\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    return noisy_audio, sr"
      ],
      "metadata": {
        "id": "IFR0Qq2Ijwou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S5TJj7bZDnR"
      },
      "source": [
        "now the answer is to digest this waveform at 1s at a time, process those seconds, at intervals of 0.5s, window the outputs and put it back together for display on spectrograms and/or for output to .wav file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import soundfile as sf\n",
        "\n",
        "def denoise_audio_chunk(chunk, sr, model, scalers, chunk_samples=2*44100, device=None):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    features = extract_features(chunk, sr, audio_length=chunk_samples)\n",
        "    transformed_features, metadata = transform_features(features, scalers)\n",
        "\n",
        "    # AI denoising\n",
        "    input_tensor = torch.tensor(np.array([transformed_features]), dtype=torch.float32).to(device)\n",
        "    denoised = model(input_tensor)\n",
        "\n",
        "    #input_array = input_tensor.detach().cpu().numpy()[0]\n",
        "    denoised_array = denoised.detach().cpu().numpy()[0]\n",
        "\n",
        "    denoised_spectrogram = reconstruct_spectrogram(denoised_array, metadata)\n",
        "    #input_spectrogram = reconstruct_spectrogram(input_array, metadata)\n",
        "\n",
        "    # Remove lower-than-average values from the spectrogram\n",
        "    denoised_spectrogram = threshold_spectrogram(denoised_spectrogram, np.mean(denoised_spectrogram), percentage=0.5)\n",
        "\n",
        "    # De-normalize\n",
        "    denoised_spectrogram = inverse_scale(denoised_spectrogram, scalers)\n",
        "    denoised_spectrogram = librosa.db_to_amplitude(denoised_spectrogram)\n",
        "\n",
        "    output_chunk = magphase_to_waveform(denoised_spectrogram, features['phase'], chunk_samples)\n",
        "\n",
        "    return output_chunk\n",
        "\n",
        "class AudioDenoiser:\n",
        "    def __init__(self, model_one, model_two, scalers, output_path, sample_rate=44100, chunk_duration=2, step_size=0.5, device=None):\n",
        "        \"\"\"\n",
        "        Audio Denoising Pipeline using AI model.\n",
        "\n",
        "        Parameters:\n",
        "            model (torch.nn.Module): AI model for denoising.\n",
        "            output_path (str): Directory to save output files.\n",
        "            sample_rate (int): Sample rate (default 44100 Hz).\n",
        "            chunk_duration (int): Duration of each chunk in seconds.\n",
        "            step_size (float): Step size for overlap-add in seconds.\n",
        "            device (str, optional): Device for PyTorch computation (\"cuda\" or \"cpu\").\n",
        "        \"\"\"\n",
        "        self.model_one = model_one\n",
        "        self.model_two = model_two\n",
        "        self.output_path = output_path\n",
        "        self.sample_rate = sample_rate\n",
        "        self.chunk_samples = sample_rate * chunk_duration\n",
        "        self.scalers = scalers\n",
        "        self.step_samples = int(self.chunk_samples * step_size)\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model_one.to(self.device)\n",
        "        self.model_one.eval()\n",
        "        self.model_two.to(self.device)\n",
        "        self.model_two.eval()\n",
        "\n",
        "    def process_audio(self, waveform, sr):\n",
        "        \"\"\"\n",
        "        Processes audio by adding noise, chunking, denoising, and reconstructing.\n",
        "\n",
        "        Parameters:\n",
        "            input_path (str): Path to the input song file.\n",
        "            noise_path (str): Path to the noise file.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (reconstructed_audio, reconstructed_audio_input).\n",
        "        \"\"\"\n",
        "        # Load audio\n",
        "        self.waveform = waveform\n",
        "        assert sr == self.sample_rate, f\"Sample rate mismatch: expected {self.sample_rate}, got {sr}\"\n",
        "\n",
        "        # Process in chunks\n",
        "        processed_audio, processed_input = [], []\n",
        "        for start in range(0, len(waveform) - self.chunk_samples + 1, self.step_samples):\n",
        "            input_chunk = waveform[start:start + self.chunk_samples]\n",
        "\n",
        "            # --- start denoising\n",
        "            output_chunk = denoise_audio_chunk(input_chunk, sr, self.model_one, self.scalers, self.chunk_samples, self.device)\n",
        "            output_chunk = denoise_audio_chunk(output_chunk, sr, self.model_two, self.scalers, self.chunk_samples, self.device)\n",
        "\n",
        "            processed_input.append(input_chunk)\n",
        "            processed_audio.append(output_chunk)\n",
        "\n",
        "        # Reconstruct waveform with overlap-add\n",
        "        reconstructed_audio = self._overlap_add(processed_audio)\n",
        "        reconstructed_audio_input = waveform #self._overlap_add(processed_input)\n",
        "\n",
        "        # Save output\n",
        "        self._save_audio(reconstructed_audio, \"output_audio_song.wav\")\n",
        "        self._save_audio(reconstructed_audio_input, \"input_audio_song.wav\")\n",
        "\n",
        "        # Plot spectrograms\n",
        "        self._plot_spectrograms(reconstructed_audio, reconstructed_audio_input)\n",
        "\n",
        "        return reconstructed_audio, reconstructed_audio_input\n",
        "\n",
        "    def _overlap_add(self, chunks):\n",
        "        \"\"\"Reconstructs the waveform using overlap-add method.\"\"\"\n",
        "        reconstructed = np.zeros(len(self.waveform))\n",
        "        weight = np.zeros(len(self.waveform))\n",
        "\n",
        "        for i, start in enumerate(range(0, len(self.waveform) - self.chunk_samples + 1, self.step_samples)):\n",
        "            reconstructed[start:start + self.chunk_samples] += chunks[i]\n",
        "            weight[start:start + self.chunk_samples] += np.hanning(self.chunk_samples)\n",
        "\n",
        "        reconstructed /= np.maximum(weight, 1e-6)\n",
        "        reconstructed = np.clip(reconstructed, -1, 1)\n",
        "\n",
        "        fade_in = int(self.sample_rate / 2)\n",
        "        reconstructed[:fade_in] *= np.hanning(self.sample_rate)[:fade_in]\n",
        "        reconstructed[-fade_in:] *= np.hanning(self.sample_rate)[-fade_in:]\n",
        "\n",
        "        return reconstructed\n",
        "\n",
        "    def _save_audio(self, audio, filename):\n",
        "        \"\"\"Saves the audio file.\"\"\"\n",
        "        output_filename = os.path.join(self.output_path, add_datetime_to_filename(filename))\n",
        "        sf.write(output_filename, audio / np.max(audio), self.sample_rate)\n",
        "        print(f\"Saved: {output_filename}\")\n",
        "\n",
        "    def _plot_spectrograms(self, reconstructed_audio, reconstructed_audio_input):\n",
        "        \"\"\"Plots spectrograms of processed and input audio with consistent color scale.\"\"\"\n",
        "\n",
        "        import librosa\n",
        "        import librosa.display\n",
        "        import numpy as np\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Compute Mel spectrograms\n",
        "        Sxx1 = librosa.feature.melspectrogram(y=reconstructed_audio, sr=self.sample_rate, n_fft=2048, hop_length=1024)\n",
        "        Sxx2 = librosa.feature.melspectrogram(y=reconstructed_audio_input, sr=self.sample_rate, n_fft=2048, hop_length=1024)\n",
        "\n",
        "        # Convert to log scale (dB)\n",
        "        Sxx1_db = librosa.amplitude_to_db(Sxx1, ref=np.max)\n",
        "        Sxx2_db = librosa.amplitude_to_db(Sxx2, ref=np.max)\n",
        "\n",
        "        # Compute shared color limits\n",
        "        vmin, vmax = min(Sxx1_db.min(), Sxx2_db.min()), max(Sxx1_db.max(), Sxx2_db.max())\n",
        "\n",
        "        # Create figure\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
        "\n",
        "        # Plot processed audio spectrogram\n",
        "        img1 = librosa.display.specshow(Sxx1_db, sr=self.sample_rate, hop_length=1024, cmap=\"viridis\", ax=axes[0], vmin=vmin, vmax=vmax)\n",
        "        axes[0].set_title(\"Spectrogram of Processed Audio\")\n",
        "        axes[0].set_xlabel(\"Time (s)\")\n",
        "        axes[0].set_ylabel(\"Frequency (Hz)\")\n",
        "\n",
        "        # Plot input spectrogram\n",
        "        img2 = librosa.display.specshow(Sxx2_db, sr=self.sample_rate, hop_length=1024, cmap=\"viridis\", ax=axes[1], vmin=vmin, vmax=vmax)\n",
        "        axes[1].set_title(\"Spectrogram of Input Audio\")\n",
        "        axes[1].set_xlabel(\"Time (s)\")\n",
        "\n",
        "        # Add shared colorbar\n",
        "        fig.colorbar(img1, ax=axes, orientation=\"vertical\", fraction=0.02, pad=0.02, label=\"Amplitude (dB)\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def resample_feature(feature, target_shape):\n",
        "    \"\"\"Resamples a 2D numpy feature array to match target shape using torch.nn.functional.interpolate.\"\"\"\n",
        "    feature_tensor = torch.tensor(feature, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, H, W)\n",
        "    target_size = (target_shape[0], target_shape[1])  # (new_H, new_W)\n",
        "\n",
        "    resized_feature = F.interpolate(feature_tensor, size=target_size, mode=\"bilinear\", align_corners=False)\n",
        "    return resized_feature.squeeze(0).squeeze(0).numpy()  # Remove batch/channel dim and return as numpy\n",
        "\n",
        "def transform_features(features, scalers):\n",
        "    input_spectrogram = features['spectrogram']\n",
        "    input_edges = features['edges']\n",
        "    input_cepstrum = features['cepstrum']\n",
        "\n",
        "    # function to transform the extracted features to an input to the\n",
        "    target_shape = input_spectrogram.shape\n",
        "    # Apply scalers\n",
        "        #input_phase = self.scalers[\"input_features_phase\"].transform(input_phase.reshape(1, -1)).reshape(input_phase.shape)\n",
        "    input_spectrogram = scalers[\"input_features_spectrogram\"].transform(input_spectrogram.reshape(1, -1)).reshape(input_spectrogram.shape)\n",
        "    input_edges = scalers[\"input_features_edges\"].transform(input_edges.reshape(1, -1)).reshape(input_edges.shape)\n",
        "    input_cepstrum = scalers[\"input_features_cepstrum\"].transform(input_cepstrum.reshape(1, -1)).reshape(input_cepstrum.shape)\n",
        "    #input_cepstrum_edges = self.scalers[\"input_features_cepstrum_edges\"].transform(input_cepstrum_edges.reshape(1, -1)).reshape(input_cepstrum_edges.shape)\n",
        "\n",
        "    # resample mfcc featues so theyre the same shape as the spectrogram and phase features\n",
        "    # Define frequency bins\n",
        "    sampling_rate = 44100  # 44.1 kHz audio\n",
        "    n_fft = 2048  # Adjust this for better resolution\n",
        "    freqs = np.linspace(0, sampling_rate / 2, n_fft // 2 + 1)  # STFT frequency bins\n",
        "\n",
        "    # Find indices corresponding to 0–4000 Hz\n",
        "    min_freq, hf, mf, lf = 0, 4000, 1000, 200\n",
        "    freq_indices_hf = np.where((freqs >= min_freq) & (freqs <= hf))[0]\n",
        "    freq_indices_mf = np.where((freqs >= min_freq) & (freqs <= mf))[0]\n",
        "    freq_indices_lf = np.where((freqs >= min_freq) & (freqs <= lf))[0]\n",
        "    # input spectrogram\n",
        "    input_spectrogram_hf = resample_feature(input_spectrogram[freq_indices_hf, :], target_shape)\n",
        "    input_spectrogram_mf = resample_feature(input_spectrogram[freq_indices_mf, :], target_shape)\n",
        "    input_spectrogram_lf = resample_feature(input_spectrogram[freq_indices_lf, :], target_shape)\n",
        "    # edges\n",
        "    input_edges_hf = resample_feature(input_edges[freq_indices_hf, :], target_shape)\n",
        "    input_edges_mf = resample_feature(input_edges[freq_indices_mf, :], target_shape)\n",
        "    input_edges_lf = resample_feature(input_edges[freq_indices_lf, :], target_shape)\n",
        "\n",
        "    # now input indices for 0-1000 and 0-200 to add as channels and as freq_indicies for reconstruction\n",
        "\n",
        "    # Resample MFCC features\n",
        "    input_cepstrum = resample_feature(input_cepstrum, target_shape)\n",
        "\n",
        "    # Convert to tensors - input_phase, is missing,..... it's too confusing\n",
        "    inputs = torch.tensor(np.stack([\n",
        "        input_spectrogram, input_spectrogram_hf, input_spectrogram_mf, input_spectrogram_lf,\n",
        "        input_edges, input_edges_hf, input_edges_mf, input_edges_lf,\n",
        "        input_cepstrum\n",
        "    ], axis=0), dtype=torch.float32)  # Shape: (6, H, W)\n",
        "\n",
        "    a = 3\n",
        "    inputs = (inputs/a) + 0.5\n",
        "\n",
        "    # metadata\n",
        "    # Extract metadata\n",
        "    metadata = {\n",
        "        \"hf_shape\": input_spectrogram[freq_indices_hf, :].shape,\n",
        "        \"mf_shape\": input_spectrogram[freq_indices_mf, :].shape,\n",
        "        \"lf_shape\": input_spectrogram[freq_indices_lf, :].shape,\n",
        "        \"freq_indices_hf\": freq_indices_hf,\n",
        "        \"freq_indices_mf\": freq_indices_mf,\n",
        "        \"freq_indices_lf\": freq_indices_lf\n",
        "    }\n",
        "\n",
        "    return inputs, metadata\n",
        "\n",
        "def reconstruct_spectrogram(outputs, metadata):\n",
        "    # lets evaluate this from a l1 loss perspective\n",
        "    # reconstruct spectrogram\n",
        "    out_spectrogram = np.array(outputs[0])\n",
        "    out_spectrogram[metadata[\"freq_indices_hf\"], :] = resample_feature(outputs[1], metadata[\"hf_shape\"])\n",
        "    out_spectrogram[metadata[\"freq_indices_mf\"], :] = resample_feature(outputs[2], metadata[\"mf_shape\"])\n",
        "    out_spectrogram[metadata[\"freq_indices_lf\"], :] = resample_feature(outputs[3], metadata[\"lf_shape\"])\n",
        "    return out_spectrogram\n",
        "\n",
        "def inverse_scale(out_spectrogram, scalers):\n",
        "    # inverse scale the\n",
        "    # transform back to 0 centred and\n",
        "    out_spectrogram = (out_spectrogram - 0.5) * 3\n",
        "    out_spec_shape = out_spectrogram.shape\n",
        "\n",
        "    # undo scaler\n",
        "    out_spectrogram = scalers[\"input_features_spectrogram\"].inverse_transform(np.array([out_spectrogram]).reshape(1, -1)).reshape(out_spec_shape)\n",
        "    return out_spectrogram\n",
        "\n",
        "def threshold_spectrogram(spectrogram, threshold, percentage=0.8):\n",
        "    \"\"\"\n",
        "    Zeroes out all values in the spectrogram that are below the given threshold.\n",
        "\n",
        "    Args:\n",
        "        spectrogram (np.ndarray): Input 2D array.\n",
        "        threshold (float): The threshold value.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The processed spectrogram with values below threshold set to zero.\n",
        "    \"\"\"\n",
        "    spectrogram = np.where(spectrogram >= threshold * percentage, spectrogram, 0)\n",
        "    return spectrogram\n",
        "\n",
        "def magphase_to_waveform(magnitude, phase, audio_length=44100):\n",
        "    \"\"\"\n",
        "    Converts a spectrogram image back into an audio waveform.\n",
        "\n",
        "    Parameters:\n",
        "        image (np.array): Spectrogram image (3 channels).\n",
        "        sr (int): Sampling rate.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Reconstructed audio waveform.\n",
        "    \"\"\"\n",
        "    stft = magnitude * np.exp(1j * phase)\n",
        "    return librosa.istft(stft, length=audio_length)"
      ],
      "metadata": {
        "id": "Z2nkgmDifvri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_number = 10\n",
        "noisy_audio, sr = generate_audio_with_noise(song_files[file_number], noise_file, start_time=20, duration=10, noise_level=0.7)"
      ],
      "metadata": {
        "id": "5hUiKk3UsWfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8UgbuKiWYXb"
      },
      "outputs": [],
      "source": [
        "denoiser = AudioDenoiser(model_one, model_two, scalers, output_path=output_path, chunk_duration=2, step_size=0.5)\n",
        "reconstructed_audio, reconstructed_input = denoiser.process_audio(noisy_audio, sr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "from google.colab import files\n",
        "\n",
        "def save_wav_sf(file_path, audio_array, sample_rate):\n",
        "    \"\"\"Saves a NumPy array as a WAV file using soundfile.\"\"\"\n",
        "    sf.write(file_path, audio_array, sample_rate, subtype=\"PCM_16\")  # Can be PCM_24, PCM_32, FLOAT\n",
        "    files.download(file_path)  # Trigger download in Colab\n",
        "\n",
        "save_wav_sf(f\"output_{file_number}.wav\", reconstructed_audio, sr)\n",
        "save_wav_sf(f\"input_{file_number}.wav\", reconstructed_input, sr)"
      ],
      "metadata": {
        "id": "_WGNAvPmi0qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN4K8F4rrwMb"
      },
      "outputs": [],
      "source": [
        "!pip install mir_eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aorJVEypw2Ry"
      },
      "source": [
        "Use the SDR metric to compare signal to noise ratios of the generated output, and the standard output and demonstrate an increase in signal to noise ratio overall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJjf5BGKdTEe"
      },
      "outputs": [],
      "source": [
        "average_rec = np.log(np.average(Pxx_rec, axis=1))\n",
        "average_spec = np.log(np.average(Pxx_spec, axis=1))\n",
        "\n",
        "plt.plot(average_rec)\n",
        "plt.plot(average_spec)\n",
        "plt.xscale('log')\n",
        "plt.show()\n",
        "\n",
        "import numpy as np\n",
        "import mir_eval\n",
        "\n",
        "def compute_sdr(reference, estimated):\n",
        "    \"\"\"\n",
        "    Compute the Signal-to-Distortion Ratio (SDR) between reference and estimated signals.\n",
        "\n",
        "    :param reference: np.ndarray of shape (channels, samples), ground-truth clean signal\n",
        "    :param estimated: np.ndarray of shape (channels, samples), predicted separated signal\n",
        "    :return: float, SDR value in dB\n",
        "    \"\"\"\n",
        "    # Ensure inputs are 2D (stereo/multichannel) or 1D (mono)\n",
        "    reference = np.atleast_2d(reference)\n",
        "    estimated = np.atleast_2d(estimated)\n",
        "\n",
        "    # Compute SDR using mir_eval\n",
        "    sdr, _, _, _ = mir_eval.separation.bss_eval_sources(reference, estimated, compute_permutation=False)\n",
        "\n",
        "    return np.mean(sdr)  # Return average SDR across channels\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Fake reference and estimated signals (replace with actual signals)\n",
        "    ref_signal = reconstructed_audio_input  # 2 channels, 1 second at 44.1kHz\n",
        "    est_signal = reconstructed_audio  # Slightly noisy estimate\n",
        "\n",
        "    # max sdr\n",
        "    sdr_max = compute_sdr(audio, audio)\n",
        "    print(f\"SDR - : {sdr_max:.2f} dB -- Max\")\n",
        "\n",
        "    # reference sdr\n",
        "    sdr_ref = compute_sdr(audio, noisy_audio)\n",
        "    print(f\"SDR - : {sdr_ref:.2f} dB -- Reference\")\n",
        "\n",
        "    # computed srd\n",
        "    sdr_value = compute_sdr(audio, est_signal)\n",
        "    print(f\"SDR - : {sdr_value:.2f} dB -- Denoising\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8pgR2SVFiqdV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}